#!/usr/bin/env python3
# -*- Mode:Python; indent-tabs-mode:nil; tab-width:3; encoding:utf-8 -*-

"""
Filename: fmslogs.py
Author: Simon Brown on 10/15/2025
Version: 0.24, 2025-12-19
Purpose: Display FileMaker Server logs and change logging options.
"""

import argparse, curses, datetime, glob, linecache, os, pathlib, platform, re, socket, subprocess, socket, sys, textwrap, time, http.client, ssl
from collections import OrderedDict
from enum import Enum
from contextlib import ExitStack

"""
POSSIBLE NEW OPTIONS
	non-standard install location
	'top' or iostat option
	summarize results where possible (eg, count, min, max, sum)?
	convert table IDs
	paged output
	list crash reports
	remote/external log source via SSH
	for succinct mode, change 'Information' to 'Info' and 'Warning' to 'Warn' (Error will then be longest)

Reset repo:  git fetch origin main; git reset --hard origin/main

fmslogs -h access event
fmslogs -h -n 100 access event
fmslogs --ssh simon@server.beezwax.net access # using user's default 'SSH key
fmslogs -s topcall 1 # enable TopCall.log
open either directory or log file in text editor, eg $EDITOR
"""

VERSION = "0.24 - 2025-12-19"
TIMESTAMP_START = None
FILTER_REGEX = None
TEXTWRAP = textwrap.TextWrapper(width=120,tabsize=10)

class OutputMode (Enum):
	HEAD = 1
	TAIL = 2
	OTHER = 3

LAST_LOG_PRINTED = None
OUTPUT_MODE = OutputMode.TAIL
SHOW_HEADERS = True
SUCCINCT_MODE = False
TRUNCATE_MODE = False

try:
	SCREENCOLS, SCREENROWS = os.get_terminal_size()
except OSError:
	# Probably being piped so no terminal
	SCREENCOLS = 255
	SCREENROWS = 48
	
MAXREADLEN = 1048576*10

# Default deployment paths (Windows paths will have forward slashes converted)
DEF_BASE_PATHS = {
	'Darwin': '/Library/FileMaker Server',
	'Linux': '/opt/FileMaker/FileMaker Server',
	'Windows': 'C:/Programs/FileMaker/FileMaker Server'
}

# This may get overridden by user option,
BASE_PATH = DEF_BASE_PATHS [platform.system()]

LOG_ALIAS = {
	('acc', 'access'),
	('adm', 'admin'),
	('ada', 'adminapi'),
	('cat', 'catalina'),
	('cli', 'clientstats'),
	('dap', 'dapi'),
	('eve', 'event'),
	('fad', 'fmsadmindebug'),
	('fcw', 'fmscwpc'),
	('fcl', 'fmscwpcli'),
	('fms', 'fmsdebug'),
	('fgp', 'fmsgetpasskeydebug'),
	('fhd', 'fmshdebug'),	# helper debug?
	('fib', 'fmsibdebug')	# incremental backup
}

# path: location of the log file, starting at the base path
# lghd: True if the log file has a header line at start of file
# head: header to use if not succinct mode
# tbst: list of tab stops to use for columns, or just a single number for tab size
# shed: succinct version of header (may not be present)
# shtb: succinct version of tab stops (may not be present)

LOG_SPECS_BASE = {
	'access': {
		'path': 'Logs/Access.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        2025-09-15 01:12:45.831 -0700  Information  228   some-dev.filemaker.beezwax.net         The previous log file reached maximum size, and was renamed to "Access-old.log".
		'head': 'TIMESTAMP                       LEVEL        CODE  HOST                                  MESSAGE',
		'tbst': [32,45,51,89],
		#        2025-09-15 01:12:45.831  Information  228   The previous log file reached maximum size, and was renamed to "Access-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45]
	},
	'admin': {
		'path': 'Admin/FAC/logs/fac.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-17 14:29:56 -0700  Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		#			2022-05-17 14:30:00 -0700 - error:  fmi   127.0.0.1   notifications  general   n/a   "New system notification generated, type: CPU_USAGE_EXCEED_HARD_LIMIT"
		# Only uses tabs with regular messages (eg, not error or warn) after timestamp.
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},
	'adminapi': {
		'path': 'Admin/FAC/logs/fac1.log',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-24 14:04:30 -0700 - error:   fmi   127.0.0.1   fmsadminapi   general   3   "Get worker list failed."
		#			2022-05-24 19:32:51 -0700       Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		# Only uses tab after timestamp with regular messages (eg, not error or warn)
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},

	'catalina': {
		'path': 'Web Publishing/publishing-engine/jwpc-tomcat/logs/catalina.*',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        09-Dec-2025 09:31:08.332 WARNING [main] org.apache.tomcat.util.digester.SetPropertiesRule.begin Match [Server/Listener] failed to set property [AWTThreadProtection] to [true]
		#			will also include backtraces
		'head': 'Catalina.*.log                 NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
	},


	'clientstats': {
		'path': 'Logs/ClientStats.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-16 15:46:18.054 -0700  37781   8559   209   0     46442     0    28    Xeronthia Shilnow (XS ETMD6M) [255.143.244.179]
		'head': 'ClientStats.log                NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
		#			2025-10-16 15:46:18.054  37781    8559    209     0    46442    0     28   Xeronthia Shilnow (XS ETMD6M) [255.143.144.79]
		'shed': 'ClientStats.log                     NET BYTES  NET BYTES CALLS     CALLS     TIME      TIME      TIME\n' + \
				  'TIMESTAMP                IN         OUT       COMPLETE  IN PROG   ELAPSED   WAIT      I/O       CLIENT NAME',
		'shtb': [26,37,48,59,70,81,92,103]
	},
	
	'dapi': {
		'path': 'Logs/fmdapi.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-27 11:08:10.055 -0700  4101   ERROR	250.130.228.236  some-user-name   POST  Script Error -- Script File: 'filename', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		# Size at end (re-arrange columns?). Rarely a 4 digit error code.
		'head': 'TIMESTAMP	                   CODE   LEVEL   HOST            USER             HTTP  MESSAGE  SIZE',
		'tbst': [32,39,47,63,81,87],
		#			2025-08-27 11:08:10.055  301   ERROR  some-user-name   POST  Script Error -- Script File: 'Tool', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		'shed': 'TIMESTAMP	             CODE  LEVEL  USER             HTTP  MESSAGE  SIZE',
		'shtb': [26,32,39,56,62]
	},
	
	'event': {
		'path': 'Logs/Event.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-18 23:15:30.125 -0700  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'head': 'TIMESTAMP                ZONE  LEVEL        CODE  HOST                            MESSAGE',
		'tbst': [31,44,50,82],
		#			2025-08-18 23:15:30.125  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45,77]
	},
	
	'fmsadmindebug': {
		'path': 'Database Server/bin/fmsadminDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmsasedebug': {
		'path': 'Database Server/bin/fmsaseDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpc': {
		'path': 'Database Server/bin/fmscwpc.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpcli': {
		'path': 'Database Server/bin/fmscwpcli.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},

	'fmsgetpasskeydebug': {
		'path': 'Database Server/bin/fmsgetpasskeyDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmshdebug': {
		'path': 'Database Server/bin/fmshDebug.log',
		'lghd': True,
			#		2024-10-21 09:55:47.101 -0700 [HelperApp] Debug::DumpBitSettings() feature(s) enabled: [ AlwaysPrint | Assert | ForceOutput | Backup | UploadDownload | Server | ServerCommands | ServerComponents | ServerDependencies | FMSECLI | NotificationsStackCrawls | Anchors | BPlusTree | FileBox | LicenseServer | CalcEngine | Add-ons | OData | TransactSupport | AdminAPI | CertVerify | CurrentThread | Consolidator | DownloadService | WIP | EditOtherClientLayout | CLI | ThreadedSorting | DisableServerSideSorting | DisableMemoryKeyCmpIndexing | PurgeTempDB | DisableSharingLockOnServer | PersistentData | DisableServerSideSummary | SupportNestedPSOS ]
			#		2024-10-21 09:55:47.117 -0700 [HelperApp] AdminMgrConfigFile() installation directory = /opt/FileMaker/FileMaker Server/, config file = Admin/conf/deployment.xml, encrypted = false
		'head': None,
		'tbst': 8	# replace any tabs with two spaces
	},
	
	'fmsibdebug': {
		'path': 'Database Server/bin/fmsibDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmslogdebug': {
		'path': 'Database Server/bin/fmslogDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmwipd': {
		'path': 'Database Server/bin/fmwipd.log',
		'lghd': False,
		'tbst': 8
	},

	'loadschedules': {
		'path': 'Logs/LoadSchedules.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'odata': {
		'path': 'Logs/fmodata.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#		  '2025-10-14T13:01:31.232452-08:00  0     INFO   170.255.255.218  GET   /fmi/odata/v4	 75'
		'head': 'TIMESTAMP                         CODE  LEVEL  HOST             OP    ENDPOINT  SIZE',
		'tbst': [34,40,47,64,70],  # 'size' value will be padded on end
		#		  '2025-10-14T13:01:31.232452  0     INFO   GET   /fmi/odata/v4	 75'
		'shed': 'TIMESTAMP                   CODE  LEVEL  OP    ENDPOINT  SIZE',
		'shtb': [29,35,48]
	},
	
	'odatadebug': {
		'path': 'Database Server/bin/fmodataDebug.log',
		'lghd': False,
		'tbst': 8
	},
	
	'scriptevent': {
		'path': 'Logs/scriptEvent.log',
		'lghd': False,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-11 03:00:26.470 -0700  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'head': 'TIMESTAMP                ZONE  CODE  MESSAGE',
		'tbst': [32,38],
		#			2025-08-11 03:00:26.470  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'shtb': [26,32],
	},
	
	'stats': {
		'path': 'Logs/Stats.log',
		'lghd': True,
		#			---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------
		#			2025-10-17 17:54:42.335 -0700	0	14	11	0	98	0	0	1	0	0	0	2	0	546	40	81	0
		'head': 'Stats.log                      NET      NET       DISK       DISK       CACHE   CACHE     CLIENTS  OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                ZONE  KBs IN   KBs OUT   KBs READ   KBs WRITE  HIT %   UNSAVD %  PRO      DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'tbst': [31,40,50,61,72,80,90,99,105,114,123,132,142,150,159,168,177],
		'shed': 'Stats.log                NET      NET       DISK       DISK        CACHE   CACHE     PRO      OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                KB/s In  KB/s OUT  KB/s READ  KB/s WRITE  HIT %   UNSAVD %  CLIENTS  DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'shtb': [26,35,45,56,68,76,86,95,101,110,119,128,138,146,155,164,173]
	},

	'topcall': {
		'path': 'Logs/TopCallStats.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-31 22:12:31.419 -0700   166630.877193  166635.541004  4663811   Query (Find)                     Tool::table(181)::field definitions(356)     509        33         4663811    0        235659   tool-beezwax-net (172.30.8.236) [172.30.8.236]
		'head': 'TopCallStats.log               TIME           TIME           TOTAL                                                                                    NET BYTES  NET BYTES  TIME       TIME     TIME\n' + \
				  'TIMESTAMP                      START          END            ELAPSED   OPERATION                         TARGET                                       IN         OUT        ELAPSED    WAIT     I/O       CLIENT NAME',
		'tbst': [31,46,61,71,105,150,161,172,183,192,202],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},
	
	'trimlog': {
		'path': 'Database Server/bin/trimlog.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'wpe': {
		'path': 'Logs/wpe0.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2023-09-13 11:50:57 -0700  INFO	-	-	User [WebDirect-ABC8F](nnnnnnnnn_n@beezwax.net) has been signed out from database Tool.
		#			2025-08-04 08:06:10 -0700  172.130.211.135  127.0.0.1:57874  -  -  INFO  -  -  FileMaker WebDirect is enabled.
		'head': 'TIMESTAMP                  HOSTNAME         CLIENT           ACCOUNT                     MODULE_TYPE  SEVERITY  ERROR  BYTES     MESSAGE',
		'tbst': [27,44,61,89,102,112,119,129],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},
	
	'wpedebug': {
		'path': 'Logs/wpe_debug.log',
		'lghd': False,
		'tbst': 8
	}
}

LOG_SPECS_DARWIN = {
	'install': {
		'path': 'Logs/install.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'syslog': {
		'path': '!/usr/bin/log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stderr': {
		'path': 'Logs/stderr',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdout': {
		'path': 'Logs/stdout',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_LINUX = {
	'fmshelper': {
		'path': 'Logs/fmshelper.log',
		'lghd': True,
			# This log has no consistent format
			#		2025-08-03 20:12:38.182 -0700   Log file /opt/FileMaker/FileMaker Server/Logs/fmshelper.log size: 478 bytes (0 MB), threshold ratio: 0
			#		2025-08-03 20:12:38.185 -0700 === stopSystemWebServer()
			#		(Use `facstart.sh --trace-warnings ...` to show where the warning was created)
			#		Thrift: Sun Aug  3 20:12:47 2025 TNonblockingServer: Serving with 5 io threads.
			#		127.0.0.1 POST /fmi/admin/internal/v1/dbs-notification/xPR2AgRM1TODanCZ56eikiYXcbzvTDdtLIbd9Avs3Z4kuxie - - - ms
			#		Aug 03, 2025 8:12:53 PM org.apache.jasper.servlet.TldScanner scanJars
			#		2025/08/03 20:39:27.0128: [ 2525]:    TRACE:       mongoc: ENTRY: _mongoc_linux_distro_scanner_get_distro():389
		'head': None,
		'tbst': []	# replace any tabs with two spaces
	},

	'nginxaccess': {
		'path': 'NginxServer/logs/nginx-access.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'nginxerror': {
		'path': 'NginxServer/logs/nginx-error.log',
		'lghd': True,  
		'head': None,
		'tbst': 8
	},

	'stdoutserverscripting': {
		'path': 'Logs/StdOutServerScripting.log',
		'lghd': False,
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdoutserverscripting': {
		'path': 'Logs/StdOutServerScripting.log',
		'lghd': False,
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_APACHE = {
	'httpaccess': {
		'path': 'HTTPServer/logs/access_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpdctlerr': {
		'path': 'HTTPServer/logs/httpdctl.err',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpdctlout': {
		'path': 'HTTPServer/logs/httpdctl.out',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httperror': {
		'path': 'HTTPServer/logs/error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslaccess': {
		'path': 'HTTPServer/logs/ssl_access_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslerror': {
		'path': 'HTTPServer/logs/ssl_error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslrequest': {
		'path': 'HTTPServer/logs/ssl_request_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	}
}

LOG_SPECS_DARWIN.update (LOG_SPECS_BASE)
LOG_SPECS_DARWIN.update (LOG_SPECS_APACHE)

LOG_SPECS_LINUX.update (LOG_SPECS_BASE)
LOG_SPECS_LINUX.update (LOG_SPECS_APACHE)	# although not default, Apache can be used on Linux

LOG_SPECS_WINDOWS = LOG_SPECS_BASE

LOG_SPECS_ALL = {
	'Darwin': LOG_SPECS_DARWIN,
	'Linux': LOG_SPECS_LINUX,
	'Windows': LOG_SPECS_WINDOWS
}

LOG_SPECS = LOG_SPECS_ALL [platform.system()]


LOG_CHOICES = list (LOG_SPECS.keys())
LOG_CHOICES.sort()

# Even if using RawDescriptionHelpFormatter, argparse tries hard to strip leading or trailing whitespace.
HELP_EPILOGUE = """Log names supported on this platform are:\n\n  """ + '\n  '.join (LOG_CHOICES) + '\n '

ALL_CHOICES = LOG_CHOICES  # TODO: need to add in choices that aren't log names?

CLARIS_CONFIG = """
{
	"AlwaysPrint": true,
	"ForceOutput": true
}
"""

class terminal_colors:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   LIGHT_GRAY = '\033[37m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'


#
#	calc_row_metrics
#

def calc_row_metrics (numLogs, logLinesCountVal) -> tuple:
	"""
	Using the user supplied quantity,the current terminal screen size,
	and the number of logs we have to print, determine the number of rows
	we should attempt to print.
	"""
	
	screenLinesCount = 0
	numMode = None

	if logLinesCountVal.count ('s'):
		screensNumStr = logLinesCountVal.replace('s','')
		numMode = 's'
					
		if len (screensNumStr) == 0:
			screensNum = 1		# just an 's' by itself counts as 1'
		try:
			screensNum = int (screensNumStr)
		except ValueError:
			print ("Error: invalid number for number parameter")
			return (-1,-1)
			
		screenLinesCount = max ([SCREENROWS * screensNum - 1, 6])
		
		if numLogs == 1:
			linesPerLog = screenLinesCount
		else:
			linesPerLog = screenLinesCount // numLogs
	
	else:
		try:
			linesPerLog = int (logLinesCountVal)
		except ValueError:
			print ("Error: invalid number for number parameter")
			return (-1,-1)
		
	return (numMode,linesPerLog,screenLinesCount)


#
#	c h e c k _ e n d p o i n t _ s t a t u s
#

def check_endpoint_status (name: str, host: str, endpoint: str, expectedStatus: int, method='GET', port=80, useSSL=False) -> bool:
	"""
	Check for expected status result for the given endpoint.
	"""
	resultFlag = False
	print ('Checking', name, end='')
	if useSSL:
		conn = http.client.HTTPSConnection (host, port, context=ssl._create_unverified_context(), timeout=5)
	else:
		conn = http.client.HTTPConnection (host, port, timeout=5)
	try:
		conn.request(method, endpoint)
		response = conn.getresponse()
		if response.status in (200,202) or response.status == expectedStatus:
			# In some cases we get a 200 or similar, but there is an error in the XML.
			body = response.read(500).decode('utf-8')
			if body.startswith ('<?xml '):
				match = re.search(r'<error code="(\d+)', body)
				if match:
					if int (match.group(1)) == expectedStatus:
						print (': Responding')
					else:
						print (': Error %s' % match.group(1))
				else:
					print (': Error, Result Missing')
			else:
				print (': Responding')
			resultFlag = True
		else:
			print (':', response.reason)
	except http.client.HTTPException as e:
		print (':', e)
	except Exception as e:
		print (':', e)
	finally:
		conn.close()
	return resultFlag

def print_ssl_options (name: str, host: str, port=443) -> bool:
	from cryptography import x509
	from cryptography.hazmat.backends import default_backend
	returnFlag = False
	#context = ssl._create_unverified_context()
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		ssock = context.wrap_socket(sock, server_hostname=host)
		conn = ssock.connect ((host, port))
		returnFlag = True
		print('   Security Protocol:', ssock.version())
		cert_der = ssock.getpeercert(binary_form=True)
		cert = x509.load_der_x509_certificate(cert_der, default_backend())
		print ('  ', cert)
	except ssl.SSLError as e:
		print ('  ', e)
	finally:
		sock.close()
	
	return returnFlag

        
def check_tcp_status (name: str, host: str, port):
	"""
	Print the result of testing if TCP connection can be opened to the given host's port.
	"""
	resultFlag = False
	print ('Checking', name, end='')
	
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout(5)	# 5 seconds
		result = sock.connect_ex((host,port))
		if result == 0:
			print (': Responding')
		else:
			print (': Error %i, Could not open port %i' % (result, port))
	except socket.gaierror:
		print (': Could not resolve host "%s"' % host)
	finally:
		sock.close()


def check_connectivity() -> str:
	"""
	Check both internal and external interfaces for a response from the DAPI endpoint.
	"""
	
	internAddr = '127.0.0.1'
	externAddr = get_local_ip()
	print ()
	print ('Internal Interface:', internAddr)
	print ('External Interface:', externAddr)
	print ()
	print ('Web SSL Usage')
	print_ssl_options ('External Web', externAddr)
	print ()
	
	# FMP/FMGO CLIENT
	check_tcp_status ('FMP Client - Internal', internAddr, 5003)
	check_tcp_status ('FMP Client - External', externAddr, 5003)
	
	# ADMIN API
	adminEndpoint = '/fmi/admin/api/v2/server/metadata'
	check_endpoint_status ('Admin API - Internal', internAddr, adminEndpoint, 400, method='POST', port=16001)
	check_endpoint_status ('Admin API - External', externAddr, adminEndpoint, 400, method='POST', port=443, useSSL=True)
	
	# ADMIN CONSOLE
	facEndpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Internal', internAddr, facEndpoint, 200, port=16001)
	check_endpoint_status ('Admin Console - External', externAddr, facEndpoint, 200, port=443, useSSL=True)
	
	# ADMIN CONSOLE - WORKER
	facEndpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Worker - Internal', internAddr, facEndpoint, 200, port=16003)
	
	# CATALINA/CWP
	check_tcp_status ('CWP - Port 9889', internAddr, 9889)
	xmlEndpoint = '/fmi/xml'
	check_endpoint_status ('CWP - XML - Internal', internAddr, xmlEndpoint, 954, port=16021)
	check_endpoint_status ('CWP - XML - External', externAddr, xmlEndpoint, 954, port=443, useSSL=True)
	check_endpoint_status ('CWP - wpem - Internal', internAddr, 'fmswpem/', 400, port=16002)
	check_endpoint_status ('CWP - wpem - External', externAddr, '/fmi/mwpem/', 400, port=443, useSSL=True)

	# DATA API
	dapiEndpoint = '/fmi/data/vLatest/productInfo'
	check_endpoint_status ('Data API - Internal', internAddr, dapiEndpoint, 200, port=3000)
	check_endpoint_status ('Data API - External', externAddr, dapiEndpoint, 200, port=443, useSSL=True)
	
	# JDBC
	check_tcp_status ('JDBC - Internal', internAddr, 2399)
	check_tcp_status ('JDBC - External', externAddr, 2399)
	
	# ODATA API
	odataEndpoint = '/fmi/odata/v4'
	check_endpoint_status ('OData API - Internal', internAddr, odataEndpoint, 401, port=3001)
	check_endpoint_status ('OData API - External', externAddr, odataEndpoint, 401, port=443, useSSL=True)

	# docws/fmws handler
	xmlEndpoint = '/fmws/serverinfo'
	check_endpoint_status ('CWP - XML - Internal', internAddr, xmlEndpoint, 954, port=1895)
	check_endpoint_status ('CWP - XML - External', externAddr, xmlEndpoint, 954, port=443, useSSL=True)	# this may get removed

	# WEBD
	webdEndpoint = '/fmi/webd/'
	check_endpoint_status ('WebDirect - Internal', internAddr, webdEndpoint, 200, port=16021)
	check_endpoint_status ('WebDirect - External', externAddr, webdEndpoint, 200, port=443, useSSL=True)

	# alternate: curl -vk -X DELETE https://127.0.0.1/fmi/admin/api/v2/user/auth/abcdef0123456789
	print ()

#
#	c h e c k _ f i l e _ v a l i d i t y
#

def check_file_validity (path: str):
	'''
	Check whether the a given file exists, readable and is a file.
	Return None if no issue found, otherwise return an error message.
	'''
	if not os.access(path, os.F_OK):
		raise FileCheckError ("'%s' does not exist" % (path))
	if not os.access(path, os.R_OK):
		raise FileCheckError ("'%s' is not readable" % (path))
	if os.path.isdir(path):
		raise FileCheckError ("'%s' is a directory" % (path))


#
#	c o m p i l e _ f i l t e r
#

def compile_filter(regex: str) -> bool:
	
	global FILTER_REGEX
	isValid = False
	
	try:
		FILTER_REGEX = re.compile(regex)
		isValid = True
	except re.error as e:       # aliased to PatternError as of 3.13
		print(f"Error: bad regex expression: {e}\n")
	
	return isValid


#
#	e d i t _ l o g
#

def edit_log (logName: str):

	# TODO: needs Windows version for first & third options
	# TODO: use vi as second fallback if no nano?
	# TODO: open in GUI editor if available
	
	# Below will return None if syslog or glob can't find file.
	logPath = get_log_path (logName)
	
	if logPath:
		if 'EDITOR' in os.environ:
			exitCode = subprocess.call ('$EDITOR "' + logPath + '"', shell=True)
		elif platform.system() == 'Darwin' and subprocess.call (['/usr/bin/pgrep','loginwindow']) == 0:
			exitCode = subprocess.call (['/usr/bin/open', '-e', logPath])
		elif platform.system() == 'Windows':
			exitCode = subprocess.call (['notepad', logPath])		
		else:
			exitCode = subprocess.call (['/usr/bin/nano', logPath])
	
	sys.exit (exitCode)


#
#	e x p a n d _ t a b s _ f o r _ l i n e
#

def expand_tabs_for_line (logName: str, line: str) -> str:
	"""
	Expands tabs in the string `line` using the given tabstops.
	`tabstops` can be a list of column positions (e.g., [4, 8, 12]) or a single integer for fixed tab width.
	TODO: truncate if over column width?
	"""
	
	try:
		if SUCCINCT_MODE:
			tabstops = LOG_SPECS [logName]['shtb']
		else:
			tabstops = LOG_SPECS [logName]['tbst']
	except KeyError:
		tabstops = 8
	
	if logName in ['admin','adminapi']:
		# This log doesn't use tabs, but instead uses 2 or 3 spaces to separate columns.
		respace = re.compile (r"  +")
		line = respace.sub ('\t', line)
	
	if isinstance(tabstops, int):
		return line.expandtabs(tabstops)

	result = []

	parts = line.split('\t')
		
	col = 0
	tab_iter = iter(tabstops)
	next_tab = next(tab_iter, None)
	for i, part in enumerate(parts):
		result.append(part)
		col += len(part)
		if i < len(parts) - 1:
			if next_tab is not None and col < next_tab:
				spaces = next_tab - col
				result.append(' ' * spaces)
				col = next_tab
				next_tab = next(tab_iter, None)
			else:
				result.append('++')	# pad two spaces if no stop specified
				col += 1
	
	if TRUNCATE_MODE:
		line = ''.join(result)
		if len (line) > SCREENCOLS:
			return line [:SCREENCOLS-1] + "â€¦" + '\n'
		else:
			return line
	else:
		return ''.join(result)



def follow_file(some_file):
	"""
	was tail_F
	Capture output as it is added to the file.
	"""
	# https://gist.github.com/pylixm/e6bd4f5456740c12e462eecbc66692fb # tail/follow a file
	
	first_call = True
	while True:
		try:
			with open(some_file) as input:
				if first_call:
					input.seek(0, 2)
					first_call = False
				latest_data = input.read()
				while True:
					if '\n' not in latest_data:
						latest_data += input.read()
						if '\n' not in latest_data:
							yield ''
							if not os.path.isfile(some_file):
								break
							continue
					latest_lines = latest_data.split('\n')
					if latest_data[-1] != '\n':
						latest_data = latest_lines[-1]
					else:
						latest_data = input.read()
					for line in latest_lines[:-1]:
						yield line + '\n'
		except IOError:
			yield ''

def get_local_ip() -> str:
	s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
	try:
		# doesn't even have to be reachable
		s.connect(('10.255.255.255', 1))
		addr = s.getsockname()[0]
	except:
		addr = '127.0.0.1'
	finally:
		s.close()
	return addr


#
#	g e t _ l o g _ p a t h
#

def get_log_path (log: str) -> str:
	# TODO: convert Windows paths
	#print ('basePath:', BASE_PATH)
	#print ('logPath', LOG_PATHS [log])
	pathSuffix = LOG_SPECS [log]['path']

	if pathSuffix[0] == '!':
		fullPath = None
	elif pathSuffix[0] != '/':
		fullPath = BASE_PATH + '/' + LOG_SPECS[log]['path']
	else:
		fullPath = LOG_SPECS[log]['path']

	if '*' in pathSuffix:
		pathList = glob.glob (fullPath)
		if pathList != []:
			pathList.sort(reverse=True) # so that newest file is first
			fullPath = pathList[0]
		else:
			fullPath = None
	
	return fullPath


def get_file_timestamps (path: str) -> tuple:
	"""Return a file's creation and modification timestamps"""
	return (os.path.getmtime(path), pathlib.Path(path).stat().st_mtime)

	
class FileCheckError(Exception):
	def __init__(self, msg):
		self.message = msg
	def __str__(self):
		return self.message


#
#	f i n d _ f i r s t _ t i m e s t a m p
#

def find_first_timestamp (filePath: str, timestamp: datetime.datetime) -> int:
	
	"""
	Scan file until the first log timestamp equal or greater than the search
	timestamp is found, returning the line number (base 1) of matching line.
	Note that a few logs don't emit timestamps consistently. 
	If a match is never found -1 is returned.
	"""
	
	lineResult = -1
	lineNum = 0
	
	while True:
		lineTS = None
		lineNum += 1
		line = linecache.getline (filePath, lineNum)
		if line == '': break
		# Sniff the line to guess the date format.
		try:
			if line[4] == '-' and line [24] == '-':
				# Access, ClientStats, Event, Stats, etc.
				# 2025-10-27 04:13:24.101 -0700	Information	228	tool.beezwax.net	The previous log file reached maximum size, and was renamed to "Access-old.log".
				lineTS = datetime.datetime.fromisoformat(line [:23])
			elif line [4] == '/' and line [24] == ':':
				# 2025/10/25 17:49:09.0162
				lineTS = datetime.strptime(line [:23], "%Y/%m/%d %H:%M:%S.%f")
			elif line [6] == ',' and line [22] in 'APM':
				# Sep 11, 2025 12:40:52 PM org.atmosphere.cpr.AtmosphereFramework addInterceptorToAllWrappers
				# Oct 22, 2025 2:16:56 PM org.atmosphere.util.IOUtils guestRawServletPath
				lineTS = datetime.datetime.strptime(line [:24].rstrip(), '%b %d, %Y %I:%M:%S %p')
			elif line [:7] == 'Thrift:':
				# Thrift: Sat Jun  7 10:47:03 2025
				lineTS = datetime.datetime.strptime (line [8:32], '%a %b %d %H:%M:%S %Y')
			else:
				continue
		
		# Skip over any lines that have too few characters (very few) or where we can't evaluate date.
		
		except IndexError:
			continue
		#except ValueError:
		#	continue

		# If we reached the timestamp, go into next while loop to match by text value.
		if lineTS != None and lineTS >= timestamp:
			lineResult = lineNum
			break
	
	return lineResult


#
#	h a n d l e _ s e t
#

def handle_set (verb: str, noun: str):
	"""
	Execute the changes specified in parameters.
	"""
	
	if parameters [0] == '':
		pass
	

def init_curses():
    global STDSCR, SCREENCOLS, SCREENROWS
    STDSCR = curses.initscr()
    curses.noecho()
    SCREENROWS, SCREENCOLS = STDSCR.getmaxyx()
    STDSCR.scrollok(1)


#
#	l i s t_ a c t i v e _ p o r t s
#

def list_active_ports() -> list:
	'''
	List the open (listening or established) ports for processes using the fmserver user.
	'''
		
	currPlatform = platform.system()
	openList = []
	
	if currPlatform == 'Darwin' or currPlatform == 'Linux':
		if os.getuid() != 0:
			print ('Running lsof command with sudo, you may be prompted for credentials.')
		# Could've used -F here, but seems easier to process formatted output.
		lsofLines = subprocess.run(['sudo','lsof', '-P', '-u', 'fmserver', '+c15'], capture_output=True, text=True).stdout.split('\n')
		for line in lsofLines:
			accessMatch = re.search ('LISTEN|ESTABLISHED',line)
			if accessMatch == None: continue
			
			if accessMatch.group(0) == 'LISTEN':
				items = line.split()
				portNum = int (re.search(r'\d+',items[8]).group())
				# process name, IP version, connection, port, type
				openList.append (('L', items[0], items[4], items[8], portNum))
			elif accessMatch.group(0) == 'ESTABLISHED':
				# fmserverd 90834       fmserver   91u     IPv6  0x814de6ad3239a30       0t0                 TCP 172.16.184.175:5003->172.16.184.175:59835 (ESTABLISHED)
				items = line.split()
				# select digits after first colon
				portNum = int (re.search(r':\d+',items[8]).group()[1:])
				# process name, IP version, connection, port
				openList.append (('E', items[0], items[4], items[8], portNum))
				
		# remove duplicates (eg, nginx)
		openList = list (set (openList))
		openList.sort(key=lambda tupl: tupl[4])
	return openList


#
#	l i s t _ c r a s h _ r e p o r t s
#

def list_crash_reports() -> list:
	"""
	Return the full path of any recent crash report files.
	"""
	pass


#
#	r e a d _ t a i l
#

def read_tail (filePath: str, linesFromEnd: int) -> list:
	"""
	Scan lines in file, return up to linesFromEnd line numbers from the end of file.
	Line numbers are base 1 for use with linecache.getline.
	"""
	
	check_file_validity (filePath)
	lineNum = 1
	matching = []
	
	while True:
		line = linecache.getline (filePath, lineNum)
		if line == '': break
		matching.append (lineNum)
		lineNum += 1
	
	#print ('lineNum:',lineNum, 'matching:',len (matching))
	return matching [-linesFromEnd:]


#
#	r e a d _ t a i l _ f i l t e r e d
#

def read_tail_filtered (filePath: str, linesFromEnd: int) -> list:
	"""
	Search file for any matching lines, return up to linesFromEnd
	line numbers from the end of file that match. Line numbers are base 1
	for use with linecache.getline.
	"""
	
	check_file_validity (filePath)
	
	matching = []
	lineNum = 1
	while True:
		line = linecache.getline (filePath, lineNum)
		if line == '': break
		if FILTER_REGEX.search (line):
			matching.append (lineNum)
		
		lineNum += 1
		
	return matching [-linesFromEnd:]
		

#
#	r e a d _ t a i l _ f i l t e r _ a n d _ t i m e
#

def read_tail_filtered_and_time (filePath: str, linesFromEnd: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	From there, then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""
	
	check_file_validity (filePath)
	matching = []
	lastTimeLine = -1
	
	# First, find the first line containing some kind of message date
	# that is on or after our start date.
	
	lineNum = find_first_timestamp (filePath, TIMESTAMP_START)
	
	if lineNum > 0:
		# Now, filter anything after start date.
		while True:
			line = linecache.getline (filePath, lineNum)

			if line == '': break

			if FILTER_REGEX.search (line):
				matching.append (lineNum)
			lineNum += 1
	
	# Cut result down to no more than requested lines from end of file.
	return matching [-linesFromEnd:]


#
#	r e a d _ t a i l _ t i m e
#

def read_tail_time (filePath: str, linesFromEnd: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	Then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""

	check_file_validity (filePath)
	matching = []
	
	# First, find the first line containing some kind of message date
	# that is on or after our start date.
	
	lineNum = find_first_timestamp (filePath, TIMESTAMP_START)

	# Find the last line
	while True:
		line = linecache.getline (filePath, lineNum)
		if line == '': break
		# TODO: purge line list when it gets too big
		matching.append (lineNum)
		lineNum += 1

	# Cut result down to no more than requested lines from end of file.
	return matching [-linesFromEnd:]


#
#	i n i t _ p a r s e r
#

def init_parser() -> argparse.ArgumentParser:
	"""Setup parameters used for command interface. Does not attempt to parse."""

	parser = argparse.ArgumentParser(
		prog='fmslogs',
		add_help=False,
		formatter_class=argparse.RawDescriptionHelpFormatter,
		description='View FileMaker Server logs and set logging options.')

	parser.add_argument('-b', '--begin', nargs=1, help='start at first message on or after time or time interval')
	parser.add_argument('-c', '--check-connectivity', dest='check_connectivity', action='store_true', help='check connectivity of API endpoints')
	parser.add_argument('-e', '--edit', nargs=1, help='open the log file using the command defined by $EDITOR')
	parser.add_argument('-f', '--filter', nargs=1, help='only return lines matching regex expression')
	parser.add_argument('-h', '--head', action='store_true', help='display the first lines of log files')
	parser.add_argument('-H', '--headers-off', dest='headers_off', action='store_true', help='turn off headers for all logs')
	parser.add_argument('--help', action='help', help='display command details')
	parser.add_argument('-l', '--list', action='store_true', help='list all log files, including size, date created & modified, sorted by modification time')
	parser.add_argument('-L', '--lognames', action='store_true', help='list log names supported by command')
	parser.add_argument('-m', '--merge', action='store_true', help='combine output of two or more logs based on the message timestamps')
	parser.add_argument('-n', '--number', nargs=1, default=['1s'], help='range or number of lines to print')
	parser.add_argument('-p', '--password', help='FMS console or SSH password')
	parser.add_argument('-S', '--set', nargs=2, help='change log configuration option')
	parser.add_argument('-s', '--succinct', action='store_true', help='strip less useful details from log output')
	parser.add_argument('--ssh', nargs=1, help='use the connection string to fetch logs from remote server')
	parser.add_argument('-t', dest='tail', action='store_true', help='wait for any new messages')
	parser.add_argument('--tail', dest='tail', type=float, nargs=1, default=[-1.0], help='wait for any new messages, specifying number of seconds to wait for each file')
	parser.add_argument('--truncate', action='store_true', help='cut off any output if beyond width of screen')
	parser.add_argument('-u', '--user', help='FMS console or SSH account name')
	parser.add_argument('-N', '--network', action='store_true', help='list connections and their status') # TODO: throwing "expected log name" error
	parser.add_argument('-V', '--version', action='store_true', help='version info for fmslogs and FMS')
	# Hack to avoid error if there is only an option specified but no positional argument
	parser.add_argument('logs', nargs='*', help='log name to display')
	#parser.add_argument('log2', nargs='?', help='additional log to display')

	parser.epilog = HELP_EPILOGUE
	
	return parser


#
#	s t r i p _ l i n e
#

def strip_line (logName: str, line: str) -> str:
	"""
	When possible, remove repetitive or extraneous text in the logs.
	This is done after expanding tabs so columns should be at fixed positions.
	"""
	
	if logName in ['access','event']:
		if line [24] == '-':
			items = line.split('\t')
			line = '{:23.23}  {:11}  {:4}  {:<}'.format (items[0], items[1], items[2], items[4])		# remove timezone and hostname
	if logName == 'admin':
		if line [20] == '-':
			line = line [:20] + line [26:]							# remove timezone
	if logName in ['clientstats','dapi','topcall']:
			line = line [:23] + line [29]								# remove timezone
	if logName == 'dapi':
			line = line [:23] + line [29:45] + line[62:]			# remove timezone & hostname

	return line


# =================

class TailPrint (object):
	
	# based on https://github.com/kasun/python-tail
	
	''' Represents a tail command. '''
	def __init__(self, logNames: list):
		'''Initiate a Tail instance.
			Check for file validity, assigns callback function to standard out.
			Arguments:
				tailedFiles - List of file to be followed. '''
				
		self.logNames = []
		self.logPaths = []
		
		for log in logNames:
			path = get_log_path (log)
			self.check_file_validity(path)
			self.logNames.append (log)
			self.logPaths.append (path)
	
	def follow(self, s=1.0):
		''' Do a tail follow. If a callback function is registered it is called with every new line. 
		Else printed to standard out.

		Arguments:
			s - Number of seconds to wait between checking each file'''
		
		MINDELAY = 0.01 # wait at least 1/100 second before checking next log
		secs = max (s, MINDELAY)
		
		try:
			with ExitStack() as stack:
				#with open(self.tailed_file) as file_:
				fileList = [stack.enter_context(open(fpath)) for fpath in self.logPaths]
				for file in fileList:
					file.seek(0,2)
				
				# Go to the end of file
				while True:
					time.sleep (secs)

					for (file,log) in zip (fileList, self.logNames):
						while True:
							currPosition = file.tell()
							line = file.readline()
							if line:
								self.print_line (log, line)
							else:
								break
		
		except KeyboardInterrupt:
			return

	def check_file_validity(self, file_):
		''' Check whether the a given file exists, readable and is a file '''
		if not os.access(file_, os.F_OK):
			raise FileCheckError("File '%s' does not exist" % (file_))
		if not os.access(file_, os.R_OK):
			raise FileCheckError("File '%s' not readable" % (file_))
		if os.path.isdir(file_):
			raise FileCheckError("File '%s' is a directory" % (file_))

	def print_line(self, logName: str, line: str):
		"""
		Print output sent by one or more TailPrint objects. If output transitions to different file,
		indicate change by printing blank line followed by file path.
		"""
		
		global LAST_LOG_PRINTED
		
		# Check that we actually want to print this line.
		if FILTER_REGEX == None or FILTER_REGEX.search (line):
			
			if logName != LAST_LOG_PRINTED:
				if LAST_LOG_PRINTED != None: print ()
				# only print prefix line if we have transitioned output from one file to another
				print ('===', logName)
				LAST_LOG_PRINTED = logName
				
			print (expand_tabs_for_line (logName, line),end='')

# =================

#
#	p a r s e _ b e g i n _ t i m e
#

def parse_begin_time (timeStr: str) -> datetime.datetime:
	"""
	Convert the various possible time values into a timestamp value. Some
	possible values could be '30s' for 30 seconds from now, or '14:00:10'
	for 10 secs past 2pm today, or '2025-11-04 14:00' for 2pm on Nov 4.
	"""
	
	newTime = None
	
	# Duration value or a timestamp?
	m = re.search('[0-9]{0,6}[smhd]', timeStr)
	if m.group(0) != None:
		if len (timeStr) == len (m.group(0)):
			newTime = datetime.datetime.now()
			unit = m.group(0)[-1]
			unitCount = 1
			
			if len (m.group(0)) > 1:
				unitCount = max (int (m.group(0)[:-1]), 1)
			if unit == 's':
				newTime -= datetime.timedelta(seconds=unitCount)
			elif unit == 'm':
				newTime -= datetime.timedelta(minutes=unitCount)
			elif unit == 'h':
				newTime -= datetime.timedelta(hours=unitCount)
			elif unit == 'd':
				# Start days at midnight
				newTime = newTime.replace (hour=0, minute=0, second=0, microsecond=0)
				newTime -= datetime.timedelta(days=unitCount-1)

			else:
				print ('Error: invalid duration')
				sys.exit(9)
	
	print (newTime, unitCount)
	return newTime

#
#	p r i n t _ l o g
#

def print_log (logName: str, count: int):

	linesPrinted = -1
	
	if OUTPUT_MODE is OutputMode.TAIL:
		linesPrinted = print_tail (logName, count, SHOW_HEADERS, SUCCINCT_MODE)
	
	elif OUTPUT_MODE is OutputMode.HEAD:
		linesPrinted = print_head (logName, count, SHOW_HEADERS, SUCCINCT_MODE)
	else:
		print ('Error: unknown output mode')
	
	return linesPrinted
		

#
#	p r i n t _ l o g _ n a m e s
#

def print_log_names():
	"""Print out log names as used by command with their expected paths."""
	print ('LOG NAME                PATH')
	for log in LOG_CHOICES:
		print('{:22}  {:<40}'.format (log, get_log_path (log)))

#
#	p r i n t _ l o g _ i n f o
#

def print_log_info():
	"""Print one line per supported log with path, size, creation & mod timestamps."""
	print ('LOG NAME                    SIZE  CREATED                    MODIFIED')
	for log in LOG_CHOICES:
		fullPath = get_log_path (log)
				
		modTime = 0;
		#TODO: check for permissions issue

		if fullPath:
			try:
				modTime = os.path.getmtime(fullPath)
			except FileNotFoundError:
				pass
		
		if modTime > 0:
			modTimestamp = time.ctime(modTime)
			
			statInfo = os.stat(fullPath)
			
			# As of Python 3.12, st_birthtime is only available on macOS & Windows
			# ubuntu: stat -c "%w" Access.log
			# macos:  stat -f "%B"
			
			if platform.system() == 'Linux':
				# TODO: Replace with a more accurate method unless st_birthtime fixed in Python 3.15
				createTimestamp = subprocess.run(['stat', '-c', '%w', fullPath], capture_output=True, text=True).stdout[:19]
			else:
				createTimestamp = time.ctime(statInfo.st_birthtime)
				#createTimestamp = datetime.datetime.fromtimestamp(statInfo.st_birthtime).isoformat()
			
			size = statInfo.st_size
			print('{:21} {:>10}  {:<25}  {:<25}'.format (log, size, createTimestamp, modTimestamp))
		else:
			print('{:21}             <missing>'.format (log))

#
#	p r i n t _ l o g _ h e a d e r
#

def print_log_header (logName:str, succinct: bool) -> int:

	headerStr = None
	lineCount = 0

	try:
		if succinct and 'shed' in LOG_SPECS[logName]:
			headerStr = LOG_SPECS [logName]['shed']
		else:
			headerStr = LOG_SPECS [logName]['head']
	except:
		pass

	if headerStr:
		lineCount = 1 + headerStr.count ('\n')
		print (terminal_colors.BOLD,end='')
		print (headerStr)
		print (terminal_colors.END,end='')
	
	return lineCount


def print_file_head_faster (filePath: str, lines: int) -> bool:

	"""Print up to the given number of lines of text from the start of the file at the provided path.
	If MAX_READ_LEN is reached, stop output and append a '+++'.
	Result is False if there was an error opening or reading the file."""

	result = False

	if lines > 0:
		with open (filePath, 'r') as logfile:
			lines = logfile.readlines (MAX_READ_LEN)
			for line in lines[0:SCREENROWS-1]:
				 print (line, end="")
			#STDSCR.erase()
			#for line in lines:
			#    STDSCR.addstr(line)
			result = True
			if len (lines) == MAX_READ_LEN:
				 # indicate that we reached read limit
				 print ('+++')
	else:
		# We never opened the file, but will consider this a success.
		result = True

	#STDSCR.getch()
	return result

#
#   p r i n t _ f i l e _ h e a d
#

def print_file_head (logName:str, lines:int) -> bool:
    
	filePath = LOG_SPECS [logName]['path']
	lineCount = 0

	try:
		with open(filePath, "r+b") as f:
			m=mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
			print_log_header (logName)
			while True:
				line=m.readline()
				if line == '': break
				if FILTER_REGEX.search (line):
					print (line.rstrip())
					lineCount =+ 1
	except IOError:
		print ('File Error:', filePath, 'could not be opened')
		return False

	return True

#
#   p r i n t _ h e a d
#

def print_head (logName: str, count: int, header: bool, succinct: bool) -> bool:
    
	lineList = []
	lineCounter = count
	logPath =  get_log_path (logName)

	try:
		check_file_validity (logPath)
	except FileCheckError as e:
		print ('Error:', e)
		return -1
	
	# First, find first line containing some kind of message date
	# that is on or after our start date.
	
	if header:
		headerCount = print_log_header(logName, succinct)
		lineCounter -= headerCount
	else:
		headerCount = 0
	
	if TIMESTAMP_START != None:
		lineNum = find_first_timestamp (logPath, TIMESTAMP_START)
	else:
		if LOG_SPECS[logName]['lghd']:
			lineNum = 2		# skip the column header row
		else:
			lineNum = 1
	
	if lineNum > 0:
		#print (lineNum, count, maxLine)
		
		while True:
			line = linecache.getline (logPath, lineNum)
			if line == '': break
			if FILTER_REGEX != None and FILTER_REGEX.search (line):
				lineList.append (lineNum)
			else:
				lineList.append (lineNum)
			lineCounter -= 1
			if lineCounter < 1: break
			lineNum += 1
	
	if lineList != None and len (lineList) == 0:
		print ('<' + logName + ' has no messages>')

	for lineNum in lineList:
		print (expand_tabs_for_line (logName, linecache.getline (logPath, lineNum)),end='')
			
	return len (lineList) + headerCount


#
#	p r i n t _ n e t _ s t a t u s
#

def print_net_status():
	
	#' | grep -E "LISTEN" | sort -n -k 8'
	
	openPorts = list_active_ports()
	print ()
	print ('LISTENING PROCESSES')
	print ('PROCESS           VERS  CONNECTION')
	for line in openPorts:
		#print (items[0],items[4],items[8])
		if line[0] == 'L':
			print('{:16}  {:<4}  {:<15}'.format (line[1], line[2], line[3]))
	
	print ()
	print ('CONNECTIONS')
	print ('PROCESS           VERS  CONNECTION')
	for line in openPorts:
		if line[0] == 'E':
			print('{:16}  {:<4}  {:<15}'.format (line[1], line[2], line[3]))
	

#
#	p r i n t _ t a i l
#

def print_tail (logName: str, count: int, header: bool, succinct: bool) -> int:
    
	"""
	Print up to 'count' number of lines of text from the end of the file at path.
	Result is False if there was an error opening or reading the file.
	If 'header' is true, display the log headers (if any) as first line.
	If 'succinct' is true, strip less useful info from lines.
	"""

	lineList = []
	lineCount = count
	
	logPath =  get_log_path (logName)
	
	try:
		check_file_validity (logPath)
	except FileCheckError as e:
		print ('Error:', e)
		return -1
	
	# TODO: only print headers if there's log output
	if header:
		headerCount = print_log_header (logName, succinct)
		lineCount = lineCount - headerCount
	else:
		headerCount = 0

	# Below we can files only, creating a list of records to later print.
	
   # JUST DETERMINE START LINE AND USE THAT AS PARAM TO SINGLE READ FUNC?
    
	if TIMESTAMP_START != None:
		if FILTER_REGEX != None:
			lineList = read_tail_filtered_and_time (logPath, lineCount)
		else:
			lineList = read_tail_time (logPath, lineCount)
	else:
		if FILTER_REGEX != None:
			lineList = read_tail_filtered (logPath, lineCount)
		else:
			#print ('unfiltered')
			lineList = read_tail (logPath, lineCount)
	
	# the actual lineCount is now this:
	lineCount = len (lineList)
	
	if lineCount > 0:
		if LOG_SPECS[logName]['lghd'] and lineList[0] == 1:
			# Remove the header line, we'll be using our own
			del lineList[0]

	if succinct and 'shtb' in LOG_SPECS[logName].keys():
		for lineNum in lineList:
			line = strip_line (logName, linecache.getline (logPath, lineNum))
			try:
				print (expand_tabs_for_line (logName, line), end='')
			except BrokenPipeError:
				# Catch errors if output is piped and the other end closes prematurely (eg, using `head`).
				# Use below to avoid a later error when Python flushes stdout
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				lineCount = -lineCount
				break

	else:
		tabstops =  LOG_SPECS [logName]['tbst']
		
		for lineNum in lineList:
			try:
				print (expand_tabs_for_line (logName, linecache.getline (logPath, lineNum)),end='')
			except BrokenPipeError:
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				lineCount = -lineCount
				break
	
	return len (lineList) + headerCount


#
#	p r i n t _ v e r s i o n
#

def print_version():

	print ()
	print ('fmslogs', VERSION)
	print ('Latest version at: https://github.com/beezwax/fmslogs')
	print ('Questions or comments: info@beezwax.net')
	print ()
	
	# FMS VERSION
	
	if platform.system() == 'Darwin':
		fmsOutLines = subprocess.run(['/usr/sbin/pkgutil', '--pkg-info','com.filemaker.fms.worker.pkg'], capture_output=True, text=True).stdout.split('\n')
		print ('FMS:',fmsOutLines[1].split(': ')[1])
		
	if platform.system() == 'Linux':
		fmsOutLines = subprocess.run(['/usr/bin/apt-cache', 'policy', 'filemaker-server'], capture_output=True, text=True).stdout.split('\n')
		print ('FMS:',fmsOutLines[1].split(': ')[1])

	# OPENSSL
	
	if platform.system() == 'Darwin':
		openSSLWords = subprocess.run(['/Library/FileMaker Server/Database Server/bin/openssl', 'version'], capture_output=True, text=True).stdout.split()
		print ('OpenSSL:',openSSLWords[1],openSSLWords[2])
	
	if platform.system() == 'Linux':
		openSSLWords = subprocess.run(['/usr/bin/openssl', 'version'], capture_output=True, text=True).stdout.split()
		print ('OpenSSL:',openSSLWords[1],openSSLWords[2])
	
	# APACHE/NGINX/IIS
	
	if platform.system() == 'Linux' or platform.system() == 'Windows':
		if platform.system() == 'Linux':
			nginxPath = '/usr/sbin/nginx'
		else:
			nginxPath = 'C:\\Programs\\'
		
		try:
			nginxOut = subprocess.run([nginxPath, '-v'], capture_output=True, text=True).stderr
			# nginx version: nginx/1.28.0
			nginxVers = nginxOut.split ('/')[1] 
		except FileNotFoundError:
			nginxVers = 'not present'
		
		print ('NGINX:',nginxVers)
	
	if platform.system() == 'Darwin':
		httpdPath = '/usr/sbin/httpd'
		try:
			httpdOut = subprocess.run([httpdPath, '-v'], capture_output=True, text=True).stdout
			# Server version: Apache/2.4.62 (Unix)
			# Server built:   Nov  8 2025 20:07:11
			httpdVersLine = httpdOut.split('\n')[0]
			httpdVers = httpdVersLine.split('/')[1].split()[0]
		except FileNotFoundError:
			httpdVers = 'not present'
		print ('Apache:',httpdVers)
		
	if platform.system() == 'Windows':
		print ('IIS: ')
	
	print ()


#
#	m a i n
#

def main():
	global OUTPUT_MODE, SHOW_HEADERS, SUCCINCT_MODE, TIMESTAMP_START, TRUNCATE_MODE
	
	parser = init_parser()
	args = parser.parse_args()
	ignorePositionals = False
	screenLinesCounter = 0					# if used, will start (num lines on screen * num screens) - space for prompt line

	#print(args.count, args.verbose)
	#print (args)
	#init_curses()

	while True:
		if args.check_connectivity:
			check_connectivity()
			ignorePositionals = True
		
		if args.lognames:
			print_log_names()
			ignorePositionals = True
			
		if args.list:
			print_log_info()
			ignorePositionals = True
			
		if args.set:
			# Do first in case enabling a log
			print (args.set)
			handle_set (args.set[0], args.set[1])
			ignorePositionals = True
			
		if args.version:
			print_version()
			ignorePositionals = True
			break
		
		if ignorePositionals: break	# only follow through if not limiting output to the above options
		
		if args.begin:
			#print (args.begin[0])
			TIMESTAMP_START = parse_begin_time (args.begin[0])
		
		if args.edit:
			edit_log (args.edit[0])
		
		if args.network:
			print_net_status()
			break
			
		if args.head:
			OUTPUT_MODE = OutputMode.HEAD
		
		if args.begin:
			# Use head mode with the -b option
			OUTPUT_MODE = OutputMode.HEAD			
		
		if args.headers_off:
			SHOW_HEADERS = False
		
		if args.succinct:
			SUCCINCT_MODE = True
			
		if args.truncate:
			TRUNCATE_MODE = True
			
		if args.filter:
			if compile_filter (args.filter[0]) == False:	# Compile the default or the filter that was just set
				break													# bad regex

		#print (args.tail, args.logs,type (args.logs))
		
		# Assume we are printing at least one log.
		numLogsToPrint = len (args.logs)
		if numLogsToPrint == 0:
			print ('Error: expected at least one log name')
			break

		# Couldn't use choices=ALL_CHOICES in argparse, so must check names manually.
		for logName in args.logs:
				if logName not in ALL_CHOICES:
					print ('Error:', logName,'not a valid log name')
					sys.exit (1)
		
		# TAILING/FOLLOWING LOGS
		
		if (type (args.tail) == bool and args.tail) or (type (args.tail) == list and args.tail[0] > 0):
			# Instead of printing the current tail or head, wait for new messages
			tailer = TailPrint (args.logs)
			if type (args.tail) == list:
				tailer.follow(args.tail[0])
			else:
				tailer.follow(1.0)

			break
			
		# HEAD OR TAIL
		
		linesPrinted = 0;
		lastLog = args.logs[-1]

		numMode,linesPerLog,screenLinesCounter = calc_row_metrics (numLogsToPrint, args.number[0])
		if linesPerLog < 1: break

		#print (logLinesCountVal, linesPerLog, screenLinesCounter)

		for logName in args.logs:
			if logName == lastLog and numMode == 's':
				linesPerLog = screenLinesCounter			# Use whatever is left for last log (eg, there may be a remainder)

			linesPrinted = print_log (logName, linesPerLog)

			if linesPrinted < 0: break # error occured
			
			screenLinesCounter -= linesPrinted
					
		#curses.endwin()
		break

if __name__=="__main__":
    main()


#!/usr/bin/env python3
# -*- Mode:Python; indent-tabs-mode:nil; tab-width:3; encoding:utf-8 -*-

"""
Filename: fmslogs.py
Author: Simon Brown on 10/15/2025
Purpose: Display FileMaker Server logs and change logging options.
"""

import argparse, curses, datetime
import glob, http.client, itertools
import linecache, mmap, os, pathlib
import platform, pprint, re, shutil
import socket, subprocess, socket, sys
import tempfile, textwrap, threading, time, ssl, urllib.request
from enum import Enum
from contextlib import ExitStack

"""
POSSIBLE NEW OPTIONS
	non-standard install location
	'top' or iostat option
	summarize results where possible (eg, count, min, max, sum)?
	convert table IDs
	paged output
	list crash reports
	remote/external log source via SSH
	for succinct mode, change 'Information' to 'Info' and 'Warning' to 'Warn' (Error will then be longest)

EXAMPLES OF UNIMPLEMENTED USAGE
fmslogs --ssh simon@server.beezwax.net access  # if using user's default 'SSH key
fmslogs -s enable topcall  # enable TopCall.log

Reset repo:  git fetch origin main; git reset --hard origin/main
"""

VERSION = "0.33 - 2026-01-13"

TIMESTAMP_START = None
FILTER_REGEX = None
PASSWORD = None
TEXTWRAP = textwrap.TextWrapper(width=120,tabsize=10)
USER = None

class OutputMode (Enum):
	HEAD = 1
	TAIL = 2
	OTHER = 3

LAST_LOG_PRINTED = None
OUTPUT_MODE = OutputMode.TAIL
SHOW_HEADERS = True
SUCCINCT_MODE = False
TRUNCATE_MODE = False

try:
	SCREENCOLS, SCREENROWS = os.get_terminal_size()
except OSError:
	# Probably being piped so no terminal
	SCREENCOLS = 255
	SCREENROWS = 48

MAXREADLEN = 1048576*10

# Default deployment paths (Windows paths will have forward slashes converted)
DEF_BASE_PATHS = {
	'Darwin': '/Library/FileMaker Server',
	'Linux': '/opt/FileMaker/FileMaker Server',
	'Windows': 'C:/Programs/FileMaker/FileMaker Server'
}

# This may get overridden by user option,
BASE_PATH = DEF_BASE_PATHS [platform.system()]

DBS_CONFIG_PATH = BASE_PATH + '/Data/Preferences/dbs_config.xml'

LOG_ALIAS = {
	('acc', 'access'),
	('adm', 'admin'),
	('ada', 'adminapi'),
	('cat', 'catalina'),
	('cli', 'clientstats'),
	('dap', 'dapi'),
	('eve', 'event'),
	('fad', 'fmsadmindebug'),
	('fcw', 'fmscwpc'),
	('fcl', 'fmscwpcli'),
	('fms', 'fmsdebug'),
	('fgp', 'fmsgetpasskeydebug'),
	('fhd', 'fmshdebug'),	# helper debug?
	('fib', 'fmsibdebug')	# incremental backup
}

# path: location of the log file, starting at the base path
# lghd: True if the log file has a header line at start of file
# head: header to use if not succinct mode
# tbst: list of tab stops to use for columns, or just a single number for tab size
# shed: succinct version of header (may not be present)
# shtb: succinct version of tab stops (may not be present)

LOG_SPECS_BASE = {
	'access': {
		'path': 'Logs/Access.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        2025-09-15 01:12:45.831 -0700  Information  228   some-dev.filemaker.beezwax.net         The previous log file reached maximum size, and was renamed to "Access-old.log".
		'head': 'TIMESTAMP                       LEVEL        CODE  HOST                                  MESSAGE',
		'tbst': [32,45,51,89],
		#        2025-09-15 01:12:45.831  Information  228   The previous log file reached maximum size, and was renamed to "Access-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45]
	},
	'admin': {
		'path': 'Admin/FAC/logs/fac.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-17 14:29:56 -0700  Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		#			2022-05-17 14:30:00 -0700 - error:  fmi   127.0.0.1   notifications  general   n/a   "New system notification generated, type: CPU_USAGE_EXCEED_HARD_LIMIT"
		# Only uses tabs with regular messages (eg, not error or warn) after timestamp.
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},
	'adminapi': {
		'path': 'Admin/FAC/logs/fac1.log',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-24 14:04:30 -0700 - error:   fmi   127.0.0.1   fmsadminapi   general   3   "Get worker list failed."
		#			2022-05-24 19:32:51 -0700       Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		# Only uses tab after timestamp with regular messages (eg, not error or warn)
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},

	'catalina': {
		'path': 'Web Publishing/publishing-engine/jwpc-tomcat/logs/catalina.*',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        09-Dec-2025 09:31:08.332 WARNING [main] org.apache.tomcat.util.digester.SetPropertiesRule.begin Match [Server/Listener] failed to set property [AWTThreadProtection] to [true]
		#			will also include backtraces
		'head': 'Catalina.*.log                 NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
	},


	'clientstats': {
		'path': 'Logs/ClientStats.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-16 15:46:18.054 -0700  37781   8559   209   0     46442     0    28    Xeronthia Shilnow (XS ETMD6M) [255.143.244.179]
		'head': 'ClientStats.log                NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
		#			2025-10-16 15:46:18.054  37781    8559    209     0    46442    0     28   Xeronthia Shilnow (XS ETMD6M) [255.143.144.79]
		'shed': 'ClientStats.log                     NET BYTES  NET BYTES CALLS     CALLS     TIME      TIME      TIME\n' + \
				  'TIMESTAMP                IN         OUT       COMPLETE  IN PROG   ELAPSED   WAIT      I/O       CLIENT NAME',
		'shtb': [26,37,48,59,70,81,92,103]
	},
	
	'dapi': {
		'path': 'Logs/fmdapi.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-27 11:08:10.055 -0700  4101   ERROR	250.130.228.236  some-user-name   POST  Script Error -- Script File: 'filename', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		# Size at end (re-arrange columns?). Rarely a 4 digit error code.
		'head': 'TIMESTAMP	                CODE   LEVEL   HOST            USER              HTTP  MESSAGE  SIZE',
		'tbst': [32,39,47,63,81,87],
		#			2025-08-27 11:08:10.055  301   ERROR  some-user-name   POST  Script Error -- Script File: 'Tool', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		'shed': 'TIMESTAMP	             CODE  LEVEL  USER             HTTP  MESSAGE  SIZE',
		'shtb': [26,32,39,56,62]
	},
	
	'event': {
		'path': 'Logs/Event.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-18 23:15:30.125 -0700  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'head': 'TIMESTAMP                ZONE  LEVEL        CODE  HOST                            MESSAGE',
		'tbst': [31,44,50,82],
		#			2025-08-18 23:15:30.125  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45,77]
	},
	
	'fmsadmindebug': {
		'path': 'Database Server/bin/fmsadminDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmsasedebug': {
		'path': 'Database Server/bin/fmsaseDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpc': {
		'path': 'Database Server/bin/fmscwpc.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpcli': {
		'path': 'Database Server/bin/fmscwpcli.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},

	'fmsgetpasskeydebug': {
		'path': 'Database Server/bin/fmsgetpasskeyDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmshdebug': {
		'path': 'Database Server/bin/fmshDebug.log',
		'lghd': True,
			#		2024-10-21 09:55:47.101 -0700 [HelperApp] Debug::DumpBitSettings() feature(s) enabled: [ AlwaysPrint | Assert | ForceOutput | Backup | UploadDownload | Server | ServerCommands | ServerComponents | ServerDependencies | FMSECLI | NotificationsStackCrawls | Anchors | BPlusTree | FileBox | LicenseServer | CalcEngine | Add-ons | OData | TransactSupport | AdminAPI | CertVerify | CurrentThread | Consolidator | DownloadService | WIP | EditOtherClientLayout | CLI | ThreadedSorting | DisableServerSideSorting | DisableMemoryKeyCmpIndexing | PurgeTempDB | DisableSharingLockOnServer | PersistentData | DisableServerSideSummary | SupportNestedPSOS ]
			#		2024-10-21 09:55:47.117 -0700 [HelperApp] AdminMgrConfigFile() installation directory = /opt/FileMaker/FileMaker Server/, config file = Admin/conf/deployment.xml, encrypted = false
		'head': None,
		'tbst': 8	# replace any tabs with two spaces
	},
	
	'fmsibdebug': {
		'path': 'Database Server/bin/fmsibDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmslogdebug': {
		'path': 'Database Server/bin/fmslogDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmwipd': {
		'path': 'Database Server/bin/fmwipd.log',
		'lghd': False,
		'tbst': 8
	},

	'loadschedules': {
		'path': 'Logs/LoadSchedules.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'odata': {
		'path': 'Logs/fmodata.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#		  '2025-10-14T13:01:31.232452-08:00  0     INFO   170.255.255.218   GET   /fmi/odata/v4	 75'
		'head': 'TIMESTAMP                         CODE  LEVEL  HOST                 OP      ENDPOINT               SIZE',
		'tbst': [34,40,47,66,68,76,99],  # 'size' value will be padded on end
		#		  '2025-10-14T13:01:31.232452  0     INFO   GET   /fmi/odata/v4	 75'
		'shed': 'TIMESTAMP                   CODE  LEVEL  OP    ENDPOINT  SIZE',
		'shtb': [29,35,48]
	},
	
	'odatadebug': {
		'path': 'Database Server/bin/fmodataDebug.log',
		'lghd': False,
		'tbst': 8
	},
	
	'scriptevent': {
		'path': 'Logs/scriptEvent.log',
		'lghd': False,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-11 03:00:26.470 -0700  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'head': 'TIMESTAMP                ZONE  CODE  MESSAGE',
		'tbst': [32,38],
		#			2025-08-11 03:00:26.470  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'shtb': [26,32],
	},
	
	'stats': {
		'path': 'Logs/Stats.log',
		'lghd': True,
		#			---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------
		#			2025-10-17 17:54:42.335 -0700	0	14	11	0	98	0	0	1	0	0	0	2	0	546	40	81	0
		'head': 'Stats.log                      NET      NET       DISK       DISK       CACHE   CACHE     CLIENTS  OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                ZONE  KBs IN   KBs OUT   KBs READ   KBs WRITE  HIT %   UNSAVD %  PRO      DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'tbst': [31,40,50,61,72,80,90,99,105,114,123,132,142,150,159,168,177],
		'shed': 'Stats.log                NET      NET       DISK       DISK        CACHE   CACHE     PRO      OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                KB/s In  KB/s OUT  KB/s READ  KB/s WRITE  HIT %   UNSAVD %  CLIENTS  DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'shtb': [26,35,45,56,68,76,86,95,101,110,119,128,138,146,155,164,173]
	},

	'topcall': {
		'path': 'Logs/TopCallStats.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-31 22:12:31.419 -0700   166630.877193  166635.541004  4663811   Query (Find)                     Tool::table(181)::field definitions(356)     509        33         4663811    0        235659   tool-beezwax-net (172.30.8.236) [172.30.8.236]
		'head': 'TopCallStats.log               TIME           TIME           TOTAL                                                                                    NET BYTES  NET BYTES  TIME       TIME     TIME\n' + \
				  'TIMESTAMP                      START          END            ELAPSED   OPERATION                         TARGET                                       IN         OUT        ELAPSED    WAIT     I/O       CLIENT NAME',
		'tbst': [31,46,61,71,105,150,161,172,183,192,202],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},
	
	'trimlog': {
		'path': 'Database Server/bin/trimlog.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'wpe': {
		'path': 'Logs/wpe0.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2023-09-13 11:50:57 -0700  INFO	-	-	User [WebDirect-ABC8F](nnnnnnnnn_n@beezwax.net) has been signed out from database Tool.
		#			2025-08-04 08:06:10 -0700  172.130.211.135  127.0.0.1:57874  -  -  INFO  -  -  FileMaker WebDirect is enabled.
		'head': 'TIMESTAMP                  HOSTNAME         CLIENT           ACCOUNT                     MODULE_TYPE  SEVERITY  ERROR  BYTES     MESSAGE',
		'tbst': [27,44,61,89,102,112,119,129],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},

	'wpedebug': {
		'path': 'Logs/wpe_debug.log',
		'lghd': False,
		'tbst': 8
	}
}

LOG_SPECS_DARWIN = {
	'install': {
		'path': 'Logs/install.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'syslog': {
		'path': '!/usr/bin/log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stderr': {
		'path': 'Logs/stderr',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdout': {
		'path': 'Logs/stdout',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_LINUX = {
	'fmshelper': {
		'path': 'Logs/fmshelper.log',
		'lghd': True,
			# This log has no consistent format
			#		2025-08-03 20:12:38.182 -0700   Log file /opt/FileMaker/FileMaker Server/Logs/fmshelper.log size: 478 bytes (0 MB), threshold ratio: 0
			#		2025-08-03 20:12:38.185 -0700 === stopSystemWebServer()
			#		(Use `facstart.sh --trace-warnings ...` to show where the warning was created)
			#		Thrift: Sun Aug  3 20:12:47 2025 TNonblockingServer: Serving with 5 io threads.
			#		127.0.0.1 POST /fmi/admin/internal/v1/dbs-notification/xPR2AgRM1TODanCZ56eikiYXcbzvTDdtLIbd9Avs3Z4kuxie - - - ms
			#		Aug 03, 2025 8:12:53 PM org.apache.jasper.servlet.TldScanner scanJars
			#		2025/08/03 20:39:27.0128: [ 2525]:    TRACE:       mongoc: ENTRY: _mongoc_linux_distro_scanner_get_distro():389
		'head': None,
		'tbst': []	# replace any tabs with two spaces
	},

	'nginxaccess': {
		'path': 'NginxServer/logs/nginx-access.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'nginxerror': {
		'path': 'NginxServer/logs/nginx-error.log',
		'lghd': True,  
		'head': None,
		'tbst': 8
	},

	'stderrserverscripting': {
		'path': 'Logs/StdErrServerScripting.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdoutserverscripting': {
		'path': 'Logs/StdOutServerScripting.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_APACHE = {
	'httpaccess': {
		'path': 'HTTPServer/logs/access_log.*',
		'lghd': True,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'REQUESTER        STATUS  LENGTH  IDENTITY      USER          TIMESTAMP                   MESSAGE',
		'tbst': [              17,     25,   33,             47,           61,                          89]
	},
	'httpdctlerr': {
		'path': 'HTTPServer/logs/httpdctl.err',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpdctlout': {
		'path': 'HTTPServer/logs/httpdctl.out',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httperror': {
		'path': 'HTTPServer/logs/error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslaccess': {
		'path': 'HTTPServer/logs/ssl_access_log.*',
		'lghd': True,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'REQUESTER        STATUS  LENGTH  IDENTITY      USER          TIMESTAMP                   MESSAGE',
		'tbst': [              17,     25,   33,             47,           61,                          89]
	},
	'httpsslerror': {
		'path': 'HTTPServer/logs/ssl_error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslrequest': {
		'path': 'HTTPServer/logs/ssl_request_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST             VERSION  CIPHER                        OP   PATH',
		'tbst': [                          29,              46,      55,                           85,                      99]
	}
}

LOG_SPECS_DARWIN.update (LOG_SPECS_BASE)
LOG_SPECS_DARWIN.update (LOG_SPECS_APACHE)

LOG_SPECS_LINUX.update (LOG_SPECS_BASE)
LOG_SPECS_LINUX.update (LOG_SPECS_APACHE)	# although not default, Apache can be used on Linux

LOG_SPECS_WINDOWS = LOG_SPECS_BASE

LOG_SPECS_ALL = {
	'Darwin': LOG_SPECS_DARWIN,
	'Linux': LOG_SPECS_LINUX,
	'Windows': LOG_SPECS_WINDOWS
}

LOG_SPECS = LOG_SPECS_ALL [platform.system()]


LOG_CHOICES = list (LOG_SPECS.keys())
LOG_CHOICES.sort()

# Even if using RawDescriptionHelpFormatter, argparse tries hard to strip leading or trailing whitespace.
HELP_EPILOGUE = """Log names supported on this platform are:\n\n  """ + '\n  '.join (LOG_CHOICES) + '\n '

ALL_CHOICES = LOG_CHOICES  # TODO: need to add in choices that aren't log names?

CLARIS_CONFIG = """
{
	"AlwaysPrint": true,
	"ForceOutput": true,
	"Server": true
}
"""

class TerminalColors:
	PURPLE = '\033[95m'
	CYAN = '\033[96m'
	DARKCYAN = '\033[36m'
	BLUE = '\033[94m'
	GREEN = '\033[92m'
	LIGHT_GRAY = '\033[37m'
	YELLOW = '\033[93m'
	RED = '\033[91m'
	BOLD = '\033[1m'
	UNDERLINE = '\033[4m'
	END = '\033[0m'


#
#	calc_row_metrics
#

def calc_row_metrics (num_logs, log_lines_count_val) -> tuple:
	"""
	Using the user supplied quantity,the current terminal screen size,
	and the number of logs we have to print, determine the number of rows
	we should attempt to print.
	"""

	screen_lines_count = 0
	num_mode = None

	if log_lines_count_val.count ('s'):
		screens_num_str = log_lines_count_val.replace('s','')
		num_mode = 's'

		if len (screens_num_str) == 0:
			screens_num = 1		# just an 's' by itself counts as 1'
		try:
			screens_num = int (screens_num_str)
		except ValueError:
			print ("Error: invalid number for number parameter")
			return (-1,-1)
			
		screen_lines_count = max ([SCREENROWS * screens_num - 1, 6])

		if num_logs == 1:
			lines_per_log = screen_lines_count
		else:
			lines_per_log = screen_lines_count // num_logs

	else:
		try:
			lines_per_log = int (log_lines_count_val)
		except ValueError:
			print ('Error: invalid value of "{log_lines_count_val}" for number parameter')
			return (-1,-1,-1)
		
	return (num_mode,lines_per_log,screen_lines_count)


#
#	c h e c k _ e n d p o i n t _ s t a t u s
#

def check_endpoint_status (name: str, host: str, endpoint: str, expectedStatus: int, method='GET', port=80, useSSL=False) -> bool:
	"""
	Check for expected status result for the given endpoint.
	"""
	result_flag = False
	print ('Checking', name, end='')
	if useSSL:
		conn = http.client.HTTPSConnection (host, port, context=ssl._create_unverified_context(), timeout=4)
	else:
		conn = http.client.HTTPConnection (host, port, timeout=4)
	try:
		conn.request(method, endpoint)
		response = conn.getresponse()
		if response.status in (200,202) or response.status == expectedStatus:
			# In some cases we get a 200 or similar, but there is an error in the XML.
			body = response.read(500).decode('utf-8')
			if body.startswith ('<?xml '):
				match = re.search(r'<error code="(\d+)', body)
				if match:
					if int (match.group(1)) == expectedStatus:
						print (': Responding')
					else:
						print (': Error %s' % match.group(1))
				else:
					print (': Error, Result Missing')
			else:
				print (': Responding')
			result_flag = True
		else:
			print (':', response.reason)
	except http.client.HTTPException as e:
		print (':', e)
	except Exception as e:
		print (':', e)
	finally:
		conn.close()
	return result_flag


#
#	p r i n t _ t l s _ v e r s i o n
#

def print_tls_version (host: str, port=443) -> bool:
	"""
	Create connection to server and print the TLS version that was used.
	"""
	
	return_flag = False
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout (4)
		ssock = context.wrap_socket(sock, server_hostname=host)
		ssock.connect ((host, port)) # we are ignoring the connection value
		return_flag = True
		print('SSL/TLS version:', ssock.version())
	except ssl.SSLError as e:
		print ('  ', e)
	finally:
		sock.close()
	
	return return_flag

        
def check_tcp_status (name: str, host: str, port):
	"""
	Print the result of testing if TCP connection can be opened to the given host's port.
	"""
	result_flag = False
	print ('Checking', name, end='')
	
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout(4)	# 4 seconds
		result = sock.connect_ex((host,port))
		if result == 0:
			print (': Responding')
		else:
			print (f': Error {result}, Could not open port {port}')
	except socket.gaierror:
		print (f': Could not resolve host "{host}"')
	finally:
		sock.close()


#
#	c h e c k _ c o n n e c t i v i t y
#

def check_connectivity() -> str:
	"""
	Check both internal and external interfaces for a response from the DAPI endpoint.
	"""
	
	intern_addr = '127.0.0.1'
	extern_addr = get_local_ip()

	print ()
	print ('Internal Interface:', intern_addr)
	print ('External Interface:', extern_addr)
	print ()
	print ('=== Web SSL Usage ===')
	try:
		print_tls_version (extern_addr, 443)
	except TimeoutError:
		print ('HTTPS connection timed out')
	else:
		print_certificate_info ('External Web', extern_addr)
	print ()
	print ('=== Connectivity Tests ===')
	
	# FMP/FMGO CLIENT
	check_tcp_status ('FMP Client - Internal', intern_addr, 5003)
	check_tcp_status ('FMP Client - External', extern_addr, 5003)
	
	# ADMIN API
	adminEndpoint = '/fmi/admin/api/v2/server/metadata'
	check_endpoint_status ('Admin API - Internal', intern_addr, adminEndpoint, 400, method='POST', port=16001)
	check_endpoint_status ('Admin API - External', extern_addr, adminEndpoint, 400, method='POST', port=443, useSSL=True)
	
	# ADMIN CONSOLE
	fac_endpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Internal', intern_addr, fac_endpoint, 200, port=16001)
	check_endpoint_status ('Admin Console - External', extern_addr, fac_endpoint, 200, port=443, useSSL=True)
	
	# ADMIN CONSOLE - WORKER
	fac_endpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Worker - Internal', intern_addr, fac_endpoint, 200, port=16003)
	
	# CATALINA/CWP
	check_tcp_status ('CWP - Port 9889', intern_addr, 9889)
	xmlEndpoint = '/fmi/xml'
	check_endpoint_status ('CWP - XML - Internal', intern_addr, xmlEndpoint, 954, port=16021)
	check_endpoint_status ('CWP - XML - External', extern_addr, xmlEndpoint, 954, port=443, useSSL=True)
	check_endpoint_status ('CWP - wpem - Internal', intern_addr, 'fmswpem/', 400, port=16002)
	check_endpoint_status ('CWP - wpem - External', extern_addr, '/fmi/mwpem/', 400, port=443, useSSL=True)

	# DATA API
	dapiEndpoint = '/fmi/data/vLatest/productInfo'
	check_endpoint_status ('Data API - Internal', intern_addr, dapiEndpoint, 200, port=3000)
	check_endpoint_status ('Data API - External', extern_addr, dapiEndpoint, 200, port=443, useSSL=True)
	
	# JDBC
	check_tcp_status ('JDBC - Internal', intern_addr, 2399)
	check_tcp_status ('JDBC - External', extern_addr, 2399)
	
	# ODATA API
	odataEndpoint = '/fmi/odata/v4'
	check_endpoint_status ('OData API - Internal', intern_addr, odataEndpoint, 401, port=3001)
	check_endpoint_status ('OData API - External', extern_addr, odataEndpoint, 401, port=443, useSSL=True)

	# docws/fmws handler
	xmlEndpoint = '/fmws/serverinfo'
	check_endpoint_status ('CWP - XML - Internal', intern_addr, xmlEndpoint, 954, port=1895)
	check_endpoint_status ('CWP - XML - External', extern_addr, xmlEndpoint, 954, port=443, useSSL=True)	# this may get removed

	# WEBD
	webdEndpoint = '/fmi/webd/'
	check_endpoint_status ('WebDirect - Internal', intern_addr, webdEndpoint, 200, port=16021)
	check_endpoint_status ('WebDirect - External', extern_addr, webdEndpoint, 200, port=443, useSSL=True)

	# alternate: curl -vk -X DELETE https://127.0.0.1/fmi/admin/api/v2/user/auth/abcdef0123456789
	print ()


#
#	c h e c k _ f i l e _ v a l i d i t y
#

def check_file_validity (path: str):
	'''
	Check whether the a given file exists, readable and is a file.
	Return None if no issue found, otherwise return an error message.
	'''
	if not os.access(path, os.F_OK):
		raise FileCheckError (f"'{path}' does not exist")
	if not os.access(path, os.R_OK):
		raise FileCheckError (f"'{path}' is not readable")
	if os.path.isdir(path):
		raise FileCheckError (f"'{path}' is a directory")

#
#	c l a r i s _ c o n f i g _ r e m o v e
#

def claris_config_remove(target_dir: str) -> bool:
	"""
	Remove ClarisConfig.json from target directory.
	"""

	config_path = os.path.join(target_dir, "ClarisConfig.json")
	print ()

	try:
		os.remove(config_path)
	except FileNotFoundError:
		print ('No ClarisConfig.json file was found to remove.')
		print ()
		return False
	except PermissionError:
		print("Error: Could not remove", config_path)
		print ('Adjust permissions or run fmslogs with elevated privileges.')
		print ()
		return False

	print ('The ClarisConfig.json file has been removed.')
	print ('Restart FMS to apply the change.')
	print ()
	return True

#
#	c l a r i s _ c o n f i g _ w r i t e
#

def claris_config_write(target_dir: str) -> bool:
	
	"""
	Write CLARIS_CONFIG text to a ClarisConfig.json file in target directory.
	If the file already exists, attempt to move it aside (renames with .bak).
	"""

	config_path = os.path.join(target_dir, 'ClarisConfig.json')
	print ()

	try:
		try:
			# If there's an existing config file, attempt to move it aside.
			backup_path = config_path + '.bak'
			shutil.move(config_path, backup_path)
		except FileNotFoundError:
			pass
		except FileExistsError:
			pass

		with open(config_path, 'w', encoding='utf-8') as f:
			f.write(CLARIS_CONFIG)
		
		print ('The ClarisConfig.json file has been created.')
		print ('Restart FMS to apply the change.')
		print ()
		return True
	except PermissionError:
		print("Error: Could not write to", config_path)
		print ('Adjust permissions or run fmslogs with elevated privileges.')
		print ()
		return False
	
	return True


#
#	c o m p i l e _ f i l t e r
#

def compile_filter(regex: str) -> bool:
	"""
	Compiles the text into the global text regex that will be used
	for filtering messages. If there's an error in the express,
	prints and error and returns False.
	"""
	
	global FILTER_REGEX
	is_valid = False
	
	try:
		FILTER_REGEX = re.compile(regex)
		is_valid = True
	except re.error as e:       # aliased to PatternError as of 3.13
		print(f"Error: bad regex expression: {e}\n")
	
	return is_valid

#
#	c o n v e r t _ f i l e m a k e r _ p a t h
#

def convert_filemaker_path (fm_path: str) -> str:
	"""
	Convert a FileMaker path (filemac:, filelinux:, filewin:) to a POSIX style path that can be used in Python.
	"""

	# Example paths:
	"""
	filemac:/internal-vol/Library/FileMaker Server/Data/Databases/	# drive name used as prefix
	filelinux:/opt/FileMaker/FileMaker Server/Data/Databases/
	filewin:/C:/Program Files/FileMaker/FileMaker Server/Data/Databases/
	"""

	if fm_path:
		if fm_path.startswith ('filemac:/'):
			fm_path = fm_path.replace ('filemac:/', '/Volumes/')
			mac_path_old = pathlib.Path (fm_path)
			if mac_path_old.exists():
				return str (mac_path_old)
			else:
				# Rebuild path without drive name and return that if it exists.
				mac_path_new = pathlib.Path().joinpath ('/', *mac_path_old.parts[3:])
				if mac_path_new.exists():
					return str (mac_path_new)
				else:
					return str (mac_path_old)
	
		elif fm_path.startswith ('filelinux:/'):
			return fm_path.replace ('filelinux:/', '/')
		elif fm_path.startswith ('filewin:/'):
			win_path = fm_path.replace ('filewin:/', '')
			win_path = win_path.replace ('/', '\\')
			return win_path
		else:
			return fm_path

	return None


#
#	e d i t _ l o g
#

def edit_log (log_name: str):

	# TODO: needs Windows version for first & third options
	# TODO: use vi as second fallback if no nano?
	# TODO: open in GUI editor if available
	
	# Below will return None if syslog or glob can't find file.
	log_path = get_log_path (log_name)
	exit_code = 0
	
	if log_path:
		if 'EDITOR' in os.environ:
			exit_code = subprocess.call ('$EDITOR "' + log_path + '"', shell=True)
		elif platform.system() == 'Darwin' and subprocess.call (['/usr/bin/pgrep','loginwindow']) == 0:
			exit_code = subprocess.call (['/usr/bin/open', '-e', log_path])
		elif platform.system() == 'Windows':
			exit_code = subprocess.call (['notepad', log_path])		
		else:
			exit_code = subprocess.call (['/usr/bin/nano', log_path])
	
	sys.exit (exit_code)


#
#	e x p a n d _ t a b s _ f o r _ l i n e
#

def expand_tabs_for_line (log_name: str, line: str) -> str:
	"""
	Expands tabs in the string `line` using the given tabstops.
	`tabstops` can be a list of column positions (e.g., [4, 8, 12]) or a single integer for fixed tab width.
	TODO: truncate if over column width?
	"""
	
	try:
		if SUCCINCT_MODE:
			tabstops = LOG_SPECS [log_name]['shtb']
		else:
			tabstops = LOG_SPECS [log_name]['tbst']
	except KeyError:
		tabstops = 8
	
	if isinstance(tabstops, int):
		# All tab stops are the same size.
		return line.expandtabs(tabstops)

	if log_name in ['admin','adminapi']:
		# This log doesn't use tabs, but instead uses 2 or 3 spaces to separate columns.
		respace = re.compile (r'  +')
		line = respace.sub ('\t', line)
	
	if log_name in ['httpaccess','httpsslaccess']:
		#                 host   iden user  time    zone  requ  stat  size
		match = re.match (r'(.*) (.*) (.*) \[(.*) (.*)\] "(.*)" (\d*) (\d*)', line)
		line = match.group(1) + '\t' + match.group(7) + '\t' + match.group(8) + '\t' + match.group(2) + '\t' + match.group(3) + '\t' + match.group(4) + ' ' + match.group(5) + '\t' + match.group(6) + '\n'
	
	if log_name in ['httpsslrequest']:
		#                   timest zone   host vers ciph oppath len
		match = re.match (r'\[(.*) (.*)\] (.*) (.*) (.*) "(.*)" (\d*)', line)
		line = match.group (1) + ' ' + match.group(2) + '\t' + match.group(3) + '\t' + match.group(4) + '\t' + match.group(5) + '\t' + match.group(6) + '\n'

	result = []

	parts = line.split('\t')

	col = 0
	tab_iter = iter(tabstops)
	next_tab = next(tab_iter, None)
	for i, part in enumerate(parts):
		result.append(part)
		col += len(part)
		if i < len(parts) - 1:
			if next_tab is not None and col < next_tab:
				spaces = next_tab - col
				result.append(' ' * spaces)
				col = next_tab
				next_tab = next(tab_iter, None)
			else:
				result.append('  ')	# pad two spaces if no stop specified
				col += 1

	if TRUNCATE_MODE:
		line = ''.join(result)
		if len (line) > SCREENCOLS:
			return line [:SCREENCOLS-1] + "â€¦" + '\n'
		else:
			return line
	else:
		return ''.join(result)

class FileCheckError(Exception):
	def __init__(self, msg):
		self.message = msg
	def __str__(self):
		return self.message


#
#	f i n d _ f i r s t _ t i m e s t a m p
#

def find_first_timestamp (file_path: str, timestamp: datetime.datetime) -> int:
	"""
	Scan file until the first log timestamp equal or greater than the search
	timestamp is found, returning the line number (base 1) of matching line.
	Note that a few logs don't emit timestamps consistently. 
	If a match is never found -1 is returned.
	"""

	line_result = -1
	line_num = 0

	while True:
		line_ts = None
		line_num += 1
		line = linecache.getline (file_path, line_num)
		if line == '': break
		# Sniff the line to guess the date format.
		try:
			if line[4] == '-' and line [24] == '-':
				# Access, ClientStats, Event, Stats, etc.
				# 2025-10-27 04:13:24.101 -0700	Information	228	tool.beezwax.net	The previous log file reached maximum size, and was renamed to "Access-old.log".
				line_ts = datetime.datetime.fromisoformat(line [:23])
			elif line [4] == '/' and line [24] == ':':
				# 2025/10/25 17:49:09.0162
				line_ts = datetime.strptime(line [:23], "%Y/%m/%d %H:%M:%S.%f")
			elif line [6] == ',' and line [22] in 'APM':
				# Sep 11, 2025 12:40:52 PM org.atmosphere.cpr.AtmosphereFramework addInterceptorToAllWrappers
				# Oct 22, 2025 2:16:56 PM org.atmosphere.util.IOUtils guestRawServletPath
				line_ts = datetime.datetime.strptime(line [:24].rstrip(), '%b %d, %Y %I:%M:%S %p')
			elif line [:7] == 'Thrift:':
				# Thrift: Sat Jun  7 10:47:03 2025
				line_ts = datetime.datetime.strptime (line [8:32], '%a %b %d %H:%M:%S %Y')
			else:
				continue
		
		# Skip over any lines that have too few characters (very few) or where we can't evaluate date.
		
		except IndexError:
			continue
		#except ValueError:
		#	continue

		# If we reached the timestamp, go into next while loop to match by text value.
		if line_ts and line_ts >= timestamp:
			line_result = line_num
			break
	
	return line_result

def fmsadmin_toggle_log (fm_logname: str, enable_flag: bool, user: str, password: str) -> bool:
	"""
	Enable or disable a FileMaker log using its fmsadmin command.
	Returns true if the operation succeeded.
	TODO: should have a prompt to allow user to enter password.
	"""

	if fm_logname == 'debug':
		# This is a special case where we write or remove the ClarisConfig.json file.
		if enable_flag:
			claris_config_write (get_fms_root_path())
		else:
			claris_config_remove (get_fms_root_path())
		return True
	
	if enable_flag:
		verb_str = 'enable'
	else:
		verb_str = 'disable'

	fmsadmin_proc = subprocess.run(
		['fmsadmin', verb_str, fm_logname],
		capture_output=True,
		text=True,
		check=False)

	print (fmsadmin_proc.stdout)
	return (fmsadmin_proc.returncode == 0)

	
def follow_file(some_file):
	"""
	was tail_F
	Capture output as it is added to the file.
	"""
	# https://gist.github.com/pylixm/e6bd4f5456740c12e462eecbc66692fb # tail/follow a file
	
	first_call = True
	while True:
		try:
			with open(some_file) as input:
				if first_call:
					input.seek(0, 2)
					first_call = False
				latest_data = input.read()
				while True:
					if '\n' not in latest_data:
						latest_data += input.read()
						if '\n' not in latest_data:
							yield ''
							if not os.path.isfile(some_file):
								break
							continue
					latest_lines = latest_data.split('\n')
					if latest_data[-1] != '\n':
						latest_data = latest_lines[-1]
					else:
						latest_data = input.read()
					for line in latest_lines[:-1]:
						yield line + '\n'
		except IOError:
			yield ''


#
#	g e t _ f o l d e r _ s i z e
#

def get_folder_size(path='.', exclude_dir=None) -> int:

	total = 0
	if path:
		try:
			for entry in os.scandir(path):
				if entry.is_file():
					total += entry.stat().st_size
				elif entry.is_dir():
					if entry.name != exclude_dir:
						total += get_folder_size(entry.path, exclude_dir=exclude_dir)
		except PermissionError:
			print ('Error: Permission denied for', path)
		except FileNotFoundError:
			return -1

	return total

#
#	g e t _ s u b f o l d e r _ n a m e s
#

def get_subfolder_names (folder: str) -> list:
	"""
	Return a list of all subfolder names in the given folder.
	Does not recurse into subfolders.
	"""
	try:
		return [f.name for f in os.scandir(folder) if f.is_dir()]
	except FileNotFoundError:
		return None


def get_db_directories() -> tuple:
	"""
	Return a tuple of all four possible database directory paths and two possible container paths by extracting
	the locations in the FMS Event.log file, and if necessary, the -old log.
	There is a possibility that this information may not be present in the either log file,
	in which case the default path will be None.
	"""
	
	additional_path1 = None
	remote_container_path1 = None
	additional_path2 = None
	remote_container_path2 = None

	default_pathRE = re.compile (r'Default database location: (file.*)')
	secure_pathRE = re.compile (r'Secure database folder enabled: (file.*)')
	additional_path1RE = re.compile (r'Additional database folder \[1\] enabled: (file.*)')
	additional_path2RE = re.compile (r'Additional database folder \[2\] enabled: (file.*)')
	remote_container_path1RE = re.compile (r'Separate remote container folder \[1\] enabled, .*: (.*)')
	remote_container_path2RE = re.compile (r'Separate remote container folder \[2\] enabled, .*: (.*)')

	"""
	Default database location: filemac:/internal/Library/FileMaker Server/Data/Databases/
	Secure database folder enabled: filemac:/internal/Library/FileMaker Server/Data/Secure/
	Additional database folder [1] enabled: filemac:/internal/Library/FileMaker Server/Data/DBs/
	Additional database folder [2] disabled.
	Separate remote container folder [1] enabled, without backups: filelinux:/FileMakerData/Containers/
	Additional database folder [2] disabled.
	"""

	event_log_path = get_log_path ('event')
	default_path = scan_file_last_match (event_log_path, default_pathRE)

	if default_path is None:
		# Nothing in current log file, see if we can find in -old log.
		event_log_path = event_log_path.replace ('.log', '-old.log')
		default_path = scan_file_last_match (event_log_path, default_pathRE)
	default_path = convert_filemaker_path (default_path)

	secure_path = scan_file_last_match (event_log_path, secure_pathRE)
	secure_path = convert_filemaker_path (secure_path)

	# These paths are optional.
	additional_path1 = scan_file_last_match (event_log_path, additional_path1RE)
	if additional_path1:
		additional_path1 = convert_filemaker_path (additional_path1)
		remote_container_path1 = scan_file_last_match (event_log_path, remote_container_path1RE)
		if remote_container_path1:
			remote_container_path1 = convert_filemaker_path (remote_container_path1)

		additional_path2 = scan_file_last_match (event_log_path, additional_path2RE)
		if additional_path2:
			additional_path2 = convert_filemaker_path (additional_path2)
			remote_container_path2 = scan_file_last_match (event_log_path, remote_container_path2RE)
			if remote_container_path2:
				remote_container_path2 = convert_filemaker_path (remote_container_path2)
			
	return (default_path, secure_path, additional_path1, additional_path2, remote_container_path1, remote_container_path2)


#
#	g e t _ l o c a l _ i p
#

def get_local_ip() -> str:
	s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
	try:
		s.settimeout (4)
		# doesn't even have to be reachable
		s.connect(('8.8.8.8', 1))
		addr = s.getsockname()[0]
	except Exception as e:
		print (e)
		addr = '127.0.0.1'
	finally:
		s.close()
	return addr

#
#	g e t _ n e t w o r k _ a d d r e s s
#

def get_network_address() -> str:
	"""
	Extract the last logged network addresses from FMS' Event.log.
	Result may contain multiple space delimited addresses.
	"""

	eventPath = get_log_path ('event')
	match = scan_file_last_match (eventPath, r'Network address.*: (.*)')
	if match == None:
		return 'No matches in current Event.log'

	return match


#
#	g e t _ l o g _ p a t h
#

def get_log_path (log: str) -> str:
	# TODO: convert Windows paths
	#print ('basePath:', BASE_PATH)
	#print ('log_path', LOG_PATHS [log])
	pathSuffix = LOG_SPECS [log]['path']

	if pathSuffix[0] == '!':
		full_path = None
	elif pathSuffix[0] != '/':
		full_path = BASE_PATH + '/' + LOG_SPECS[log]['path']
	else:
		full_path = LOG_SPECS[log]['path']

	if '*' in pathSuffix:
		path_list = glob.glob (full_path)
		if path_list:
			path_list.sort(reverse=True) # so that newest file is first
			full_path = path_list[0]
		else:
			full_path = None
	
	return full_path


def get_file_timestamps (path: str) -> tuple:
	"""Return a file's creation and modification timestamps"""
	return (os.path.getmtime(path), pathlib.Path(path).stat().st_mtime)

	
def get_fms_version() -> str:
	"""
	Fetch the version of FileMaker Server currently installed.
	"""
	# TODO: handle if not installed
	
	if platform.system() == 'Darwin':
		fmsout_lines = subprocess.run(
			['/usr/sbin/pkgutil', '--pkg-info','com.filemaker.fms.worker.pkg'],
			capture_output=True,
			text=True,
			check=False).stdout.split('\n')
		return fmsout_lines[1].split(': ')[1]
		
	elif platform.system() == 'Linux':
		fmsout_lines = subprocess.run(['/usr/bin/apt-cache', 'policy', 'filemaker-server'], capture_output=True, text=True).stdout.split('\n')
		return fmsout_lines[1].split(': ')[1]
	
	elif platform.system() == 'Windows':
		return 'not implemented'
	
	return 'Unknown platform'


def get_vaadin_version() -> str:
	"""
	Get the Vaadin version based off of the name of the installed vaadin-server lib file.
	"""
	
	# Only match server-# so that -gae or -mpr is not included. 
	vaadinLibServerPath = BASE_PATH + '/Web Publishing/publishing-engine/jwpc-tomcat/fmi/WEB-INF/lib/vaadin-server-[1-9]*.fmi.jar'
	path_list = glob.glob (vaadinLibServerPath)
	if path_list:
		match = re.search (r'server-(\d*\.\d*\.\d*)', path_list[0])
		vaadin_str = match.group(1)
	else:
		vaadin_str = '<missing>'
	
	return vaadin_str


#
#	h a n d l e _ s e t
#

def handle_set (verb: str, noun: str):
	"""
	Execute the config changes specified in parameters.
	"""
	
	valid_options = False	# may still fail for other reasons
	
	if noun == 'debuglogging':
		if platform.system() in ['Darwin', 'Linux']:
			claris_config_dir = BASE_PATH + '/Database Server/bin'
		else:
			claris_config_dir = BASE_PATH + '/Database Server'
		if verb == 'enable':
			claris_config_write (claris_config_dir)
		elif verb == 'disable':
			claris_config_remove (claris_config_dir)
		else:
			print ('Error: unrecognized parameter for debuglogging:', noun)
			print ('Use "enable" or "disable" instead.')
		
		valid_options = True
	
	if not valid_options:
		print ('Error: unrecognized parameter(s) for set:', verb, noun)


def init_curses():
	global STDSCR, SCREENCOLS, SCREENROWS
	STDSCR = curses.initscr()
	curses.noecho()
	SCREENROWS, SCREENCOLS = STDSCR.getmaxyx()
	STDSCR.scrollok(1)


#
#	i n i t _ p a r s e r
#

def init_parser() -> argparse.ArgumentParser:
	"""Setup parameters used for command interface. Does not attempt to parse."""

	parser = argparse.ArgumentParser(
		prog='fmslogs',
		add_help=False,
		formatter_class=argparse.RawDescriptionHelpFormatter,
		description='View FileMaker Server logs and set logging options.')

	parser.add_argument('-B', '--backups', action='store_true', help='list paths & sizes of backup sets')
	parser.add_argument('-b', '--begin', nargs=1, help='start at first message on or after time or time interval')
	parser.add_argument('-c', '--check-connectivity', dest='check_connectivity', action='store_true', help='check connectivity of API endpoints')
	parser.add_argument('-d', '--databases', action='store_true', help='display database, container, document, and temp directory info')
	parser.add_argument('-e', '--edit', nargs=1, help='open the log file using the command defined by $EDITOR')
	parser.add_argument('-f', '--filter', nargs=1, help='only return lines matching regex expression')
	parser.add_argument('-h', '--head', action='store_true', help='display the first lines of log files')
	parser.add_argument('-H', '--headers-off', dest='headers_off', action='store_true', help='turn off headers for all logs')
	parser.add_argument('--help', action='help', help='display command details')
	parser.add_argument('-l', '--list', action='store_true', help='list all log files, including size, date created & modified, sorted by modification time')
	parser.add_argument('-m', '--merge', action='store_true', help='combine output of two or more logs based on the message timestamps')
	parser.add_argument('-n', '--number', nargs=1, default=['1s'], help='range or number of lines to print')
	parser.add_argument('-p', '--process-info', action='store_true', dest='process_info', help='list FMS related processes and their stats')
	parser.add_argument('-P', '--password', help='FMS console or SSH password')
	parser.add_argument('-S', '--set', nargs=2, help='change log configuration option')
	parser.add_argument('-s', '--succinct', action='store_true', help='strip less useful details from log output')
	parser.add_argument('--ssh', nargs=1, help='use the connection string to fetch logs from remote server')
	parser.add_argument('-t', dest='tail', action='store_true', help='wait for any new messages')
	parser.add_argument('--tail', dest='tail', type=float, nargs=1, default=[-1.0], help='wait for any new messages, specifying number of seconds to wait for each file')
	parser.add_argument('--truncate', action='store_true', help='cut off any output if beyond width of screen')
	parser.add_argument('-U', '--user', help='FMS console or SSH account name')
	parser.add_argument('-N', '--network', action='store_true', help='list connections and their status') # TODO: throwing "expected log name" error
	parser.add_argument('-V', '--version', action='store_true', help='version info for fmslogs and FMS')
	# Hack to avoid error if there is only an option specified but no positional argument
	parser.add_argument('logs', nargs='*', help='log name to display')
	#parser.add_argument('log2', nargs='?', help='additional log to display')

	parser.epilog = HELP_EPILOGUE
	
	return parser


#
#	l i s t_ a c t i v e _ p o r t s
#

def list_active_ports() -> list:
	'''
	List the open (listening or established) ports for processes using the fmserver user.
	'''
	
	curr_platform = platform.system()
	open_list = []
	
	if curr_platform == 'Darwin' or curr_platform == 'Linux':
		if os.getuid() != 0:
			print ('Running lsof command with sudo, you may be prompted for credentials.')
		# Could've used -F here, but seems easier to process formatted output.
		lsof_lines = subprocess.run(['sudo','lsof', '-P', '-u', 'fmserver', '+c15'], capture_output=True, text=True).stdout.split('\n')
		for line in lsof_lines:
			access_match = re.search ('LISTEN|ESTABLISHED',line)
			if access_match is None:
				continue

			if access_match.group(0) == 'LISTEN':
				items = line.split()
				port_num = int (re.search(r'\d+',items[8]).group())
				# process name, IP version, connection, port, type
				open_list.append (('L', items[0], items[4], items[8], port_num))

			elif access_match.group(0) == 'ESTABLISHED':
				# fmserverd 90834       fmserver   91u     IPv6  0x814de6ad3239a30       0t0                 TCP 172.16.184.175:5003->172.16.184.175:59835 (ESTABLISHED)
				items = line.split()
				# select digits after first colon
				port_num = int (re.search(r':\d+',items[8]).group()[1:])
				# process name, IP version, connection, port
				open_list.append (('E', items[0], items[4], items[8], port_num))
				
		# remove duplicates (eg, nginx)
		open_list = list (set (open_list))
		open_list.sort(key=lambda tupl: tupl[4])
	return open_list

#
#
#

def list_backup_locations() -> set:
	"""
	Return the full path of any backup schedule locations currently defined in FMS Preferences.
	The paths will be the base path; each backup will be in its own subfolder.
	"""
	
	backup_locations = []
	
	# There are two forms of the BackupTarget key, depending on where the type is defined.
	backupRE = re.compile (r'name="BackupTarget".*>(.*)</key>')

	with open (DBS_CONFIG_PATH, 'r', encoding='utf-8', errors='ignore') as f:
		for line in f:
			match = backupRE.search (line)
			# Above may match targests that are emtpy except for "file:"
			if match and match.group(1)[-1] != ':':
				backup_locations.append (convert_filemaker_path (match.group(1)))
	
	return set (backup_locations)


#
#	l i s t _ c r a s h _ r e p o r t s
#

def list_crash_reports() -> list:
	"""
	Return the full path of any recent crash report files.
	"""
	pass


def mac_log_tail (minutes_back: str):
	"""
	Return recent lines from the macOS system log for the given duration.
	"""
	return subprocess.run(['log', 'show', '--style', 'json', '--last', minutes_back + 'm', '--info', '--predicate' 'process=="fmserverd"'], capture_output=True, text=True).stdout.split('\n')


#
#	p r i n t _ s s l _ o p t i o n s_ c r y p t o
#

def print_ssl_options_crypto (host: str, port=443) -> bool:
	"""
	Get the TLS version and SSL cert data used for a test connection to the server.
	"""
	# Requires non-default module.
	#from cryptography import x509
	#from cryptography.hazmat.backends import default_backend
	
	return_flag = False
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout (4)
		ssock = context.wrap_socket(sock, server_hostname=host)
		ssock.connect ((host, port)) # we don't need the connection value
		return_flag = True
		print('   Security Protocol:', ssock.version())
		cert_der = ssock.getpeercert(binary_form=True)
		print (cert_der,'\n')
		cert = x509.load_der_x509_certificate(cert_der, default_backend())
		print ('  ', cert)
	except ssl.SSLError as e:
		print ('  ', e)
	finally:
		sock.close()
	
	return return_flag


def pick_from_list (options: list, prompt: str) -> int:
	"""
	Display a numbered list of items from item_list, prompting user to select one.
	Returns the index of the selected item, or -1 if cancelled.
	"""
	
	input_prompt = prompt + ' (ctl-C to cancel): '

	for i, item in enumerate (options):
		print (f'  {i+1}: {item}')

	print ()
	user_input = ''

	try:
		selection = input (input_prompt)

		while user_input not in map (str, range(1, len(options) + 1)):
			user_input = input (input_prompt)

	except KeyboardInterrupt:
		print ('\nCancelled.')
		return -1

	return int(user_input) - 1


#
#	p r o c e s s _ i n f o
#

def process_info ():
	"""
	Print the FMS related processes and their stats.
	"""

	print ()

	if platform.system() == 'Darwin':
		ps_result = subprocess.run(
			['ps', '-u', 'fmserver', '-o', 'pid,%cpu,rss,stime,time,majflt,stat,comm'],
			capture_output=True,
			text=True,
			check=False).stdout.split('\n')
	elif platform.system() == 'Linux':
		ps_result = subprocess.run(
			['ps', '-u', 'fmserver', '-o', 'pid,%cpu,rss,stime,times,maj_flt,stat,comm'],
			capture_output=True,
			text=True,
			check=False).stdout.split('\n')
	else:
		ps_result = ['not implemented'] # Windows

	for proc in ps_result:
		print (proc)


#
#	r e a d _ t a i l
#

def read_tail (file_path: str, lines_from_end: int) -> list:
	"""
	Scan lines in file, return up to lines_from_end line numbers from the end of file.
	Line numbers are base 1 for use with linecache.getline.
	"""

	check_file_validity (file_path)
	line_num = 1
	matching = []

	while True:
		line = linecache.getline (file_path, line_num)
		if line == '': break
		matching.append (line_num)
		line_num += 1

	#print ('line_num:',line_num, 'matching:',len (matching))
	return matching [-lines_from_end:]


#
#	p a r s e _ b e g i n _ t i m e
#

def parse_begin_time (timeStr: str) -> datetime.datetime:
	"""
	Convert the various possible time values into a timestamp value. Some
	possible values could be '30s' for 30 seconds from now, or '14:00:10'
	for 10 secs past 2pm today, or '2025-11-04 14:00' for 2pm on Nov 4.
	"""
	
	new_time = None
	
	# Duration value or a timestamp?
	m = re.search('[0-9]{0,6}[smhd]', timeStr)
	if m.group(0):
		if len (timeStr) == len (m.group(0)):
			new_time = datetime.datetime.now()
			unit = m.group(0)[-1]
			unit_count = 1
			
			if len (m.group(0)) > 1:
				unit_count = max (int (m.group(0)[:-1]), 1)
			if unit == 's':
				new_time -= datetime.timedelta(seconds=unit_count)
			elif unit == 'm':
				new_time -= datetime.timedelta(minutes=unit_count)
			elif unit == 'h':
				new_time -= datetime.timedelta(hours=unit_count)
			elif unit == 'd':
				# Start days at midnight
				new_time = new_time.replace (hour=0, minute=0, second=0, microsecond=0)
				new_time -= datetime.timedelta(days=unit_count-1)

			else:
				print ('Error: invalid duration')
				sys.exit(9)
	
	print (new_time, unit_count)
	return new_time

#
#
#	p r i n t _ b a c k u p _ i n f o
#

def print_backup_info():
	"""
	Print backup schedule information from FMS Preferences.
	"""
	
	print ()
	print ('=== BACKUP SETS ===\n')
	backup_locations = list_backup_locations()

	if backup_locations:
		for location in backup_locations:
			#print ('Base Path:', location)
			subfolders = get_subfolder_names(location)
			if subfolders:
				print ('{:>14}  {}'.format ('SIZE', 'PATH'))
				for subfolder in subfolders:
					full_path = os.path.join(location, subfolder)
					size = get_folder_size(full_path)
					print(f'{size:14,}  {full_path}')
			else:
				print ('Empty or Missing:', location)
			print ()
	else:
		print ('No backup locations defined in FMS Preferences.')
		print ()


def print_documents_info():
	"""
	Display information about FMS's Documents folder.
	"""
	documents_path = os.path.join (BASE_PATH, 'Data','Documents') 
	with Spinner ('FMS Documents            '):
		documents_size = get_folder_size (documents_path)
	print (f'{documents_size:16,}')

def print_fmtmp_info():
	"""
	Display information about FMS' usage of FMTEMP* files in /tmp.
	"""
	tmp_path = '/tmp/FMTEMPFM*'
	path_list = glob.glob (tmp_path)
	file_count = 0
	size_sum = 0
	if path_list:
		file_count = len (path_list)
		for file in path_list:
			stat_info = os.stat(file)
			size_sum += stat_info.st_size
		path_label = f'/tmp/FMTMPFM* ({file_count})'
	else:
		path_label = '/tmp/FMTMPFM* (0)'

	print (f'{path_label:30} {size_sum:>11}')


def print_log (log_name: str, count: int):

	lines_printed = -1
	print (OUTPUT_MODE)
	
	if OUTPUT_MODE is OutputMode.TAIL:
		lines_printed = print_tail (log_name, count, SHOW_HEADERS, SUCCINCT_MODE)
	
	elif OUTPUT_MODE is OutputMode.HEAD:
		lines_printed = print_head (log_name, count, SHOW_HEADERS, SUCCINCT_MODE)
	else:
		print ('Error: unknown output mode')
	
	return lines_printed


#
#	p r i n t _ l o g _ i n f o
#

def print_log_info():
	"""Print one line per supported log with path, size, creation & mod timestamps."""

	print()
	print ('LOG NAME                    SIZE  CREATED                    MODIFIED                   PATH')

	for log in LOG_CHOICES:
		full_path = get_log_path (log)
				
		mod_time = 0
		#TODO: check for permissions issue

		if full_path:
			try:
				mod_time = os.path.getmtime(full_path)
			except FileNotFoundError:
				pass
		
		if mod_time > 0:
			mod_timestamp = time.ctime(mod_time)
			
			stat_info = os.stat(full_path)
			
			# As of Python 3.12, st_birthtime is only available on macOS & Windows
			# ubuntu: stat -c "%w" Access.log
			# macos:  stat -f "%B"
			
			if platform.system() == 'Linux':
				# TODO: Replace with a more accurate method unless st_birthtime fixed in Python 3.15
				create_timestamp = subprocess.run(
					['stat', '-c', '%w', full_path],
					capture_output=True,
					text=True,
					check=False).stdout[:19]
			else:
				create_timestamp = time.ctime(stat_info.st_birthtime)
				#create_timestamp = datetime.datetime.fromtimestamp(stat_info.st_birthtime).isoformat()
			
			size = stat_info.st_size
			print(f'{log:21} {size:>11,}  {create_timestamp:<25}  {mod_timestamp:<25}  {full_path:<40}')
		else:
			if full_path is None:
				print(f'{log:21}              ---')
			else:
				print(f'{log:21}              <missing>')

	print ()


#
#	p r i n t _ d a t a _ d i r s _ i n f o
#
 
def print_data_dirs_info():
	"""
	Print the database folder paths extracted from Event.log.
	"""
	RCDIR = 'RC_Data_FMS'

	(default_path, secure_path, additional_path1, additional_path2, remote_container_path1, remote_container_path2) = get_db_directories()
	
	print ()
	print ('=== DATABASE DIRECTORIES ===')

	# DEFAULT
	db_size = get_folder_size (default_path, exclude_dir=RCDIR)
	print (f'Default:                  {db_size:16,}  {default_path}')

	with Spinner ('  ' + RCDIR + ':           '):
		rc_path = os.path.join (default_path, RCDIR)
		rc_size = get_folder_size (rc_path)
	print (f'{rc_size:16,}  {rc_path}')

	# SECURE
	size = get_folder_size (secure_path)
	print (f'Secure:                   {size:16,}  {secure_path}')

	with Spinner ('  ' + RCDIR + ':           '):
		rc_path = os.path.join (secure_path, RCDIR)
		rc_size = get_folder_size (rc_path)
	if rc_size >= 0:
		print (f'{rc_size:16,}  {rc_path}')
	else:
		print ('{:>16}  {}'.format ('-', '-'))

	# ADDITIONAL 1
	if additional_path1:
		size = get_folder_size (additional_path1, RCDIR)
		print (f'Additional Databases #1:  {size:16,}  {additional_path1}')

		with Spinner ('  ' + RCDIR + ':           '):
			rc_path = os.path.join (additional_path1, RCDIR)
			rc_size = get_folder_size (rc_path)
		if rc_size >= 0:
			print (f'{rc_size:16,}  {rc_path}')
		else:
			print ('{:>16}  {}'.format ('-', '-'))

		with Spinner ('  External Container #1: '):
			rc_size = get_folder_size (remote_container_path1)
		if True: # rc_size >= 0:
			print (f'{rc_size:16,}  {remote_container_path1}')
		else:
			print ('{:>16}  {}'.format ('-', '-'))

	# ADDITIONAL 2
	if additional_path2:
		size = get_folder_size (additional_path2, RCDIR)
		print (f'Additional Databases #2:  {size:16,}  {additional_path2}')

		with Spinner ('  ' + RCDIR + ':           '):
			rc_path = os.path.join (additional_path2, RCDIR)
			rc_size = get_folder_size (rc_path)
		if rc_size >= 0:
			print (f'{rc_size:16,}  {rc_path}')
		else:
			print ('{:16}  {}'.format ('-', '-'))

		with Spinner ('  External Container #2: '):
			rc_size = get_folder_size (remote_container_path2)
		print (f'{rc_size:16,}  {remote_container_path2}')

	print ()
	print ('=== OTHER DATA DIRECTORIES ===')
	print ()
	print_documents_info()
	print_fmtmp_info()
	print ()


#
#	p r i n t _ l o g _ h e a d e r
#

def print_log_header (log_name:str, succinct: bool) -> int:
	"""
	Print the appropriate column headers depending on the log.
	"""

	header_str = None
	line_count = 0

	try:
		if succinct and 'shed' in LOG_SPECS[log_name]:
			header_str = LOG_SPECS [log_name]['shed']
		else:
			header_str = LOG_SPECS [log_name]['head']
	except:
		pass

	if header_str:
		line_count = 1 + header_str.count ('\n')
		print (TerminalColors.BOLD,end='')
		print (header_str)
		print (TerminalColors.END,end='')

	return line_count


def print_file_head_faster (file_path: str, lines: int) -> bool:
	"""
	Print up to the given number of lines of text from the start of the file at the provided path.
	If MAX_READ_LEN is reached, stop output and append a '+'.
	Result is False if there was an error opening or reading the file.
	"""

	MAX_READ_LEN = 4096
	result = False

	if lines > 0:
		with open (file_path, 'r', encoding='utf-8') as logfile:
			lines = logfile.readlines (MAX_READ_LEN)
			for line in lines[0:SCREENROWS-1]:
				print (line, end="")
			#STDSCR.erase()
			#for line in lines:
			#    STDSCR.addstr(line)
			result = True
			if len (lines) == MAX_READ_LEN:
				# indicate that we reached read limit
				print ('+++')
	else:
		# We never opened the file, but will consider this a success.
		result = True

	#STDSCR.getch()
	return result


#
#   p r i n t _ h e a d
#

def print_head (log_name: str, count: int, header: bool, succinct: bool) -> bool:
    
	line_list = []
	line_counter = count
	log_path =  get_log_path (log_name)
	
	try:
		check_file_validity (log_path)
	except FileCheckError as e:
		print ('Error:', e)
		return -1
	
	# First, find first line containing some kind of message date
	# that is on or after our start date.
	
	if header:
		header_count = print_log_header(log_name, succinct)
		line_counter -= header_count
	else:
		header_count = 0
	
	if TIMESTAMP_START:
		line_num = find_first_timestamp (log_path, TIMESTAMP_START)
	else:
		if LOG_SPECS[log_name]['lghd']:
			line_num = 2		# skip the column header row
		else:
			line_num = 1
	
	if line_num > 0:
		#print (line_num, count, maxLine)
		
		while True:
			line = linecache.getline (log_path, line_num)
			if line == '':
				break
			if FILTER_REGEX:
				if FILTER_REGEX.search (line):
					line_list.append (line_num)
					line_counter -= 1
			else:
				line_list.append (line_num)
				line_counter -= 1

			if line_counter < 1:
				break
			line_num += 1

	if line_list:
		for line_num in line_list:
			print (expand_tabs_for_line (log_name, linecache.getline (log_path, line_num)), end='')
	else:
		print ('<' + log_name + ' has no messages>')

	return len (line_list) + header_count


#
#	p r i n t _ n e t _ s t a t u s
#

def print_net_status():
	"""
	Print info on ports that are currently open and
	active connections to FMS components.
	"""

	#' | grep -E "LISTEN" | sort -n -k 8'

	open_ports = list_active_ports()
	print ()
	print ('LISTENING PROCESSES')
	print ('PROCESS           VERS  CONNECTION')
	for line in open_ports:
		#print (items[0],items[4],items[8])
		if line[0] == 'L':
			print('{line[1]:16}  {line[2]:<4}  {line[3]:<15}')

	print ()
	print ('CONNECTIONS')
	print ('PROCESS           VERS  CONNECTION')
	for line in open_ports:
		if line[0] == 'E':
			print('{line[1]:16}  {line[2]:<4}  {line[3]:<15}')

	try:
		response = urllib.request.urlopen('http://checkip.amazonaws.com', timeout=5)
		gateway_ip = response.read().decode("utf-8")[:-1]
	except urllib.error.URLError:
		gateway_ip = 'Could not connect to checkip.amazonaws.com'

	print ()
	print ('Internet Address:', gateway_ip)
	print ('External Interface:', get_local_ip())
	print ('FMS Network Address(es):', get_network_address())
	print ()


#
#	p r i n t _ c e r t i f i c a t e _ i n f o
#

def print_certificate_info (host: str, port=443) -> bool:
	"""
	Display the SSL certificate attributes used by host.
	This uses an undocumented interface, as all Python alternatives
	to extract SSL info require additional modules to be installed.
	https://github.com/python/cpython/blob/main/Lib/test/test_ssl.py
	"""
	
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	
	with tempfile.NamedTemporaryFile (mode='w', encoding='utf8') as cert_file:
		cert_file_path = cert_file.name

		try:
			cert_file.write(ssl.get_server_certificate((host, port)))
			# We don't have to close before _test_decode_cert below as long as we flush first.
			cert_file.flush()
		except ConnectionRefusedError:
			print (f'https://{host}:{port} not responding')
			return False
		except Exception as e:
			print (':', e)
			return False
		
		try:
			cert_dict = ssl._ssl._test_decode_cert(cert_file_path)
		except Exception as e:
			print('Error decoding certificate: {e:}')
		else:
			print ('Subject:')
			print (' ',end='')
			pprint.pprint (cert_dict ['subject'])
			if 'subjectAltName' in cert_dict:
				print (' ',end='')
				pprint.pprint (cert_dict ['subjectAltName'], indent=2)
			print ('Not Before:', cert_dict ['notBefore'])
			print ('Not After:', cert_dict ['notAfter'])
			if 'OCSP' in cert_dict:
				print ('OCSP:', cert_dict ['OCSP'])
			if 'caIssuers' in cert_dict:
				print ('CA Issuers:', cert_dict ['caIssuers'])
			print ('Serial Number:', cert_dict ['serialNumber'])
			print ('Issuer:')
			pprint.pprint (cert_dict['issuer'], indent=1)
			print ('Version:', cert_dict ['version'])
	
	return True


#
#	p r i n t _ t a i l
#

def print_tail (log_name: str, count: int, header: bool, succinct: bool) -> int:
	"""
	Print up to 'count' number of lines of text from the end of the file at path.
	Result is False if there was an error opening or reading the file.
	If 'header' is true, display the log headers (if any) as first line.
	If 'succinct' is true, strip less useful info from lines.
	"""

	line_list = []
	line_count = count

	log_path =  get_log_path (log_name)

	try:
		check_file_validity (log_path)
	except FileCheckError as e:
		print ('Error:', e)
		return -1

	# TODO: only print headers if there's log output
	if header:
		header_count = print_log_header (log_name, succinct)
		line_count = line_count - header_count
	else:
		header_count = 0

	# Below we can files only, creating a list of records to later print.
	
	# JUST DETERMINE START LINE AND USE THAT AS PARAM TO SINGLE READ FUNC?
	
	if TIMESTAMP_START:
		if FILTER_REGEX:
			line_list = read_tail_filtered_and_time (log_path, line_count)
		else:
			line_list = read_tail_time (log_path, line_count)
	else:
		if FILTER_REGEX:
			line_list = read_tail_filtered (log_path, line_count)
		else:
			#print ('unfiltered')
			line_list = read_tail (log_path, line_count)
	
	# the actual line_count is now this:
	line_count = len (line_list)
	
	if line_count > 0:
		if LOG_SPECS[log_name]['lghd'] and line_list[0] == 1:
			# Remove the header line, we'll be using our own
			del line_list[0]

	if succinct and 'shtb' in LOG_SPECS[log_name].keys():
		for line_num in line_list:
			line = strip_line (log_name, linecache.getline (log_path, line_num))
			try:
				print (expand_tabs_for_line (log_name, line), end='')
			except BrokenPipeError:
				# Catch errors if output is piped and the other end closes prematurely (eg, using `head`).
				# Use below to avoid a later error when Python flushes stdout
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				line_count = -line_count
				break

	else:		
		for line_num in line_list:
			try:
				print (expand_tabs_for_line (log_name, linecache.getline (log_path, line_num)),end='')
			except BrokenPipeError:
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				line_count = -line_count
				break
	
	return len (line_list) + header_count


#
#	p r i n t _ v e r s i o n
#

def print_version():
	"""
	Handle a request to display the fmslog version and the versions of various FMS components.
	"""
	
	print ()
	print ('fmslogs', VERSION)
	print ('Latest version at: https://github.com/beezwax/fmslogs')
	print ('Questions or comments: info@beezwax.net')
	print ()
	
	# FMS VERSION
	
	print ('FMS:', get_fms_version())
	
	# JAVA
	try:
		if platform.system() == 'Darwin':
			java_words = subprocess.run(
				[BASE_PATH + '/Web Publishing/java/bin/java', '-version'],
				capture_output=True,
				text=True,
				check=False).stderr.split('"')
			print ('Java:', java_words[1])
		elif platform.system() == 'Linux':
			java_words = subprocess.run(
				['/usr/bin/java', '-version'],
				capture_output=True,
				text=True,
				check=False).stderr.split('"')
			print ('Java:', java_words[1])
	except FileNotFoundError:
		print ('Java: <missing>')
	
	# NODE.JS
	
	if platform.system() in ['Darwin', 'Linux']:
		node_version = subprocess.run(
			[BASE_PATH + '/node/bin/node', '-v'],
			capture_output=True,
			text=True,
			check=False).stdout[1:]
		print ('Node:', node_version, end='')
	
	# OPENSSL
	
	if platform.system() == 'Darwin':
		open_ssl_words = subprocess.run(
			[BASE_PATH + '/Database Server/bin/openssl','version'],
			capture_output=True,
			text=True,
			check=False).stdout.split()
		print ('OpenSSL:',open_ssl_words[1],open_ssl_words[2])
	
	elif platform.system() == 'Linux':
		open_ssl_words = subprocess.run(
			['/usr/bin/openssl', 'version'],
				capture_output=True,
				text=True,
				check=False).stdout.split()
		print ('OpenSSL:',open_ssl_words[1],open_ssl_words[2])
	
	# APACHE/NGINX/IIS
	
	if platform.system() == 'Linux' or platform.system() == 'Windows':
		if platform.system() == 'Linux':
			nginx_path = '/usr/sbin/nginx'
		else:
			nginx_path = 'C:\\Programs\\'
		
		try:
			nginx_out = subprocess.run(
				[nginx_path, '-v'],
				capture_output=True,
				text=True,
				check=False).stderr
			# nginx version: nginx/1.28.0\n
			nginx_vers = nginx_out.split ('/')[1][:-1]
		except FileNotFoundError:
			nginx_vers = 'not present'
		
		print ('NGINX:',nginx_vers)
	
	if platform.system() == 'Darwin':
		httpd_path = '/usr/sbin/httpd'
		try:
			httpd_out = subprocess.run(
				[httpd_path, '-v'],
				capture_output=True,
				text=True,
				check=False).stdout
			# Server version: Apache/2.4.62 (Unix)
			# Server built:   Nov  8 2025 20:07:11
			httpd_vers_line = httpd_out.split('\n')[0]
			httpd_vers = httpd_vers_line.split('/')[1].split()[0]
		except FileNotFoundError:
			httpd_vers = 'not present'
		print ('Apache:',httpd_vers)
		
	if platform.system() == 'Windows':
		print ('IIS: ')
	
	print ('Vaadin:', get_vaadin_version())
	print ()


#
#	r e a d _ t a i l _ f i l t e r e d
#

def read_tail_filtered (file_path: str, lines_from_end: int) -> list:
	"""
	Search file for any matching lines, return up to lines_from_end
	line numbers from the end of file that match. Line numbers are base 1
	for use with linecache.getline.
	"""
	
	check_file_validity (file_path)
	
	matching = []
	line_num = 1
	while True:
		line = linecache.getline (file_path, line_num)
		if line == '': break
		if FILTER_REGEX.search (line):
			matching.append (line_num)
		
		line_num += 1
		
	return matching [-lines_from_end:]


#
#	r e a d _ t a i l _ f i l t e r _ a n d _ t i m e
#

def read_tail_filtered_and_time (file_path: str, lines_from_end: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	From there, then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""
	
	check_file_validity (file_path)
	matching = []
	
	# First, find the first line containing some kind of message date
	# that is on or after our start date.
	
	line_num = find_first_timestamp (file_path, TIMESTAMP_START)
	
	if line_num > 0:
		# Now, filter anything after start date.
		while True:
			line = linecache.getline (file_path, line_num)

			if line == '': break

			if FILTER_REGEX.search (line):
				matching.append (line_num)
			line_num += 1
	
	# Cut result down to no more than requested lines from end of file.
	return matching [-lines_from_end:]


#
#	r e a d _ t a i l _ t i m e
#

def read_tail_time (file_path: str, lines_from_end: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	Then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""

	check_file_validity (file_path)
	matching = []

	# First, find the first line containing some kind of message date
	# that is on or after our start date.

	line_num = find_first_timestamp (file_path, TIMESTAMP_START)

	# Find the last line
	while True:
		line = linecache.getline (file_path, line_num)
		if line == '':
			break
		# TODO: purge line list when it gets too big
		matching.append (line_num)
		line_num += 1

	# Cut result down to no more than requested lines from end of file.
	return matching [-lines_from_end:]


#
#   s c a n _ f i l e _ l a s t _ m a t c h
#

def scan_file_last_match (log_path: str, match_str: str) -> str:
	"""
	Scan the file at given path, returning the result of last found matching pattern.
	Returns None if no matches were found or the file is not readable.
	"""

	try:
		check_file_validity (log_path)
	except FileCheckError:
		return None

	line: str = None
	line_num: int = 1
	last_match: str = None

	while True:
		line = linecache.getline (log_path, line_num)
		if line == '':
			break
		m = re.search (match_str, line)
		if m:
			last_match = m.group(1)
		line_num += 1

	return last_match


class Spinner:
	"""
	Implements a spinning cursor for longer running processes.
	"""

	def __init__(self, message, delay=0.1):
		self._screen_lock = None
		self.spinner = itertools.cycle(['-', '/', '|', '\\'])
		self.delay = delay
		self.busy = False
		self.spinner_visible = False
		self.thread = None
		sys.stdout.write(message)

	def write_next(self):
		"""Update the spinner character."""
		with self._screen_lock:
			if not self.spinner_visible:
				sys.stdout.write(next(self.spinner))
				self.spinner_visible = True
				sys.stdout.flush()

	def remove_spinner(self, cleanup=False):
		"""Spinner no longer needed, so restore to initial state."""
		with self._screen_lock:
			if self.spinner_visible:
				sys.stdout.write('\b')
				self.spinner_visible = False
				if cleanup:
					sys.stdout.write(' ')       # overwrite spinner with blank
					#sys.stdout.write('\r')      # move to next line
				sys.stdout.flush()

	def spinner_task(self):
		while self.busy:
			self.write_next()
			time.sleep(self.delay)
			self.remove_spinner()

	def __enter__(self):
		if sys.stdout.isatty():
			self._screen_lock = threading.Lock()
			self.busy = True
			self.thread = threading.Thread(target=self.spinner_task)
			self.thread.start()

	def __exit__(self, exc_type, exc_val, exc_traceback):
		if sys.stdout.isatty():
			self.busy = False
			self.remove_spinner(cleanup=True)
		else:
			sys.stdout.write('\r')


#
#	s t r i p _ l i n e
#

def strip_line (log_name: str, line: str) -> str:
	"""
	When possible, remove repetitive or extraneous text in the logs.
	This is done after expanding tabs so columns should be at fixed positions.
	"""

	if log_name in ['access','event']:
		if line [24] == '-':
			items = line.split('\t')
			#line = '{:23.23}  {:11}  {:4}  {:<}'.format (items[0], items[1], items[2], items[4])		# remove timezone and hostname
			line = f'{items[0]:23.23}  {items[1]:11}  {items[2]:4}  {items[4]:<}'
	if log_name == 'admin':
		if line [20] == '-':
			line = line [:20] + line [26:]							# remove timezone
	if log_name in ['clientstats','dapi','topcall']:
		line = line [:23] + line [29]								# remove timezone
	if log_name == 'dapi':
		line = line [:23] + line [29:45] + line[62:]			# remove timezone & hostname

	return line


# =================

class TailPrint:
	
	# based on https://github.com/kasun/python-tail
	
	''' Represents a tail command. '''
	def __init__(self, log_names: list):
		'''Initiate a Tail instance.
			Check for file validity, assigns callback function to standard out.
			Arguments:
				tailedFiles - List of file to be followed. '''
				
		self.log_names = []
		self.log_paths = []
		
		for log in log_names:
			path = get_log_path (log)
			self.check_file_validity(path)
			self.log_names.append (log)
			self.log_paths.append (path)
	
	def follow(self, s=1.0):
		''' Do a tail follow. If a callback function is registered it is called with every new line. 
		Else printed to standard out.

		Arguments:
			s - Number of seconds to wait between checking each file'''
		
		MIN_DELAY = 0.01 # wait at least 1/100 second before checking next log
		secs = max (s, MIN_DELAY)
		
		try:
			with ExitStack() as stack:
				#with open(self.tailed_file) as file_:
				file_list = [stack.enter_context(open (fpath, 'r', encoding='utf-8')) for fpath in self.log_paths]
				for file in file_list:
					file.seek(0,2)
				
				# Go to the end of file
				while True:
					time.sleep (secs)

					for (file,log) in zip (file_list, self.log_names):
						while True:
							line = file.readline()
							if line:
								self.print_line (log, line)
							else:
								break

		except KeyboardInterrupt:
			pass

	def check_file_validity(self, file_):
		''' Check whether the a given file exists, readable and is a file '''
		if not os.access(file_, os.F_OK):
			raise FileCheckError(f'File "{file_}" does not exist')
		if not os.access(file_, os.R_OK):
			raise FileCheckError(f'File "{file_}" not readable')
		if os.path.isdir(file_):
			raise FileCheckError(f'File "{file_}" is a directory')

	def print_line(self, log_name: str, line: str):
		"""
		Print output sent by one or more TailPrint objects. If output transitions to different file,
		indicate change by printing blank line followed by file path.
		"""
		
		global LAST_LOG_PRINTED
		
		# Check that we actually want to print this line.
		if FILTER_REGEX is None or FILTER_REGEX.search (line):
			
			if log_name != LAST_LOG_PRINTED:
				if LAST_LOG_PRINTED is not None:
					print ()
				# only print prefix line if we have transitioned output from one file to another
				print ('===', log_name)
				LAST_LOG_PRINTED = log_name
				
			print (expand_tabs_for_line (log_name, line),end='')

# =================


def handle_named_options (args) -> bool:
	"""
	Run through all possible named options (eg, -X or --xray) and
	execute its handler if set.
	
	Set the ignore_positionals variable if any later
	positional parameters should be skipped.
	"""

	global PASSWORD, TIMESTAMP_START, USER	
	ignore_positionals = False

	if args.backups:
		print_backup_info ()
		ignore_positionals = True

	if args.begin:
		#print (args.begin[0])
		TIMESTAMP_START = parse_begin_time (args.begin[0])

	if args.check_connectivity:
		check_connectivity()
		ignore_positionals = True

	if args.databases:
		print_data_dirs_info()
		ignore_positionals = True

	if args.list:
		print_log_info()
		ignore_positionals = True

	if args.network:
		print_net_status()
		ignore_positionals = True

	if args.password:
		PASSWORD = args.password
	
	if args.process_info:
		process_info()
		ignore_positionals = True

	if args.set:
		handle_set (args.set[0], args.set[1])
		ignore_positionals = True

	if args.user:
		USER = args.user		

	if args.version:
		print_version()
		ignore_positionals = True

	return ignore_positionals


#
#	m a i n
#

def main():
	"""
	Setup parser, handle named options, then positional options.
	"""
	
	global OUTPUT_MODE, SHOW_HEADERS, SUCCINCT_MODE, TRUNCATE_MODE
	
	parser = init_parser()
	args = parser.parse_args()
	screen_lines_counter = 0					# if used, will start (num lines on screen * num screens) - space for prompt line

	while True:
		ignore_positionals = handle_named_options (args)
		
		if ignore_positionals:
			break	# only follow through if not limiting output to the above options
				
		if args.edit:
			edit_log (args.edit[0])
		
		if args.head:
			OUTPUT_MODE = OutputMode.HEAD
		
		if args.begin:
			# Use head mode with the -b option
			OUTPUT_MODE = OutputMode.HEAD
		
		if args.headers_off:
			SHOW_HEADERS = False
		
		if args.succinct:
			SUCCINCT_MODE = True
			
		if args.truncate:
			TRUNCATE_MODE = True
			
		if args.filter:
			if not compile_filter (args.filter[0]):	# Compile the default or the filter that was just set
				break												# bad regex

		#print (args.tail, args.logs,type (args.logs))

		# Assume we are printing at least one log.
		num_logs_to_print = len (args.logs)
		if num_logs_to_print == 0:
			print ('Error: expected at least one log name\n')
			parser.print_help()
			break
		
		# Couldn't use choices=ALL_CHOICES in argparse, so must check names manually.
		for log_name in args.logs:
			if log_name not in ALL_CHOICES:
				print ('Error:', log_name,'not a valid log name')
				sys.exit (1)

		# TAILING/FOLLOWING LOGS

		if (isinstance (args.tail, bool) and args.tail) or (isinstance (args.tail,list) and args.tail[0] > 0):
			# Instead of printing the current tail or head, wait for new messages
			tailer = TailPrint (args.logs)
			if isinstance (args.tail, list):
				tailer.follow(args.tail[0])
			else:
				tailer.follow(1.0)
		
			break
		
		# HEAD OR TAIL
		
		lines_printed = 0;
		last_log = args.logs[-1]
		
		num_mode,lines_per_log,screen_lines_counter = calc_row_metrics (num_logs_to_print, args.number[0])
		if lines_per_log < 1:
			break
		
		#print (log_lines_count_val, lines_per_log, screen_lines_counter)
		
		for log_name in args.logs:
			if log_name == last_log and num_mode == 's':
				lines_per_log = screen_lines_counter			# Use whatever is left for last log (eg, there may be a remainder)
		
			lines_printed = print_log (log_name, lines_per_log)
		
			if lines_printed < 0:
				break # error occured
			
			screen_lines_counter -= lines_printed

		#curses.endwin()
		break

if __name__=="__main__":
	main()


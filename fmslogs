#!/usr/bin/env python3
# -*- Mode:Python; indent-tabs-mode:nil; tab-width:3; encoding:utf-8 -*-

"""
Filename: fmslogs.py
Author: Simon Brown on 10/15/2025
Version: 0.29, 2026-01-03
Purpose: Display FileMaker Server logs and change logging options.
"""

import argparse, curses, datetime
import glob, http.client, itertools
import linecache, os, pathlib, platform, pprint
import re, shutil, socket, subprocess, socket, sys
import tempfile, textwrap, threading, time, ssl, urllib.request
from enum import Enum
from contextlib import ExitStack

"""
POSSIBLE NEW OPTIONS
	non-standard install location
	'top' or iostat option
	summarize results where possible (eg, count, min, max, sum)?
	convert table IDs
	paged output
	list crash reports
	remote/external log source via SSH
	for succinct mode, change 'Information' to 'Info' and 'Warning' to 'Warn' (Error will then be longest)

EXAMPLES OF UNIMPLEMENTED USAGE
fmslogs --ssh simon@server.beezwax.net access  # if using user's default 'SSH key
fmslogs -s enable topcall  # enable TopCall.log

Reset repo:  git fetch origin main; git reset --hard origin/main
"""

VERSION = "0.29 - 2026-01-03"
TIMESTAMP_START = None
FILTER_REGEX = None
TEXTWRAP = textwrap.TextWrapper(width=120,tabsize=10)

class OutputMode (Enum):
	HEAD = 1
	TAIL = 2
	OTHER = 3

LAST_LOG_PRINTED = None
OUTPUT_MODE = OutputMode.TAIL
SHOW_HEADERS = True
SUCCINCT_MODE = False
TRUNCATE_MODE = False

try:
	SCREENCOLS, SCREENROWS = os.get_terminal_size()
except OSError:
	# Probably being piped so no terminal
	SCREENCOLS = 255
	SCREENROWS = 48

MAXREADLEN = 1048576*10

# Default deployment paths (Windows paths will have forward slashes converted)
DEF_BASE_PATHS = {
	'Darwin': '/Library/FileMaker Server',
	'Linux': '/opt/FileMaker/FileMaker Server',
	'Windows': 'C:/Programs/FileMaker/FileMaker Server'
}

# This may get overridden by user option,
BASE_PATH = DEF_BASE_PATHS [platform.system()]

LOG_ALIAS = {
	('acc', 'access'),
	('adm', 'admin'),
	('ada', 'adminapi'),
	('cat', 'catalina'),
	('cli', 'clientstats'),
	('dap', 'dapi'),
	('eve', 'event'),
	('fad', 'fmsadmindebug'),
	('fcw', 'fmscwpc'),
	('fcl', 'fmscwpcli'),
	('fms', 'fmsdebug'),
	('fgp', 'fmsgetpasskeydebug'),
	('fhd', 'fmshdebug'),	# helper debug?
	('fib', 'fmsibdebug')	# incremental backup
}

# path: location of the log file, starting at the base path
# lghd: True if the log file has a header line at start of file
# head: header to use if not succinct mode
# tbst: list of tab stops to use for columns, or just a single number for tab size
# shed: succinct version of header (may not be present)
# shtb: succinct version of tab stops (may not be present)

LOG_SPECS_BASE = {
	'access': {
		'path': 'Logs/Access.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        2025-09-15 01:12:45.831 -0700  Information  228   some-dev.filemaker.beezwax.net         The previous log file reached maximum size, and was renamed to "Access-old.log".
		'head': 'TIMESTAMP                       LEVEL        CODE  HOST                                  MESSAGE',
		'tbst': [32,45,51,89],
		#        2025-09-15 01:12:45.831  Information  228   The previous log file reached maximum size, and was renamed to "Access-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45]
	},
	'admin': {
		'path': 'Admin/FAC/logs/fac.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-17 14:29:56 -0700  Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		#			2022-05-17 14:30:00 -0700 - error:  fmi   127.0.0.1   notifications  general   n/a   "New system notification generated, type: CPU_USAGE_EXCEED_HARD_LIMIT"
		# Only uses tabs with regular messages (eg, not error or warn) after timestamp.
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},
	'adminapi': {
		'path': 'Admin/FAC/logs/fac1.log',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2022-05-24 14:04:30 -0700 - error:   fmi   127.0.0.1   fmsadminapi   general   3   "Get worker list failed."
		#			2022-05-24 19:32:51 -0700       Execute /opt/FileMaker/FileMaker Server/Admin/FAC/facstart.sh
		# Only uses tab after timestamp with regular messages (eg, not error or warn)
		'head': 'TIMESTAMP                   LEVEL  ENDP  ADDRESS     COMPONENT      TYPE      CODE  MESSAGE',
		'tbst': [35,41,53,68,78,84],
		'shed': 'TIMESTAMP           {LEVEL}   {END} {ADDRESS}   {COMPONENT}    {TYPE}    {CODE}  MESSAGE',
		'shtb': [23]
	},

	'catalina': {
		'path': 'Web Publishing/publishing-engine/jwpc-tomcat/logs/catalina.*',
		'lghd': False,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#        09-Dec-2025 09:31:08.332 WARNING [main] org.apache.tomcat.util.digester.SetPropertiesRule.begin Match [Server/Listener] failed to set property [AWTThreadProtection] to [true]
		#			will also include backtraces
		'head': 'Catalina.*.log                 NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
	},


	'clientstats': {
		'path': 'Logs/ClientStats.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-16 15:46:18.054 -0700  37781   8559   209   0     46442     0    28    Xeronthia Shilnow (XS ETMD6M) [255.143.244.179]
		'head': 'ClientStats.log                NET BYTES  NET BYTES  CALLS      CALLS      TIME       TIME       TIME\n' + \
				  'TIMESTAMP                      IN         OUT        COMPLETE   IN PROG    ELAPSED    WAIT       I/O        CLIENT NAME',
		'tbst': [31,42,53,64,75,86,97,108],
		#			2025-10-16 15:46:18.054  37781    8559    209     0    46442    0     28   Xeronthia Shilnow (XS ETMD6M) [255.143.144.79]
		'shed': 'ClientStats.log                     NET BYTES  NET BYTES CALLS     CALLS     TIME      TIME      TIME\n' + \
				  'TIMESTAMP                IN         OUT       COMPLETE  IN PROG   ELAPSED   WAIT      I/O       CLIENT NAME',
		'shtb': [26,37,48,59,70,81,92,103]
	},
	
	'dapi': {
		'path': 'Logs/fmdapi.log',
		'lghd': True,
		#        ----------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-27 11:08:10.055 -0700  4101   ERROR	250.130.228.236  some-user-name   POST  Script Error -- Script File: 'filename', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		# Size at end (re-arrange columns?). Rarely a 4 digit error code.
		'head': 'TIMESTAMP	                CODE   LEVEL   HOST            USER              HTTP  MESSAGE  SIZE',
		'tbst': [32,39,47,63,81,87],
		#			2025-08-27 11:08:10.055  301   ERROR  some-user-name   POST  Script Error -- Script File: 'Tool', Script Name: 'create update topic [PSoS]', Script Step: 'Set Field By Name'  0
		'shed': 'TIMESTAMP	             CODE  LEVEL  USER             HTTP  MESSAGE  SIZE',
		'shtb': [26,32,39,56,62]
	},
	
	'event': {
		'path': 'Logs/Event.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-18 23:15:30.125 -0700  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'head': 'TIMESTAMP                ZONE  LEVEL        CODE  HOST                            MESSAGE',
		'tbst': [31,44,50,82],
		#			2025-08-18 23:15:30.125  Information  228   some-dev.filemaker.beezwax.net  The previous log file reached maximum size, and was renamed to "Event-old.log".
		'shed': 'TIMESTAMP                LEVEL        CODE  MESSAGE',
		'shtb': [26,39,45,77]
	},
	
	'fmsadmindebug': {
		'path': 'Database Server/bin/fmsadminDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmsasedebug': {
		'path': 'Database Server/bin/fmsaseDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpc': {
		'path': 'Database Server/bin/fmscwpc.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmscwpcli': {
		'path': 'Database Server/bin/fmscwpcli.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},

	'fmsgetpasskeydebug': {
		'path': 'Database Server/bin/fmsgetpasskeyDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmshdebug': {
		'path': 'Database Server/bin/fmshDebug.log',
		'lghd': True,
			#		2024-10-21 09:55:47.101 -0700 [HelperApp] Debug::DumpBitSettings() feature(s) enabled: [ AlwaysPrint | Assert | ForceOutput | Backup | UploadDownload | Server | ServerCommands | ServerComponents | ServerDependencies | FMSECLI | NotificationsStackCrawls | Anchors | BPlusTree | FileBox | LicenseServer | CalcEngine | Add-ons | OData | TransactSupport | AdminAPI | CertVerify | CurrentThread | Consolidator | DownloadService | WIP | EditOtherClientLayout | CLI | ThreadedSorting | DisableServerSideSorting | DisableMemoryKeyCmpIndexing | PurgeTempDB | DisableSharingLockOnServer | PersistentData | DisableServerSideSummary | SupportNestedPSOS ]
			#		2024-10-21 09:55:47.117 -0700 [HelperApp] AdminMgrConfigFile() installation directory = /opt/FileMaker/FileMaker Server/, config file = Admin/conf/deployment.xml, encrypted = false
		'head': None,
		'tbst': 8	# replace any tabs with two spaces
	},
	
	'fmsibdebug': {
		'path': 'Database Server/bin/fmsibDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmslogdebug': {
		'path': 'Database Server/bin/fmslogDebug.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	
	'fmwipd': {
		'path': 'Database Server/bin/fmwipd.log',
		'lghd': False,
		'tbst': 8
	},

	'loadschedules': {
		'path': 'Logs/LoadSchedules.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'odata': {
		'path': 'Logs/fmodata.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#		  '2025-10-14T13:01:31.232452-08:00  0     INFO   170.255.255.218   GET   /fmi/odata/v4	 75'
		'head': 'TIMESTAMP                         CODE  LEVEL  HOST                 OP      ENDPOINT               SIZE',
		'tbst': [34,40,47,66,68,76,99],  # 'size' value will be padded on end
		#		  '2025-10-14T13:01:31.232452  0     INFO   GET   /fmi/odata/v4	 75'
		'shed': 'TIMESTAMP                   CODE  LEVEL  OP    ENDPOINT  SIZE',
		'shtb': [29,35,48]
	},
	
	'odatadebug': {
		'path': 'Database Server/bin/fmodataDebug.log',
		'lghd': False,
		'tbst': 8
	},
	
	'scriptevent': {
		'path': 'Logs/scriptEvent.log',
		'lghd': False,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-08-11 03:00:26.470 -0700  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'head': 'TIMESTAMP                ZONE  CODE  MESSAGE',
		'tbst': [32,38],
		#			2025-08-11 03:00:26.470  401   Schedule "daily mailing" scripting error (401) at "TOOL : delete mailing batches without queued logs [PSoS] : 22 : Perform Find".
		'shtb': [26,32],
	},
	
	'stats': {
		'path': 'Logs/Stats.log',
		'lghd': True,
		#			---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------
		#			2025-10-17 17:54:42.335 -0700	0	14	11	0	98	0	0	1	0	0	0	2	0	546	40	81	0
		'head': 'Stats.log                      NET      NET       DISK       DISK       CACHE   CACHE     CLIENTS  OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                ZONE  KBs IN   KBs OUT   KBs READ   KBs WRITE  HIT %   UNSAVD %  PRO      DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'tbst': [31,40,50,61,72,80,90,99,105,114,123,132,142,150,159,168,177],
		'shed': 'Stats.log                NET      NET       DISK       DISK        CACHE   CACHE     PRO      OPEN  CLIENTS  CLIENTS  CLIENTS  CALLS/s   CALLS   TIME     TIME     TIME     CLIENTS\n' + \
				  'TIMESTAMP                KB/s In  KB/s OUT  KB/s READ  KB/s WRITE  HIT %   UNSAVD %  CLIENTS  DBS   XDBC     WEBD     CWP      COMPLETE  ACTIVE  ELAPSED  WAIT     I/O      GO',
		'shtb': [26,35,45,56,68,76,86,95,101,110,119,128,138,146,155,164,173]
	},

	'topcall': {
		'path': 'Logs/TopCallStats.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2025-10-31 22:12:31.419 -0700   166630.877193  166635.541004  4663811   Query (Find)                     Tool::table(181)::field definitions(356)     509        33         4663811    0        235659   tool-beezwax-net (172.30.8.236) [172.30.8.236]
		'head': 'TopCallStats.log               TIME           TIME           TOTAL                                                                                    NET BYTES  NET BYTES  TIME       TIME     TIME\n' + \
				  'TIMESTAMP                      START          END            ELAPSED   OPERATION                         TARGET                                       IN         OUT        ELAPSED    WAIT     I/O       CLIENT NAME',
		'tbst': [31,46,61,71,105,150,161,172,183,192,202],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},
	
	'trimlog': {
		'path': 'Database Server/bin/trimlog.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'wpe': {
		'path': 'Logs/wpe0.log',
		'lghd': True,
		#        ---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------1---------2---------3---------4---------5---------6---------7---------8---------9---------0---------
		#			2023-09-13 11:50:57 -0700  INFO	-	-	User [WebDirect-ABC8F](nnnnnnnnn_n@beezwax.net) has been signed out from database Tool.
		#			2025-08-04 08:06:10 -0700  172.130.211.135  127.0.0.1:57874  -  -  INFO  -  -  FileMaker WebDirect is enabled.
		'head': 'TIMESTAMP                  HOSTNAME         CLIENT           ACCOUNT                     MODULE_TYPE  SEVERITY  ERROR  BYTES     MESSAGE',
		'tbst': [27,44,61,89,102,112,119,129],
		'shed': 'TIMESTAMP                 Start T.  End T.  Total Elapsed  Operation         Target  Net Bytes In  Net Bytes Out  Elapsed T.  Wait T.  I/O T.  Client name',
	},
	
	'wpedebug': {
		'path': 'Logs/wpe_debug.log',
		'lghd': False,
		'tbst': 8
	}
}

LOG_SPECS_DARWIN = {
	'install': {
		'path': 'Logs/install.log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'syslog': {
		'path': '!/usr/bin/log',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stderr': {
		'path': 'Logs/stderr',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdout': {
		'path': 'Logs/stdout',
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_LINUX = {
	'fmshelper': {
		'path': 'Logs/fmshelper.log',
		'lghd': True,
			# This log has no consistent format
			#		2025-08-03 20:12:38.182 -0700   Log file /opt/FileMaker/FileMaker Server/Logs/fmshelper.log size: 478 bytes (0 MB), threshold ratio: 0
			#		2025-08-03 20:12:38.185 -0700 === stopSystemWebServer()
			#		(Use `facstart.sh --trace-warnings ...` to show where the warning was created)
			#		Thrift: Sun Aug  3 20:12:47 2025 TNonblockingServer: Serving with 5 io threads.
			#		127.0.0.1 POST /fmi/admin/internal/v1/dbs-notification/xPR2AgRM1TODanCZ56eikiYXcbzvTDdtLIbd9Avs3Z4kuxie - - - ms
			#		Aug 03, 2025 8:12:53 PM org.apache.jasper.servlet.TldScanner scanJars
			#		2025/08/03 20:39:27.0128: [ 2525]:    TRACE:       mongoc: ENTRY: _mongoc_linux_distro_scanner_get_distro():389
		'head': None,
		'tbst': []	# replace any tabs with two spaces
	},

	'nginxaccess': {
		'path': 'NginxServer/logs/nginx-access.log',
		'lghd': True,
		'head': None,
		'tbst': 8
	},

	'nginxerror': {
		'path': 'NginxServer/logs/nginx-error.log',
		'lghd': True,  
		'head': None,
		'tbst': 8
	},

	'stdoutserverscripting': {
		'path': 'Logs/StdOutServerScripting.log',
		'lghd': False,
		'lghd': False,
		'head': None,
		'tbst': 8
	},
	'stdoutserverscripting': {
		'path': 'Logs/StdOutServerScripting.log',
		'lghd': False,
		'lghd': False,
		'head': None,
		'tbst': 8
	},
}

LOG_SPECS_APACHE = {
	'httpaccess': {
		'path': 'HTTPServer/logs/access_log.*',
		'lghd': True,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'REQUESTER        STATUS  LENGTH  IDENTITY      USER          TIMESTAMP                   MESSAGE',
		'tbst': [              17,     25,   33,             47,           61,                          89]
	},
	'httpdctlerr': {
		'path': 'HTTPServer/logs/httpdctl.err',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpdctlout': {
		'path': 'HTTPServer/logs/httpdctl.out',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httperror': {
		'path': 'HTTPServer/logs/error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslaccess': {
		'path': 'HTTPServer/logs/ssl_access_log.*',
		'lghd': True,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'REQUESTER        STATUS  LENGTH  IDENTITY      USER          TIMESTAMP                   MESSAGE',
		'tbst': [              17,     25,   33,             47,           61,                          89]
	},
	'httpsslerror': {
		'path': 'HTTPServer/logs/ssl_error_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST      VERSION CIPHER                      OP   PATH',
		'tbst': 8
	},
	'httpsslrequest': {
		'path': 'HTTPServer/logs/ssl_request_log.*',
		'lghd': False,
		#        [04/Nov/2025:09:48:53 -0800] 127.0.0.1 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256 "GET /fmi/mwpew/wpe/prefs HTTP/1.1" 384
		'head': 'TIMESTAMP                    HOST             VERSION  CIPHER                        OP   PATH',
		'tbst': [                          29,              46,      55,                           85,                      99]
	}
}

LOG_SPECS_DARWIN.update (LOG_SPECS_BASE)
LOG_SPECS_DARWIN.update (LOG_SPECS_APACHE)

LOG_SPECS_LINUX.update (LOG_SPECS_BASE)
LOG_SPECS_LINUX.update (LOG_SPECS_APACHE)	# although not default, Apache can be used on Linux

LOG_SPECS_WINDOWS = LOG_SPECS_BASE

LOG_SPECS_ALL = {
	'Darwin': LOG_SPECS_DARWIN,
	'Linux': LOG_SPECS_LINUX,
	'Windows': LOG_SPECS_WINDOWS
}

LOG_SPECS = LOG_SPECS_ALL [platform.system()]


LOG_CHOICES = list (LOG_SPECS.keys())
LOG_CHOICES.sort()

# Even if using RawDescriptionHelpFormatter, argparse tries hard to strip leading or trailing whitespace.
HELP_EPILOGUE = """Log names supported on this platform are:\n\n  """ + '\n  '.join (LOG_CHOICES) + '\n '

ALL_CHOICES = LOG_CHOICES  # TODO: need to add in choices that aren't log names?

CLARIS_CONFIG = """
{
	"AlwaysPrint": true,
	"ForceOutput": true
}
"""

class terminal_colors:
   PURPLE = '\033[95m'
   CYAN = '\033[96m'
   DARKCYAN = '\033[36m'
   BLUE = '\033[94m'
   GREEN = '\033[92m'
   LIGHT_GRAY = '\033[37m'
   YELLOW = '\033[93m'
   RED = '\033[91m'
   BOLD = '\033[1m'
   UNDERLINE = '\033[4m'
   END = '\033[0m'


#
#	calc_row_metrics
#

def calc_row_metrics (numLogs, logLinesCountVal) -> tuple:
	"""
	Using the user supplied quantity,the current terminal screen size,
	and the number of logs we have to print, determine the number of rows
	we should attempt to print.
	"""
	
	screenLinesCount = 0
	num_mode = None

	if logLinesCountVal.count ('s'):
		screensNumStr = logLinesCountVal.replace('s','')
		num_mode = 's'
					
		if len (screensNumStr) == 0:
			screensNum = 1		# just an 's' by itself counts as 1'
		try:
			screensNum = int (screensNumStr)
		except ValueError:
			print ("Error: invalid number for number parameter")
			return (-1,-1)
			
		screenLinesCount = max ([SCREENROWS * screensNum - 1, 6])
		
		if numLogs == 1:
			lines_per_log = screenLinesCount
		else:
			lines_per_log = screenLinesCount // numLogs
	
	else:
		try:
			lines_per_log = int (logLinesCountVal)
		except ValueError:
			print ('Error: invalid value of "%s" for number parameter' % logLinesCountVal)
			return (-1,-1,-1)
		
	return (num_mode,lines_per_log,screenLinesCount)


#
#	c h e c k _ e n d p o i n t _ s t a t u s
#

def check_endpoint_status (name: str, host: str, endpoint: str, expectedStatus: int, method='GET', port=80, useSSL=False) -> bool:
	"""
	Check for expected status result for the given endpoint.
	"""
	resultFlag = False
	print ('Checking', name, end='')
	if useSSL:
		conn = http.client.HTTPSConnection (host, port, context=ssl._create_unverified_context(), timeout=4)
	else:
		conn = http.client.HTTPConnection (host, port, timeout=4)
	try:
		conn.request(method, endpoint)
		response = conn.getresponse()
		if response.status in (200,202) or response.status == expectedStatus:
			# In some cases we get a 200 or similar, but there is an error in the XML.
			body = response.read(500).decode('utf-8')
			if body.startswith ('<?xml '):
				match = re.search(r'<error code="(\d+)', body)
				if match:
					if int (match.group(1)) == expectedStatus:
						print (': Responding')
					else:
						print (': Error %s' % match.group(1))
				else:
					print (': Error, Result Missing')
			else:
				print (': Responding')
			resultFlag = True
		else:
			print (':', response.reason)
	except http.client.HTTPException as e:
		print (':', e)
	except Exception as e:
		print (':', e)
	finally:
		conn.close()
	return resultFlag


#
#	p r i n t _ t l s _ v e r s i o n
#

def print_tls_version (host: str, port=443) -> bool:
	"""
	Create connection to server and print the TLS version that was used.
	"""
	
	returnFlag = False
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout (4)
		ssock = context.wrap_socket(sock, server_hostname=host)
		conn = ssock.connect ((host, port))
		returnFlag = True
		print('SSL/TLS version:', ssock.version())
	except ssl.SSLError as e:
		print ('  ', e)
	finally:
		sock.close()
	
	return returnFlag

        
def check_tcp_status (name: str, host: str, port):
	"""
	Print the result of testing if TCP connection can be opened to the given host's port.
	"""
	resultFlag = False
	print ('Checking', name, end='')
	
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout(4)	# 4 seconds
		result = sock.connect_ex((host,port))
		if result == 0:
			print (': Responding')
		else:
			print (': Error %i, Could not open port %i' % (result, port))
	except socket.gaierror:
		print (': Could not resolve host "%s"' % host)
	finally:
		sock.close()


#
#	c h e c k _ c o n n e c t i v i t y
#

def check_connectivity() -> str:
	"""
	Check both internal and external interfaces for a response from the DAPI endpoint.
	"""
	
	internAddr = '127.0.0.1'
	externAddr = get_local_ip()

	print ()
	print ('Internal Interface:', internAddr)
	print ('External Interface:', externAddr)
	print ()
	print ('=== Web SSL Usage ===')
	try:
		print_tls_version (externAddr, 443)
	except TimeoutError:
		print ('HTTPS connection timed out')
	else:
		print_certificate_info ('External Web', externAddr)
	print ()
	print ('=== Connectivity Tests ===')
	
	# FMP/FMGO CLIENT
	check_tcp_status ('FMP Client - Internal', internAddr, 5003)
	check_tcp_status ('FMP Client - External', externAddr, 5003)
	
	# ADMIN API
	adminEndpoint = '/fmi/admin/api/v2/server/metadata'
	check_endpoint_status ('Admin API - Internal', internAddr, adminEndpoint, 400, method='POST', port=16001)
	check_endpoint_status ('Admin API - External', externAddr, adminEndpoint, 400, method='POST', port=443, useSSL=True)
	
	# ADMIN CONSOLE
	facEndpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Internal', internAddr, facEndpoint, 200, port=16001)
	check_endpoint_status ('Admin Console - External', externAddr, facEndpoint, 200, port=443, useSSL=True)
	
	# ADMIN CONSOLE - WORKER
	facEndpoint = '/admin-console/signin'
	check_endpoint_status ('Admin Console - Worker - Internal', internAddr, facEndpoint, 200, port=16003)
	
	# CATALINA/CWP
	check_tcp_status ('CWP - Port 9889', internAddr, 9889)
	xmlEndpoint = '/fmi/xml'
	check_endpoint_status ('CWP - XML - Internal', internAddr, xmlEndpoint, 954, port=16021)
	check_endpoint_status ('CWP - XML - External', externAddr, xmlEndpoint, 954, port=443, useSSL=True)
	check_endpoint_status ('CWP - wpem - Internal', internAddr, 'fmswpem/', 400, port=16002)
	check_endpoint_status ('CWP - wpem - External', externAddr, '/fmi/mwpem/', 400, port=443, useSSL=True)

	# DATA API
	dapiEndpoint = '/fmi/data/vLatest/productInfo'
	check_endpoint_status ('Data API - Internal', internAddr, dapiEndpoint, 200, port=3000)
	check_endpoint_status ('Data API - External', externAddr, dapiEndpoint, 200, port=443, useSSL=True)
	
	# JDBC
	check_tcp_status ('JDBC - Internal', internAddr, 2399)
	check_tcp_status ('JDBC - External', externAddr, 2399)
	
	# ODATA API
	odataEndpoint = '/fmi/odata/v4'
	check_endpoint_status ('OData API - Internal', internAddr, odataEndpoint, 401, port=3001)
	check_endpoint_status ('OData API - External', externAddr, odataEndpoint, 401, port=443, useSSL=True)

	# docws/fmws handler
	xmlEndpoint = '/fmws/serverinfo'
	check_endpoint_status ('CWP - XML - Internal', internAddr, xmlEndpoint, 954, port=1895)
	check_endpoint_status ('CWP - XML - External', externAddr, xmlEndpoint, 954, port=443, useSSL=True)	# this may get removed

	# WEBD
	webdEndpoint = '/fmi/webd/'
	check_endpoint_status ('WebDirect - Internal', internAddr, webdEndpoint, 200, port=16021)
	check_endpoint_status ('WebDirect - External', externAddr, webdEndpoint, 200, port=443, useSSL=True)

	# alternate: curl -vk -X DELETE https://127.0.0.1/fmi/admin/api/v2/user/auth/abcdef0123456789
	print ()


#
#	c h e c k _ f i l e _ v a l i d i t y
#

def check_file_validity (path: str):
	'''
	Check whether the a given file exists, readable and is a file.
	Return None if no issue found, otherwise return an error message.
	'''
	if not os.access(path, os.F_OK):
		raise FileCheckError ("'%s' does not exist" % (path))
	if not os.access(path, os.R_OK):
		raise FileCheckError ("'%s' is not readable" % (path))
	if os.path.isdir(path):
		raise FileCheckError ("'%s' is a directory" % (path))

#
#	c l a r i s _ c o n f i g _ r e m o v e
#

def claris_config_remove(targetDir: str) -> bool:
	
	"""
	Remove ClarisConfig.json from target directory.
	"""

	configPath = os.path.join(targetDir, "ClarisConfig.json")
	print ()

	try:
		os.remove(configPath)
	except FileNotFoundError:
		print ('No ClarisConfig.json file was found to remove.')
		print ()
		return False
	except PermissionError:
		print("Error: Could not remove", configPath)
		print ('Adjust permissions or run fmslogs with elevated privileges.')
		print ()
		return False
	
	print ('The ClarisConfig.json file has been removed.')
	print ('Restart FMS to apply the change.')
	print ()
	return True

#
#	c l a r i s _ c o n f i g _ w r i t e
#

def claris_config_write(targetDir: str) -> bool:
	
	"""
	Write CLARIS_CONFIG text to a ClarisConfig.json file in target directory.
	If the file already exists, attempt to move it aside (renames with .bak).
	"""

	configPath = os.path.join(targetDir, "ClarisConfig.json")
	print ()

	try:
		try:
			# If there's an existing config file, attempt to move it aside.
			backupPath = configPath + ".bak"
			shutil.move(configPath, backupPath)
		except FileNotFoundError:
			pass
		except FileExistsError:
			pass

		with open(configPath, "w") as f:
			f.write(CLARIS_CONFIG)
		
		print ('The ClarisConfig.json file has been created.')
		print ('Restart FMS to apply the change.')
		print ()
		return True
	except PermissionError:
		print("Error: Could not write to", configPath)
		print ('Adjust permissions or run fmslogs with elevated privileges.')
		print ()
		return False
	
	return True


#
#	c o m p i l e _ f i l t e r
#

def compile_filter(regex: str) -> bool:
	
	global FILTER_REGEX
	isValid = False
	
	try:
		FILTER_REGEX = re.compile(regex)
		isValid = True
	except re.error as e:       # aliased to PatternError as of 3.13
		print(f"Error: bad regex expression: {e}\n")
	
	return isValid


def convert_filemaker_path (fmPath: str) -> str:
	"""
	Convert a FileMaker path (filemac:, filelinux:, filewin:) to a POSIX style path that can be used in Python.
	"""

	# Example paths:
	"""
	filemac:/internal-vol/Library/FileMaker Server/Data/Databases/	# drive name used as prefix
	filelinux:/opt/FileMaker/FileMaker Server/Data/Databases/
	filewin:/C:/Program Files/FileMaker/FileMaker Server/Data/Databases/
	"""

	if fmPath.startswith ('filemac:/'):
		return fmPath.replace ('filemac:/', '/Volumes/')
	elif fmPath.startswith ('filelinux:/'):
		return fmPath.replace ('filelinux:/', '/')
	elif fmPath.startswith ('filewin:/'):
		winPath = fmPath.replace ('filewin:/', '')
		winPath = winPath.replace ('/', '\\')
		return winPath
	else:
		return fmPath
	
#
#	e d i t _ l o g
#

def edit_log (log_name: str):

	# TODO: needs Windows version for first & third options
	# TODO: use vi as second fallback if no nano?
	# TODO: open in GUI editor if available
	
	# Below will return None if syslog or glob can't find file.
	log_path = get_log_path (log_name)
	
	if log_path:
		if 'EDITOR' in os.environ:
			exitCode = subprocess.call ('$EDITOR "' + log_path + '"', shell=True)
		elif platform.system() == 'Darwin' and subprocess.call (['/usr/bin/pgrep','loginwindow']) == 0:
			exitCode = subprocess.call (['/usr/bin/open', '-e', log_path])
		elif platform.system() == 'Windows':
			exitCode = subprocess.call (['notepad', log_path])		
		else:
			exitCode = subprocess.call (['/usr/bin/nano', log_path])
	
	sys.exit (exitCode)


#
#	e x p a n d _ t a b s _ f o r _ l i n e
#

def expand_tabs_for_line (log_name: str, line: str) -> str:
	"""
	Expands tabs in the string `line` using the given tabstops.
	`tabstops` can be a list of column positions (e.g., [4, 8, 12]) or a single integer for fixed tab width.
	TODO: truncate if over column width?
	"""
	
	try:
		if SUCCINCT_MODE:
			tabstops = LOG_SPECS [log_name]['shtb']
		else:
			tabstops = LOG_SPECS [log_name]['tbst']
	except KeyError:
		tabstops = 8
	
	if isinstance(tabstops, int):
		# All tab stops are the same size.
		return line.expandtabs(tabstops)

	if log_name in ['admin','adminapi']:
		# This log doesn't use tabs, but instead uses 2 or 3 spaces to separate columns.
		respace = re.compile (r'  +')
		line = respace.sub ('\t', line)
	
	if log_name in ['httpaccess','httpsslaccess']:
		#                 host   iden user  time    zone  requ  stat  size
		match = re.match (r'(.*) (.*) (.*) \[(.*) (.*)\] "(.*)" (\d*) (\d*)', line)
		line = match.group(1) + '\t' + match.group(7) + '\t' + match.group(8) + '\t' + match.group(2) + '\t' + match.group(3) + '\t' + match.group(4) + ' ' + match.group(5) + '\t' + match.group(6) + '\n'
	
	if log_name in ['httpsslrequest']:
		#                   timest zone   host vers ciph oppath len
		match = re.match (r'\[(.*) (.*)\] (.*) (.*) (.*) "(.*)" (\d*)', line)
		line = match.group (1) + ' ' + match.group(2) + '\t' + match.group(3) + '\t' + match.group(4) + '\t' + match.group(5) + '\t' + match.group(6) + '\n'
	
	result = []

	parts = line.split('\t')
		
	col = 0
	tab_iter = iter(tabstops)
	next_tab = next(tab_iter, None)
	for i, part in enumerate(parts):
		result.append(part)
		col += len(part)
		if i < len(parts) - 1:
			if next_tab is not None and col < next_tab:
				spaces = next_tab - col
				result.append(' ' * spaces)
				col = next_tab
				next_tab = next(tab_iter, None)
			else:
				result.append('  ')	# pad two spaces if no stop specified
				col += 1
	
	if TRUNCATE_MODE:
		line = ''.join(result)
		if len (line) > SCREENCOLS:
			return line [:SCREENCOLS-1] + "â€¦" + '\n'
		else:
			return line
	else:
		return ''.join(result)


class FileCheckError(Exception):
	def __init__(self, msg):
		self.message = msg
	def __str__(self):
		return self.message


#
#	f i n d _ f i r s t _ t i m e s t a m p
#

def find_first_timestamp (file_path: str, timestamp: datetime.datetime) -> int:
	
	"""
	Scan file until the first log timestamp equal or greater than the search
	timestamp is found, returning the line number (base 1) of matching line.
	Note that a few logs don't emit timestamps consistently. 
	If a match is never found -1 is returned.
	"""
	
	lineResult = -1
	line_num = 0
	
	while True:
		lineTS = None
		line_num += 1
		line = linecache.getline (file_path, line_num)
		if line == '': break
		# Sniff the line to guess the date format.
		try:
			if line[4] == '-' and line [24] == '-':
				# Access, ClientStats, Event, Stats, etc.
				# 2025-10-27 04:13:24.101 -0700	Information	228	tool.beezwax.net	The previous log file reached maximum size, and was renamed to "Access-old.log".
				lineTS = datetime.datetime.fromisoformat(line [:23])
			elif line [4] == '/' and line [24] == ':':
				# 2025/10/25 17:49:09.0162
				lineTS = datetime.strptime(line [:23], "%Y/%m/%d %H:%M:%S.%f")
			elif line [6] == ',' and line [22] in 'APM':
				# Sep 11, 2025 12:40:52 PM org.atmosphere.cpr.AtmosphereFramework addInterceptorToAllWrappers
				# Oct 22, 2025 2:16:56 PM org.atmosphere.util.IOUtils guestRawServletPath
				lineTS = datetime.datetime.strptime(line [:24].rstrip(), '%b %d, %Y %I:%M:%S %p')
			elif line [:7] == 'Thrift:':
				# Thrift: Sat Jun  7 10:47:03 2025
				lineTS = datetime.datetime.strptime (line [8:32], '%a %b %d %H:%M:%S %Y')
			else:
				continue
		
		# Skip over any lines that have too few characters (very few) or where we can't evaluate date.
		
		except IndexError:
			continue
		#except ValueError:
		#	continue

		# If we reached the timestamp, go into next while loop to match by text value.
		if lineTS != None and lineTS >= timestamp:
			lineResult = line_num
			break
	
	return lineResult


def follow_file(some_file):
	"""
	was tail_F
	Capture output as it is added to the file.
	"""
	# https://gist.github.com/pylixm/e6bd4f5456740c12e462eecbc66692fb # tail/follow a file
	
	first_call = True
	while True:
		try:
			with open(some_file) as input:
				if first_call:
					input.seek(0, 2)
					first_call = False
				latest_data = input.read()
				while True:
					if '\n' not in latest_data:
						latest_data += input.read()
						if '\n' not in latest_data:
							yield ''
							if not os.path.isfile(some_file):
								break
							continue
					latest_lines = latest_data.split('\n')
					if latest_data[-1] != '\n':
						latest_data = latest_lines[-1]
					else:
						latest_data = input.read()
					for line in latest_lines[:-1]:
						yield line + '\n'
		except IOError:
			yield ''


#
#	g e t _ f o l d e r _ s i z e
#

def get_folder_size(path='.', exclude_dir=None) -> int:

	total = 0
	if path != None:
		try:
			for entry in os.scandir(path):
				if entry.is_file():
					total += entry.stat().st_size
				elif entry.is_dir():
					if entry.name != exclude_dir:
						total += get_folder_size(entry.path, exclude_dir=exclude_dir)
		except PermissionError:
			print ('Error: Permission denied for', path)
		except FileNotFoundError:
			return -1

	return total


def get_db_directories() -> tuple:
	"""
	Return a tuple of all four possible database directory paths and two possible container paths by extracting
	the locations in the FMS Event.log file, and if necessary, the -old log.
	There is a possibility that this information may not be present in the either log file,
	in which case the default path will be None.
	"""
	
	additional_path1 = None
	remote_container_path1 = None
	additional_path2 = None
	remote_container_path2 = None

	default_pathRE = re.compile (r'Default database location: (file.*)')
	secure_pathRE = re.compile (r'Secure database folder enabled: (file.*)')
	additional_path1RE = re.compile (r'Additional database folder \[1\] enabled: (file.*)')
	additional_path2RE = re.compile (r'Additional database folder \[2\] enabled: (file.*)')
	remote_container_path1RE = re.compile (r'Separate remote container folder \[1\] enabled, .*: (.*)')
	remote_container_path2RE = re.compile (r'Separate remote container folder \[2\] enabled, .*: (.*)')

	"""
	Default database location: filemac:/internal/Library/FileMaker Server/Data/Databases/
	Secure database folder enabled: filemac:/internal/Library/FileMaker Server/Data/Secure/
	Additional database folder [1] enabled: filemac:/internal/Library/FileMaker Server/Data/DBs/
	Additional database folder [2] disabled.
	Separate remote container folder [1] enabled, without backups: filelinux:/FileMakerData/Containers/
	Additional database folder [2] disabled.
	"""

	event_log_path = get_log_path ('event')
	default_path = scan_file_last_match (event_log_path, default_pathRE)

	if default_path == None:
		# Nothing in current log file, see if we can find in -old log.
		event_log_path = event_log_path.replace ('.log', '-old.log')
		default_path = scan_file_last_match (event_log_path, default_pathRE)
	default_path = convert_filemaker_path (default_path)

	secure_path = scan_file_last_match (event_log_path, secure_pathRE)
	secure_path = convert_filemaker_path (secure_path)

	# These paths are optional.
	additional_path1 = scan_file_last_match (event_log_path, additional_path1RE)
	if additional_path1:
		additional_path1 = convert_filemaker_path (additional_path1)
		remote_container_path1 = scan_file_last_match (event_log_path, remote_container_path1RE)
		if remote_container_path1: remote_container_path1 = convert_filemaker_path (remote_container_path1)
		print ('remote_container_path1:', remote_container_path1)

		additional_path2 = scan_file_last_match (event_log_path, additional_path2RE)
		if additional_path2:
			additional_path2 = convert_filemaker_path (additional_path2)
			remote_container_path2 = scan_file_last_match (event_log_path, remote_container_path2RE)
			if remote_container_path2:
				remote_container_path2 = convert_filemaker_path (remote_container_path2)
			
	return (default_path, secure_path, additional_path1, additional_path2, remote_container_path1, remote_container_path2)


#
#	g e t _ l o c a l _ i p
#

def get_local_ip() -> str:
	s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
	try:
		s.settimeout (4)
		# doesn't even have to be reachable
		s.connect(('8.8.8.8', 1))
		addr = s.getsockname()[0]
	except Exception as e:
		print (e)
		addr = '127.0.0.1'
	finally:
		s.close()
	return addr

#
#	g e t _ n e t w o r k _ a d d r e s s
#

def get_network_address() -> str:
	"""
	Extract the last logged network addresses from FMS' Event.log.
	Result may contain multiple space delimited addresses.
	"""
	
	eventPath = get_log_path ('event')
	match = scan_file_last_match (eventPath, r'Network address.*: (.*)')
	if match == None:
		return 'No matches in current Event.log'
	else:
		return match


#
#	g e t _ l o g _ p a t h
#

def get_log_path (log: str) -> str:
	# TODO: convert Windows paths
	#print ('basePath:', BASE_PATH)
	#print ('log_path', LOG_PATHS [log])
	pathSuffix = LOG_SPECS [log]['path']

	if pathSuffix[0] == '!':
		fullPath = None
	elif pathSuffix[0] != '/':
		fullPath = BASE_PATH + '/' + LOG_SPECS[log]['path']
	else:
		fullPath = LOG_SPECS[log]['path']

	if '*' in pathSuffix:
		path_list = glob.glob (fullPath)
		if path_list != []:
			path_list.sort(reverse=True) # so that newest file is first
			fullPath = path_list[0]
		else:
			fullPath = None
	
	return fullPath


def get_file_timestamps (path: str) -> tuple:
	"""Return a file's creation and modification timestamps"""
	return (os.path.getmtime(path), pathlib.Path(path).stat().st_mtime)

	
def get_fms_version() -> str:
	"""
	Fetch the version of FileMaker Server currently installed.
	"""
	# TODO: handle if not installed
	
	if platform.system() == 'Darwin':
		fmsOutLines = subprocess.run(['/usr/sbin/pkgutil', '--pkg-info','com.filemaker.fms.worker.pkg'], capture_output=True, text=True).stdout.split('\n')
		return fmsOutLines[1].split(': ')[1]
		
	elif platform.system() == 'Linux':
		fmsOutLines = subprocess.run(['/usr/bin/apt-cache', 'policy', 'filemaker-server'], capture_output=True, text=True).stdout.split('\n')
		return fmsOutLines[1].split(': ')[1]
	
	elif platform.system() == 'Windows':
		return 'not implemented'
	
	return 'Unknown platform'


def get_vaadin_version() -> str:
	"""
	Get the Vaadin version based off of the name of the installed vaadin-server lib file.
	"""
	
	# Only match server-# so that -gae or -mpr is not included. 
	vaadinLibServerPath = BASE_PATH + '/Web Publishing/publishing-engine/jwpc-tomcat/fmi/WEB-INF/lib/vaadin-server-[1-9]*.fmi.jar'
	path_list = glob.glob (vaadinLibServerPath)
	if path_list != []:
		match = re.search (r'server-(\d*\.\d*\.\d*)', path_list[0])
		vaadin_str = match.group(1)
	else:
		vaadin_str = '<missing>'
	
	return vaadin_str


#
#	h a n d l e _ s e t
#

def handle_set (verb: str, noun: str):
	"""
	Execute the config changes specified in parameters.
	"""
	
	validOptions = False	# may still fail for other reasons
	
	if noun == 'debuglogging':
		if platform.system() in ['Darwin', 'Linux']:
			claris_config_dir = BASE_PATH + '/Database Server/bin'
		else:
			claris_config_dir = BASE_PATH + '/Database Server'
		if verb == 'enable':
			claris_config_write (claris_config_dir)
		elif verb == 'disable':
			claris_config_remove (claris_config_dir)
		else:
			print ('Error: unrecognized parameter for debuglogging:', noun)
			print ('Use "enable" or "disable" instead.')
		
		validOptions = True
	
	if not validOptions:
		print ('Error: unrecognized parameter(s) for set:', verb, noun)


def init_curses():
    global STDSCR, SCREENCOLS, SCREENROWS
    STDSCR = curses.initscr()
    curses.noecho()
    SCREENROWS, SCREENCOLS = STDSCR.getmaxyx()
    STDSCR.scrollok(1)


#
#	i n i t _ p a r s e r
#

def init_parser() -> argparse.ArgumentParser:
	"""Setup parameters used for command interface. Does not attempt to parse."""

	parser = argparse.ArgumentParser(
		prog='fmslogs',
		add_help=False,
		formatter_class=argparse.RawDescriptionHelpFormatter,
		description='View FileMaker Server logs and set logging options.')

	parser.add_argument('-b', '--begin', nargs=1, help='start at first message on or after time or time interval')
	parser.add_argument('-c', '--check-connectivity', dest='check_connectivity', action='store_true', help='check connectivity of API endpoints')
	parser.add_argument('-d', '--databases', action='store_true', help='display database & container directories')
	parser.add_argument('-e', '--edit', nargs=1, help='open the log file using the command defined by $EDITOR')
	parser.add_argument('-f', '--filter', nargs=1, help='only return lines matching regex expression')
	parser.add_argument('-h', '--head', action='store_true', help='display the first lines of log files')
	parser.add_argument('-H', '--headers-off', dest='headers_off', action='store_true', help='turn off headers for all logs')
	parser.add_argument('--help', action='help', help='display command details')
	parser.add_argument('-l', '--list', action='store_true', help='list all log files, including size, date created & modified, sorted by modification time')
	parser.add_argument('-m', '--merge', action='store_true', help='combine output of two or more logs based on the message timestamps')
	parser.add_argument('-n', '--number', nargs=1, default=['1s'], help='range or number of lines to print')
	parser.add_argument('-p', '--process-info', action='store_true', dest='process_info', help='list FMS related processes and their stats')
	parser.add_argument('-P', '--password', help='FMS console or SSH password')
	parser.add_argument('-S', '--set', nargs=2, help='change log configuration option')
	parser.add_argument('-s', '--succinct', action='store_true', help='strip less useful details from log output')
	parser.add_argument('--ssh', nargs=1, help='use the connection string to fetch logs from remote server')
	parser.add_argument('-t', dest='tail', action='store_true', help='wait for any new messages')
	parser.add_argument('--tail', dest='tail', type=float, nargs=1, default=[-1.0], help='wait for any new messages, specifying number of seconds to wait for each file')
	parser.add_argument('--truncate', action='store_true', help='cut off any output if beyond width of screen')
	parser.add_argument('-U', '--user', help='FMS console or SSH account name')
	parser.add_argument('-N', '--network', action='store_true', help='list connections and their status') # TODO: throwing "expected log name" error
	parser.add_argument('-V', '--version', action='store_true', help='version info for fmslogs and FMS')
	# Hack to avoid error if there is only an option specified but no positional argument
	parser.add_argument('logs', nargs='*', help='log name to display')
	#parser.add_argument('log2', nargs='?', help='additional log to display')

	parser.epilog = HELP_EPILOGUE
	
	return parser


#
#	l i s t_ a c t i v e _ p o r t s
#

def list_active_ports() -> list:
	'''
	List the open (listening or established) ports for processes using the fmserver user.
	'''
	
	currPlatform = platform.system()
	openList = []
	
	if currPlatform == 'Darwin' or currPlatform == 'Linux':
		if os.getuid() != 0:
			print ('Running lsof command with sudo, you may be prompted for credentials.')
		# Could've used -F here, but seems easier to process formatted output.
		lsofLines = subprocess.run(['sudo','lsof', '-P', '-u', 'fmserver', '+c15'], capture_output=True, text=True).stdout.split('\n')
		for line in lsofLines:
			accessMatch = re.search ('LISTEN|ESTABLISHED',line)
			if accessMatch == None: continue
			
			if accessMatch.group(0) == 'LISTEN':
				items = line.split()
				portNum = int (re.search(r'\d+',items[8]).group())
				# process name, IP version, connection, port, type
				openList.append (('L', items[0], items[4], items[8], portNum))
			elif accessMatch.group(0) == 'ESTABLISHED':
				# fmserverd 90834       fmserver   91u     IPv6  0x814de6ad3239a30       0t0                 TCP 172.16.184.175:5003->172.16.184.175:59835 (ESTABLISHED)
				items = line.split()
				# select digits after first colon
				portNum = int (re.search(r':\d+',items[8]).group()[1:])
				# process name, IP version, connection, port
				openList.append (('E', items[0], items[4], items[8], portNum))
				
		# remove duplicates (eg, nginx)
		openList = list (set (openList))
		openList.sort(key=lambda tupl: tupl[4])
	return openList


#
#	l i s t _ c r a s h _ r e p o r t s
#

def list_crash_reports() -> list:
	"""
	Return the full path of any recent crash report files.
	"""
	pass


def mac_log_tail (minutesBack: str):
	
	subprocess.run(['log', 'show', '--style', 'json', '--last', minutesBack + 'm', '--info', '--predicate' 'process=="fmserverd"'], capture_output=True, text=True).stdout.split('\n')

#
#	p r i n t _ s s l _ o p t i o n s_ c r y p t o
#

def print_ssl_options_crypto (name: str, host: str, port=443) -> bool:
	
	# Requires non-default module.
	from cryptography import x509
	from cryptography.hazmat.backends import default_backend
	
	returnFlag = False
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	try:
		sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
		sock.settimeout (4)
		ssock = context.wrap_socket(sock, server_hostname=host)
		conn = ssock.connect ((host, port))
		returnFlag = True
		print('   Security Protocol:', ssock.version())
		cert_der = ssock.getpeercert(binary_form=True)
		print (cert_der,'\n')
		cert = x509.load_der_x509_certificate(cert_der, default_backend())
		print ('  ', cert)
	except ssl.SSLError as e:
		print ('  ', e)
	finally:
		sock.close()
	
	return returnFlag


#
#	p r o c e s s _ i n f o
#

def process_info ():
	"""
	Print the FMS related processes and their stats.
	"""
	
	print ()
	
	if platform.system() == 'Darwin':
		ps_result = subprocess.run(['ps', '-u', 'fmserver', '-o', 'pid,%cpu,rss,stime,time,majflt,stat,comm'], capture_output=True, text=True).stdout.split('\n')
	elif platform.system() == 'Linux':
		ps_result = subprocess.run(['ps', '-u', 'fmserver', '-o', 'pid,%cpu,rss,stime,times,maj_flt,stat,comm'], capture_output=True, text=True).stdout.split('\n')
	else:
		ps_result = ['not implemented'] # Windows
	
	for proc in ps_result:
		print (proc)


#
#	r e a d _ t a i l
#

def read_tail (file_path: str, lines_from_end: int) -> list:
	"""
	Scan lines in file, return up to linesFromEnd line numbers from the end of file.
	Line numbers are base 1 for use with linecache.getline.
	"""
	
	check_file_validity (file_path)
	line_num = 1
	matching = []
	
	while True:
		line = linecache.getline (file_path, line_num)
		if line == '': break
		matching.append (line_num)
		line_num += 1
	
	#print ('line_num:',line_num, 'matching:',len (matching))
	return matching [-lines_from_end:]


#
#	r e a d _ t a i l _ f i l t e r e d
#

def read_tail_filtered (file_path: str, lines_from_end: int) -> list:
	"""
	Search file for any matching lines, return up to linesFromEnd
	line numbers from the end of file that match. Line numbers are base 1
	for use with linecache.getline.
	"""
	
	check_file_validity (file_path)
	
	matching = []
	line_num = 1
	while True:
		line = linecache.getline (file_path, line_num)
		if line == '': break
		if FILTER_REGEX.search (line):
			matching.append (line_num)
		
		line_num += 1
		
	return matching [-lines_from_end:]


#
#	r e a d _ t a i l _ f i l t e r _ a n d _ t i m e
#

def read_tail_filtered_and_time (file_path: str, lines_from_end: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	From there, then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""
	
	check_file_validity (file_path)
	matching = []
	
	# First, find the first line containing some kind of message date
	# that is on or after our start date.
	
	line_num = find_first_timestamp (file_path, TIMESTAMP_START)
	
	if line_num > 0:
		# Now, filter anything after start date.
		while True:
			line = linecache.getline (file_path, line_num)

			if line == '': break

			if FILTER_REGEX.search (line):
				matching.append (line_num)
			line_num += 1
	
	# Cut result down to no more than requested lines from end of file.
	return matching [-lines_from_end:]


#
#	r e a d _ t a i l _ t i m e
#

def read_tail_time (file_path: str, linesFromEnd: int) -> list:
	"""
	Search file for any matching lines that are on or after the matching timestamp.
	Then return line numbers from the end of file that match the text filter.
	Line numbers are base 1 for use with linecache.getline.
	"""

	check_file_validity (file_path)
	matching = []
	
	# First, find the first line containing some kind of message date
	# that is on or after our start date.
	
	line_num = find_first_timestamp (file_path, TIMESTAMP_START)

	# Find the last line
	while True:
		line = linecache.getline (file_path, line_num)
		if line == '': break
		# TODO: purge line list when it gets too big
		matching.append (line_num)
		line_num += 1

	# Cut result down to no more than requested lines from end of file.
	return matching [-linesFromEnd:]


#
#	p a r s e _ b e g i n _ t i m e
#

def parse_begin_time (timeStr: str) -> datetime.datetime:
	"""
	Convert the various possible time values into a timestamp value. Some
	possible values could be '30s' for 30 seconds from now, or '14:00:10'
	for 10 secs past 2pm today, or '2025-11-04 14:00' for 2pm on Nov 4.
	"""
	
	newTime = None
	
	# Duration value or a timestamp?
	m = re.search('[0-9]{0,6}[smhd]', timeStr)
	if m.group(0) != None:
		if len (timeStr) == len (m.group(0)):
			newTime = datetime.datetime.now()
			unit = m.group(0)[-1]
			unitCount = 1
			
			if len (m.group(0)) > 1:
				unitCount = max (int (m.group(0)[:-1]), 1)
			if unit == 's':
				newTime -= datetime.timedelta(seconds=unitCount)
			elif unit == 'm':
				newTime -= datetime.timedelta(minutes=unitCount)
			elif unit == 'h':
				newTime -= datetime.timedelta(hours=unitCount)
			elif unit == 'd':
				# Start days at midnight
				newTime = newTime.replace (hour=0, minute=0, second=0, microsecond=0)
				newTime -= datetime.timedelta(days=unitCount-1)

			else:
				print ('Error: invalid duration')
				sys.exit(9)
	
	print (newTime, unitCount)
	return newTime

#
#	p r i n t _ l o g
#

def print_log (log_name: str, count: int):

	lines_printed = -1
	
	if OUTPUT_MODE is OutputMode.TAIL:
		lines_printed = print_tail (log_name, count, SHOW_HEADERS, SUCCINCT_MODE)
	
	elif OUTPUT_MODE is OutputMode.HEAD:
		lines_printed = print_head (log_name, count, SHOW_HEADERS, SUCCINCT_MODE)
	else:
		print ('Error: unknown output mode')
	
	return lines_printed


#
#	p r i n t _ l o g _ i n f o
#

def print_log_info():
	"""Print one line per supported log with path, size, creation & mod timestamps."""

	print()
	print ('LOG NAME                    SIZE  CREATED                    MODIFIED                   PATH')

	for log in LOG_CHOICES:
		fullPath = get_log_path (log)
				
		modTime = 0;
		#TODO: check for permissions issue

		if fullPath:
			try:
				modTime = os.path.getmtime(fullPath)
			except FileNotFoundError:
				pass
		
		if modTime > 0:
			modTimestamp = time.ctime(modTime)
			
			statInfo = os.stat(fullPath)
			
			# As of Python 3.12, st_birthtime is only available on macOS & Windows
			# ubuntu: stat -c "%w" Access.log
			# macos:  stat -f "%B"
			
			if platform.system() == 'Linux':
				# TODO: Replace with a more accurate method unless st_birthtime fixed in Python 3.15
				createTimestamp = subprocess.run(['stat', '-c', '%w', fullPath], capture_output=True, text=True).stdout[:19]
			else:
				createTimestamp = time.ctime(statInfo.st_birthtime)
				#createTimestamp = datetime.datetime.fromtimestamp(statInfo.st_birthtime).isoformat()
			
			size = statInfo.st_size
			print('{:21} {:>10}  {:<25}  {:<25}  {:<40}'.format (log, size, createTimestamp, modTimestamp, fullPath))
		else:
			if fullPath == None:
				print('{:21}             ---'.format (log))
			else:
				print('{:21}             <missing>'.format (log))
	
	print ()


#
#	p r i n t _ d b _ d i r s _ i n f o
#
 
def print_db_dirs_info():
	"""
	Print the database folder paths extracted from Event.log.
	"""
	RCDIR = 'RC_Data_FMS'

	(default_path, secure_path, additional_path1, additional_path2, remote_container_path1, remote_container_path2) = get_db_directories()
	
	print ()
	print ('=== DATABASE DIRECTORIES ===')

	# DEFAULT
	db_size = get_folder_size (default_path, exclude_dir=RCDIR)
	print ('Default:                  {:16,}  {}'.format (db_size, default_path))

	with Spinner ('  ' + RCDIR + ':           '):
		rc_path = os.path.join (default_path, RCDIR)
		rc_size = get_folder_size (rc_path)
	print ('{:16,}  {}'.format (rc_size, rc_path))

	# SECURE
	size = get_folder_size (secure_path)
	print ('Secure:                   {:16,}  {}'.format (size, secure_path))

	with Spinner ('  ' + RCDIR + ':           '):
		rc_path = os.path.join (secure_path, RCDIR)
		rc_size = get_folder_size (rc_path)
	if rc_size >= 0:
		print ('{:16,}  {}'.format (rc_size, rc_path))
	else:
		print ('{:>16}  {}'.format ('-', '-'))

	# ADDITIONAL 1
	if additional_path1:
		size = get_folder_size (additional_path1, RCDIR)
		print ('Additional Databases #1:  {:16,}  {}'.format (size, additional_path1))

		with Spinner ('  ' + RCDIR + ':           '):
			rc_path = os.path.join (additional_path1, RCDIR)
			rc_size = get_folder_size (rc_path)
		if rc_size >= 0:
			print ('{:16,}  {}'.format (rc_size, rc_path))
		else:
			print ('{:>16}  {}'.format ('-', '-'))

		with Spinner ('  External Container #1: '):
			rc_size = get_folder_size (remote_container_path1)
		if True: # rc_size >= 0:
			print ('{:16,}  {}'.format (rc_size, remote_container_path1))
		else:
			print ('{:>16}  {}'.format ('-', '-'))
	
	# ADDITIONAL 2
	if additional_path2:
		size = get_folder_size (additional_path2, RCDIR)
		print ('Additional Databases #2:  {:16,}  {}'.format (size, additional_path2))

		with Spinner ('  ' + RCDIR + ':           '):
			rc_path = os.path.join (additional_path2, RCDIR)
			rc_size = get_folder_size (rc_path)
		if rc_size >= 0:
			print ('{:16,}  {}'.format (rc_size, rc_path))
		else:
			print ('{:16}  {}'.format ('-', '-'))

		with Spinner ('  External Container #2: '):
			rc_size = get_folder_size (remote_container_path2)
		print ('{:16,}  {}'.format (rc_size, remote_container_path2))

	print ()


#
#	p r i n t _ l o g _ h e a d e r
#

def print_log_header (log_name:str, succinct: bool) -> int:
	"""
	Print the appropriate column headers depending on the log.
	"""
	
	header_str = None
	line_count = 0

	try:
		if succinct and 'shed' in LOG_SPECS[log_name]:
			header_str = LOG_SPECS [log_name]['shed']
		else:
			header_str = LOG_SPECS [log_name]['head']
	except:
		pass

	if header_str:
		line_count = 1 + header_str.count ('\n')
		print (terminal_colors.BOLD,end='')
		print (header_str)
		print (terminal_colors.END,end='')
	
	return line_count


def print_file_head_faster (file_path: str, lines: int) -> bool:

	"""Print up to the given number of lines of text from the start of the file at the provided path.
	If MAX_READ_LEN is reached, stop output and append a '+'.
	Result is False if there was an error opening or reading the file."""

	result = False

	if lines > 0:
		with open (file_path, 'r') as logfile:
			lines = logfile.readlines (MAX_READ_LEN)
			for line in lines[0:SCREENROWS-1]:
				 print (line, end="")
			#STDSCR.erase()
			#for line in lines:
			#    STDSCR.addstr(line)
			result = True
			if len (lines) == MAX_READ_LEN:
				 # indicate that we reached read limit
				 print ('+++')
	else:
		# We never opened the file, but will consider this a success.
		result = True

	#STDSCR.getch()
	return result

#
#   p r i n t _ f i l e _ h e a d
#

def print_file_head (log_name:str, lines:int) -> bool:
    
	file_path = LOG_SPECS [log_name]['path']
	line_count = 0

	try:
		with open(file_path, "r+b") as f:
			m=mmap.mmap(f.fileno(), 0, prot=mmap.PROT_READ)
			print_log_header (log_name)
			while True:
				line=m.readline()
				if line == '': break
				if FILTER_REGEX.search (line):
					print (line.rstrip())
					line_count =+ 1
	except IOError:
		print ('File Error:', file_path, 'could not be opened')
		return False

	return True

#
#   p r i n t _ h e a d
#

def print_head (log_name: str, count: int, header: bool, succinct: bool) -> bool:
    
	line_list = []
	line_counter = count
	log_path =  get_log_path (log_name)
	
	try:
		check_file_validity (log_path)
	except FileCheckError as e:
		print ('Error:', e)
		return -1
	
	# First, find first line containing some kind of message date
	# that is on or after our start date.
	
	if header:
		header_count = print_log_header(log_name, succinct)
		line_counter -= header_count
	else:
		header_count = 0
	
	if TIMESTAMP_START:
		line_num = find_first_timestamp (log_path, TIMESTAMP_START)
	else:
		if LOG_SPECS[log_name]['lghd']:
			line_num = 2		# skip the column header row
		else:
			line_num = 1
	
	if line_num > 0:
		#print (line_num, count, maxLine)
		
		while True:
			line = linecache.getline (log_path, line_num)
			if line == '': break
			if FILTER_REGEX != None and FILTER_REGEX.search (line):
				line_list.append (line_num)
			else:
				line_list.append (line_num)
			line_counter -= 1
			if line_counter < 1: break
			line_num += 1
	
	if line_list != None and len (line_list) == 0:
		print ('<' + log_name + ' has no messages>')
	
	for line_num in line_list:
		print (expand_tabs_for_line (log_name, linecache.getline (log_path, line_num)), end='')
	
	return len (line_list) + header_count


#
#	p r i n t _ n e t _ s t a t u s
#

def print_net_status():
	
	#' | grep -E "LISTEN" | sort -n -k 8'
	
	open_ports = list_active_ports()
	print ()
	print ('LISTENING PROCESSES')
	print ('PROCESS           VERS  CONNECTION')
	for line in open_ports:
		#print (items[0],items[4],items[8])
		if line[0] == 'L':
			print('{:16}  {:<4}  {:<15}'.format (line[1], line[2], line[3]))
	
	print ()
	print ('CONNECTIONS')
	print ('PROCESS           VERS  CONNECTION')
	for line in open_ports:
		if line[0] == 'E':
			print('{:16}  {:<4}  {:<15}'.format (line[1], line[2], line[3]))
	
	try:
		response = urllib.request.urlopen('http://checkip.amazonaws.com', timeout=5)
		gateway_ip = response.read().decode("utf-8")[:-1]
	except urllib.error.URLError:
		gateway_ip = 'Could not connect to checkip.amazonaws.com'
	
	print ()
	print ('Internet Address:', gateway_ip)
	print ('External Interface:', get_local_ip())
	print ('FMS Network Address(es):', get_network_address())
	print ()


#
#	p r i n t _ c e r t i f i c a t e _ i n f o
#

def print_certificate_info (name: str, host: str, port=443) -> bool:
	"""
	Display the SSL certificate attributes used by host.
	This uses an undocumented interface, as all Python alternatives
	to extract SSL info require additional modules to be installed.
	https://github.com/python/cpython/blob/main/Lib/test/test_ssl.py
	"""
	
	context =  ssl.create_default_context()
	context.check_hostname = False
	context.verify_mode = ssl.CERT_NONE
	
	with tempfile.NamedTemporaryFile (mode='w', encoding='utf8') as cert_file:
		cert_file_path = cert_file.name

		try:
			cert_file.write(ssl.get_server_certificate((host, port)))
			# We don't have to close before _test_decode_cert below as long as we flush first.
			cert_file.flush()
		except ConnectionRefusedError:
			print ('https://%s:%i not responding' % (host,port))
			return False
		except Exception as e:
			print (':', e)
			return False
		
		try:
			cert_dict = ssl._ssl._test_decode_cert(cert_file_path)
		except Exception as e:
			print("Error decoding certificate: {:}".format(e))
		else:
			print ('Subject:')
			print (' ',end='')
			pprint.pprint (cert_dict ['subject'])
			if 'subjectAltName' in cert_dict:
				print (' ',end='')
				pprint.pprint (cert_dict ['subjectAltName'], indent=2)
			print ('Not Before:', cert_dict ['notBefore'])
			print ('Not After:', cert_dict ['notAfter'])
			if 'OCSP' in cert_dict:
				print ('OCSP:', cert_dict ['OCSP'])
			if 'caIssuers' in cert_dict:
				print ('CA Issuers:', cert_dict ['caIssuers'])
			print ('Serial Number:', cert_dict ['serialNumber'])		
			print ('Issuer:')
			pprint.pprint (cert_dict['issuer'], indent=1)
			print ('Version:', cert_dict ['version'])		
	
	return True


#
#	p r i n t _ t a i l
#

def print_tail (log_name: str, count: int, header: bool, succinct: bool) -> int:

	"""
	Print up to 'count' number of lines of text from the end of the file at path.
	Result is False if there was an error opening or reading the file.
	If 'header' is true, display the log headers (if any) as first line.
	If 'succinct' is true, strip less useful info from lines.
	"""
	
	line_list = []
	line_count = count
	
	log_path =  get_log_path (log_name)
	
	try:
		check_file_validity (log_path)
	except FileCheckError as e:
		print ('Error:', e)
		return -1
	
	# TODO: only print headers if there's log output
	if header:
		header_count = print_log_header (log_name, succinct)
		line_count = line_count - header_count
	else:
		header_count = 0
	
	# Below we can files only, creating a list of records to later print.
	
	# JUST DETERMINE START LINE AND USE THAT AS PARAM TO SINGLE READ FUNC?
	
	if TIMESTAMP_START != None:
		if FILTER_REGEX != None:
			line_list = read_tail_filtered_and_time (log_path, line_count)
		else:
			line_list = read_tail_time (log_path, line_count)
	else:
		if FILTER_REGEX != None:
			line_list = read_tail_filtered (log_path, line_count)
		else:
			#print ('unfiltered')
			line_list = read_tail (log_path, line_count)
	
	# the actual line_count is now this:
	line_count = len (line_list)
	
	if line_count > 0:
		if LOG_SPECS[log_name]['lghd'] and line_list[0] == 1:
			# Remove the header line, we'll be using our own
			del line_list[0]

	if succinct and 'shtb' in LOG_SPECS[log_name].keys():
		for line_num in line_list:
			line = strip_line (log_name, linecache.getline (log_path, line_num))
			try:
				print (expand_tabs_for_line (log_name, line), end='')
			except BrokenPipeError:
				# Catch errors if output is piped and the other end closes prematurely (eg, using `head`).
				# Use below to avoid a later error when Python flushes stdout
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				line_count = -line_count
				break

	else:
		tabstops =  LOG_SPECS [log_name]['tbst']
		
		for line_num in line_list:
			try:
				print (expand_tabs_for_line (log_name, linecache.getline (log_path, line_num)),end='')
			except BrokenPipeError:
				devnull = os.open(os.devnull, os.O_WRONLY)
				os.dup2(devnull, sys.stdout.fileno())
				line_count = -line_count
				break
	
	return len (line_list) + header_count


#
#	p r i n t _ v e r s i o n
#

def print_version():
	"""
	Handle a request to display the fmslog version and the versions of various FMS components.
	"""
	
	print ()
	print ('fmslogs', VERSION)
	print ('Latest version at: https://github.com/beezwax/fmslogs')
	print ('Questions or comments: info@beezwax.net')
	print ()
	
	# FMS VERSION
	
	print ('FMS:', get_fms_version())
	
	# JAVA
	try:
		if platform.system() == 'Darwin':
			java_words = subprocess.run([BASE_PATH + '/Web Publishing/java/bin/java', '-version'], capture_output=True, text=True).stderr.split('"')
			print ('Java:', java_words[1])
		elif platform.system() == 'Linux':
			java_words = subprocess.run(['/usr/bin/java', '-version'], capture_output=True, text=True).stderr.split('"')
			print ('Java:', java_words[1])
	except FileNotFoundError:
		print ('Java: <missing>')
	
	# NODE.JS
	
	if platform.system() in ['Darwin', 'Linux']:
		nodeVersion = subprocess.run([BASE_PATH + '/node/bin/node', '-v'], capture_output=True, text=True).stdout[1:]
		print ('Node:', nodeVersion, end='')
	
	# OPENSSL
	
	if platform.system() == 'Darwin':
		open_ssl_words = subprocess.run([BASE_PATH + '/Database Server/bin/openssl', 'version'], capture_output=True, text=True).stdout.split()
		print ('OpenSSL:',open_ssl_words[1],open_ssl_words[2])
	
	elif platform.system() == 'Linux':
		open_ssl_words = subprocess.run(['/usr/bin/openssl', 'version'], capture_output=True, text=True).stdout.split()
		print ('OpenSSL:',open_ssl_words[1],open_ssl_words[2])
	
	# APACHE/NGINX/IIS
	
	if platform.system() == 'Linux' or platform.system() == 'Windows':
		if platform.system() == 'Linux':
			nginx_path = '/usr/sbin/nginx'
		else:
			nginx_path = 'C:\\Programs\\'
		
		try:
			nginxOut = subprocess.run([nginx_path, '-v'], capture_output=True, text=True).stderr
			# nginx version: nginx/1.28.0\n
			nginxVers = nginxOut.split ('/')[1][:-1]
		except FileNotFoundError:
			nginxVers = 'not present'
		
		print ('NGINX:',nginxVers)
	
	if platform.system() == 'Darwin':
		httpd_path = '/usr/sbin/httpd'
		try:
			httpd_out = subprocess.run([httpd_path, '-v'], capture_output=True, text=True).stdout
			# Server version: Apache/2.4.62 (Unix)
			# Server built:   Nov  8 2025 20:07:11
			httpd_vers_line = httpd_out.split('\n')[0]
			httpd_vers = httpd_vers_line.split('/')[1].split()[0]
		except FileNotFoundError:
			httpd_vers = 'not present'
		print ('Apache:',httpd_vers)
		
	if platform.system() == 'Windows':
		print ('IIS: ')
	
	print ('Vaadin:', get_vaadin_version())
	print ()


#
#   s c a n _ f i l e _ l a s t _ m a t c h
#

def scan_file_last_match (log_path: str, match_str: str) -> str:
	"""
	Scan the file at given path, returning the result of last found matching pattern.
	Returns None if no matches were found or the file is not readable.
	"""
	
	try:
		check_file_validity (log_path)
	except FileCheckError as e:
		return None
	
	line: str = None
	line_num: int = 1
	last_match: str = None
	
	while True:
		line = linecache.getline (log_path, line_num)
		if line == '': break
		m = re.search (match_str, line)
		if m: last_match = m.group(1)
		line_num += 1
	
	return last_match


class Spinner:
	"""
	Implements a spinning cursor for longer running processes.
	"""

	def __init__(self, message, delay=0.1):
		self._screen_lock = None
		self.spinner = itertools.cycle(['-', '/', '|', '\\'])
		self.delay = delay
		self.busy = False
		self.spinner_visible = False
		self.thread = None
		sys.stdout.write(message)

	def write_next(self):
		with self._screen_lock:
			if not self.spinner_visible:
				sys.stdout.write(next(self.spinner))
				self.spinner_visible = True
				sys.stdout.flush()

	def remove_spinner(self, cleanup=False):
		with self._screen_lock:
			if self.spinner_visible:
				sys.stdout.write('\b')
				self.spinner_visible = False
				if cleanup:
					sys.stdout.write(' ')       # overwrite spinner with blank
					#sys.stdout.write('\r')      # move to next line
				sys.stdout.flush()

	def spinner_task(self):
		while self.busy:
			self.write_next()
			time.sleep(self.delay)
			self.remove_spinner()

	def __enter__(self):
		if sys.stdout.isatty():
			self._screen_lock = threading.Lock()
			self.busy = True
			self.thread = threading.Thread(target=self.spinner_task)
			self.thread.start()

	def __exit__(self, exc_type, exc_val, exc_traceback):
		if sys.stdout.isatty():
			self.busy = False
			self.remove_spinner(cleanup=True)
		else:
			sys.stdout.write('\r')


#
#	s t r i p _ l i n e
#

def strip_line (log_name: str, line: str) -> str:
	"""
	When possible, remove repetitive or extraneous text in the logs.
	This is done after expanding tabs so columns should be at fixed positions.
	"""
	
	if log_name in ['access','event']:
		if line [24] == '-':
			items = line.split('\t')
			line = '{:23.23}  {:11}  {:4}  {:<}'.format (items[0], items[1], items[2], items[4])		# remove timezone and hostname
	if log_name == 'admin':
		if line [20] == '-':
			line = line [:20] + line [26:]							# remove timezone
	if log_name in ['clientstats','dapi','topcall']:
			line = line [:23] + line [29]								# remove timezone
	if log_name == 'dapi':
			line = line [:23] + line [29:45] + line[62:]			# remove timezone & hostname

	return line


# =================

class TailPrint (object):
	
	# based on https://github.com/kasun/python-tail
	
	''' Represents a tail command. '''
	def __init__(self, log_names: list):
		'''Initiate a Tail instance.
			Check for file validity, assigns callback function to standard out.
			Arguments:
				tailedFiles - List of file to be followed. '''
				
		self.log_names = []
		self.log_paths = []
		
		for log in log_names:
			path = get_log_path (log)
			self.check_file_validity(path)
			self.log_names.append (log)
			self.log_paths.append (path)
	
	def follow(self, s=1.0):
		''' Do a tail follow. If a callback function is registered it is called with every new line. 
		Else printed to standard out.

		Arguments:
			s - Number of seconds to wait between checking each file'''
		
		MINDELAY = 0.01 # wait at least 1/100 second before checking next log
		secs = max (s, MINDELAY)
		
		try:
			with ExitStack() as stack:
				#with open(self.tailed_file) as file_:
				fileList = [stack.enter_context(open(fpath)) for fpath in self.log_paths]
				for file in fileList:
					file.seek(0,2)
				
				# Go to the end of file
				while True:
					time.sleep (secs)

					for (file,log) in zip (fileList, self.log_names):
						while True:
							line = file.readline()
							if line:
								self.print_line (log, line)
							else:
								break
		
		except KeyboardInterrupt:
			return

	def check_file_validity(self, file_):
		''' Check whether the a given file exists, readable and is a file '''
		if not os.access(file_, os.F_OK):
			raise FileCheckError(f'File "{file_}" does not exist')
		if not os.access(file_, os.R_OK):
			raise FileCheckError(f'File "{file_}" not readable')
		if os.path.isdir(file_):
			raise FileCheckError(f'File "{file_}" is a directory')

	def print_line(self, log_name: str, line: str):
		"""
		Print output sent by one or more TailPrint objects. If output transitions to different file,
		indicate change by printing blank line followed by file path.
		"""
		
		global LAST_LOG_PRINTED
		
		# Check that we actually want to print this line.
		if FILTER_REGEX is None or FILTER_REGEX.search (line):
			
			if log_name != LAST_LOG_PRINTED:
				if LAST_LOG_PRINTED is not None:
					print ()
				# only print prefix line if we have transitioned output from one file to another
				print ('===', log_name)
				LAST_LOG_PRINTED = log_name
				
			print (expand_tabs_for_line (log_name, line),end='')

# =================


def handle_named_options (args) -> bool:
	"""
	Run through all possible named options (eg, -X or --xray) and
	execute its handler if set.
	
	Set the ignore_positionals variable if any later
	positional parameters should be skipped.
	"""
	
	ignore_positionals = False
	
	if args.check_connectivity:
		check_connectivity()
		ignore_positionals = True
	
	if args.databases:
		print_db_dirs_info()
		ignore_positionals = True

	if args.list:
		print_log_info()
		ignore_positionals = True
	
	if args.network:
		print_net_status()
		ignore_positionals = True
	
	if args.process_info:
		process_info()
		ignore_positionals = True
		
	if args.set:
		handle_set (args.set[0], args.set[1])
		ignore_positionals = True
		
	if args.version:
		print_version()
		ignore_positionals = True

	return ignore_positionals


#
#	m a i n
#

def main():
	"""
	Setup parser, handle named options, then positional options.
	"""
	
	global OUTPUT_MODE, SHOW_HEADERS, SUCCINCT_MODE, TIMESTAMP_START, TRUNCATE_MODE
	
	parser = init_parser()
	args = parser.parse_args()
	screen_lines_counter = 0					# if used, will start (num lines on screen * num screens) - space for prompt line

	while True:
		ignore_positionals = handle_named_options (args)
		
		if ignore_positionals:
			break	# only follow through if not limiting output to the above options
		
		if args.begin:
			#print (args.begin[0])
			TIMESTAMP_START = parse_begin_time (args.begin[0])
		
		if args.edit:
			edit_log (args.edit[0])
		
		if args.head:
			OUTPUT_MODE = OutputMode.HEAD
		
		if args.begin:
			# Use head mode with the -b option
			OUTPUT_MODE = OutputMode.HEAD
		
		if args.headers_off:
			SHOW_HEADERS = False
		
		if args.succinct:
			SUCCINCT_MODE = True
			
		if args.truncate:
			TRUNCATE_MODE = True
			
		if args.filter:
			if not compile_filter (args.filter[0]):	# Compile the default or the filter that was just set
				break													# bad regex
		
		#print (args.tail, args.logs,type (args.logs))
		
		# Assume we are printing at least one log.
		num_logs_to_print = len (args.logs)
		if num_logs_to_print == 0:
			print ('Error: expected at least one log name')
			break
		
		# Couldn't use choices=ALL_CHOICES in argparse, so must check names manually.
		for log_name in args.logs:
				if log_name not in ALL_CHOICES:
					print ('Error:', log_name,'not a valid log name')
					sys.exit (1)
		
		# TAILING/FOLLOWING LOGS
		
		if (isinstance (args.tail, bool) and args.tail) or (isinstance (args.tail,list) and args.tail[0] > 0):
			# Instead of printing the current tail or head, wait for new messages
			tailer = TailPrint (args.logs)
			if isinstance (args.tail, list):
				tailer.follow(args.tail[0])
			else:
				tailer.follow(1.0)
		
			break
		
		# HEAD OR TAIL
		
		lines_printed = 0;
		lastLog = args.logs[-1]
		
		num_mode,lines_per_log,screen_lines_counter = calc_row_metrics (num_logs_to_print, args.number[0])
		if lines_per_log < 1: break
		
		#print (logLinesCountVal, lines_per_log, screen_lines_counter)
		
		for log_name in args.logs:
			if log_name == lastLog and num_mode == 's':
				lines_per_log = screen_lines_counter			# Use whatever is left for last log (eg, there may be a remainder)
		
			lines_printed = print_log (log_name, lines_per_log)
		
			if lines_printed < 0:
				break # error occured
			
			screen_lines_counter -= lines_printed
					
		#curses.endwin()
		break

if __name__=="__main__":
    main()

